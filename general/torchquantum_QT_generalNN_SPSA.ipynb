{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Tools\n",
    "import time\n",
    "import numpy as np \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from QuantumTrain.util_SPSA import *\n",
    "import random\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# TorchQuantum\n",
    "import torchquantum as tq\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "# np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.algorithms.optimizers import SPSA, QNSPSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Classical target model initialization ###\n",
    "\n",
    "# Define the CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=5)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(8, 12, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()  \n",
    "        self.fc1 = nn.Linear(12*4*4, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.conv1(x))\n",
    "        x = self.pool2(self.conv2(x))\n",
    "        x = self.flatten(x)  # Use the Flatten layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the model and loss function\n",
    "model = CNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of NN parameters:  6690\n",
      "Required qubit number:  13\n"
     ]
    }
   ],
   "source": [
    "n_qubit, nw_list_normal = required_qubits_estimation(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Training setting ########################\n",
    "\n",
    "step       = 1e-4   # Learning rate\n",
    "batch_size = 1000    # Number of samples for each training step\n",
    "num_epochs = 100      # Number of training epochs\n",
    "q_depth    = 16     # Depth of the quantum circuit (number of variational layers)\n",
    "\n",
    "# Dataset setup\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Instantiate the model, move it to GPU, and set up loss function and optimizer\n",
    "model_qt = QuantumTrain(\n",
    "                        model,\n",
    "                        n_qubit,\n",
    "                        nw_list_normal,\n",
    "                        q_depth,\n",
    "                        device,\n",
    "                        )().to(device)\n",
    "\n",
    "\n",
    "# optimizer = optim.Adam(model_qt.parameters(), lr=step, weight_decay=1e-5, eps=1e-6)\n",
    "# optimizer = optim.Adam([\n",
    "#     {'params': model_qt.QuantumNN.parameters()},\n",
    "#     {'params': model_qt.MappingNetwork.parameters()}\n",
    "# ], lr=step, weight_decay=1e-5, eps=1e-6)\n",
    "\n",
    "\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 5, verbose = True, factor = 0.5)  # 'min' because we're minimizing loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of trainable parameter in Mapping model:  249\n",
      "# of trainable parameter in QNN model:  1248\n",
      "# of trainable parameter in full model:  1497\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_trainable_params_MM = sum(p.numel() for p in model_qt.MappingNetwork.parameters() if p.requires_grad)\n",
    "num_trainable_params_QNN = sum(p.numel() for p in model_qt.QuantumNN.parameters() if p.requires_grad)\n",
    "num_trainable_params = sum(p.numel() for p in model_qt.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"# of trainable parameter in Mapping model: \", num_trainable_params_MM)\n",
    "print(\"# of trainable parameter in QNN model: \", num_trainable_params_QNN)\n",
    "print(\"# of trainable parameter in full model: \",  num_trainable_params)#num_trainable_params_MM + num_trainable_params_QNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_list = [] \n",
    "# acc_list = [] \n",
    "\n",
    "\n",
    "# # Objective function adjusted for DataLoader\n",
    "# def objective(params):\n",
    "#     total_loss = 0.0\n",
    "#     param_idx = 0  # Keep track of the position in the flat param array\n",
    "    \n",
    "\n",
    "#     # Update model parameters\n",
    "#     with torch.no_grad():\n",
    "#         for param in model_qt.parameters():\n",
    "#             param_size = param.numel()\n",
    "#             # Extract the corresponding segment from the flat parameter array\n",
    "#             new_param_segment = params[param_idx:param_idx + param_size]\n",
    "            \n",
    "#             if new_param_segment.size != param_size:\n",
    "#                 # Size mismatch error, potentially due to incorrect parameter handling\n",
    "#                 raise ValueError(f\"Parameter size mismatch: expected {param_size}, got {new_param_segment.size}\")\n",
    "\n",
    "#             # Reshape and update the model parameter\n",
    "#             param.copy_(torch.from_numpy(new_param_segment).view_as(param))\n",
    "            \n",
    "#             # Update the index for the next parameter\n",
    "#             param_idx += param_size\n",
    "            \n",
    "#     # Iterate over batches\n",
    "#     for i, (images, labels) in enumerate(train_loader):\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         since_batch = time.time()\n",
    "        \n",
    "#         images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
    "#         # Compute loss for this batch\n",
    "#         outputs = model_qt(images)\n",
    "        \n",
    "#         labels_one_hot = F.one_hot(labels, num_classes=10).float()\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()     \n",
    "#         loss = nn.CrossEntropyLoss()(outputs, labels_one_hot)\n",
    "\n",
    "#         loss_list.append(loss.cpu().detach().numpy())\n",
    "#         acc = 100 * correct / total\n",
    "#         acc_list.append(acc)\n",
    "        \n",
    "#         total_loss += loss.cpu().detach().numpy()\n",
    "#         # if i % 30 == 0:\n",
    "#         print(f\"Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}, batch time: {time.time() - since_batch:.2f}, accuracy:  {(acc):.2f}%\")\n",
    "    \n",
    "#     # Return average loss over all batches\n",
    "#     return total_loss / len(train_loader)\n",
    "\n",
    "loss_list = [] \n",
    "acc_list = [] \n",
    "\n",
    "def objective(params):\n",
    "    since_batch = time.time()\n",
    "    total_loss = 0.0\n",
    "    param_idx = 0  # Keep track of the position in the flat param array\n",
    "\n",
    "    # Update model parameters\n",
    "    with torch.no_grad():\n",
    "        for param in model_qt.parameters():\n",
    "            param_size = param.numel()\n",
    "            # Extract the corresponding segment from the flat parameter array\n",
    "            new_param_segment = params[param_idx:param_idx + param_size]\n",
    "            \n",
    "            if new_param_segment.size != param_size:\n",
    "                # Size mismatch error, potentially due to incorrect parameter handling\n",
    "                raise ValueError(f\"Parameter size mismatch: expected {param_size}, got {new_param_segment.size}\")\n",
    "\n",
    "            # Reshape and update the model parameter\n",
    "            param.copy_(torch.from_numpy(new_param_segment).view_as(param))\n",
    "            \n",
    "            # Update the index for the next parameter\n",
    "            param_idx += param_size\n",
    "    \n",
    "    # Sample a random batch\n",
    "    batch_idx = random.randint(0, len(train_loader)-1)\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if i == batch_idx:\n",
    "            images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
    "            outputs = model_qt(images)\n",
    "            \n",
    "            labels_one_hot = F.one_hot(labels, num_classes=10).float()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = labels.size(0)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels_one_hot)\n",
    "\n",
    "            acc = 100 * correct / total\n",
    "            print(f\"Loss: {loss.item():.4f}, accuracy:  {(acc):.2f}%, batch time: {time.time() - since_batch:.2f} s\")\n",
    "            \n",
    "            \n",
    "            loss_list.append(loss.cpu().detach().numpy())\n",
    "            acc_list.append(acc)\n",
    "\n",
    "            total_loss = loss.cpu().detach().numpy()  # Since we are only considering one batch, this is the total loss\n",
    "            break  # Exit the loop after processing the random batch\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten initial parameters\n",
    "initial_params = []\n",
    "for param in model_qt.parameters():\n",
    "    initial_params.append(param.data.flatten().cpu().numpy())\n",
    "initial_params = np.concatenate(initial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1497"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 62.7658, accuracy:  7.40%, batch time: 2.53 s\n",
      "Loss: 64.6343, accuracy:  8.70%, batch time: 0.62 s\n",
      "Loss: 62.7448, accuracy:  9.60%, batch time: 1.14 s\n",
      "Loss: 63.5283, accuracy:  10.60%, batch time: 0.62 s\n",
      "Loss: 63.5409, accuracy:  8.80%, batch time: 3.26 s\n",
      "Loss: 61.6612, accuracy:  8.20%, batch time: 4.26 s\n",
      "Loss: 61.4781, accuracy:  9.00%, batch time: 4.25 s\n",
      "Loss: 59.8364, accuracy:  9.20%, batch time: 3.90 s\n",
      "Loss: 61.6975, accuracy:  8.80%, batch time: 0.76 s\n",
      "Loss: 60.1003, accuracy:  7.80%, batch time: 0.92 s\n",
      "Loss: 62.7465, accuracy:  8.60%, batch time: 0.46 s\n",
      "Loss: 60.6501, accuracy:  8.40%, batch time: 3.82 s\n",
      "Loss: 59.1667, accuracy:  10.10%, batch time: 4.58 s\n",
      "Loss: 63.1841, accuracy:  8.40%, batch time: 4.40 s\n",
      "Loss: 64.3883, accuracy:  9.20%, batch time: 2.52 s\n",
      "Loss: 61.0165, accuracy:  10.10%, batch time: 2.89 s\n",
      "Loss: 61.7201, accuracy:  9.10%, batch time: 3.67 s\n",
      "Loss: 64.5686, accuracy:  9.20%, batch time: 3.36 s\n",
      "Loss: 61.1507, accuracy:  8.60%, batch time: 4.61 s\n",
      "Loss: 62.9354, accuracy:  8.60%, batch time: 0.61 s\n",
      "Loss: 64.1145, accuracy:  7.60%, batch time: 0.93 s\n",
      "Loss: 62.5040, accuracy:  8.00%, batch time: 3.97 s\n",
      "Loss: 62.8433, accuracy:  9.60%, batch time: 0.24 s\n",
      "Loss: 62.3934, accuracy:  8.80%, batch time: 1.07 s\n",
      "Loss: 62.0040, accuracy:  8.70%, batch time: 4.07 s\n",
      "Loss: 59.4858, accuracy:  9.20%, batch time: 3.35 s\n",
      "Loss: 75.1929, accuracy:  8.90%, batch time: 4.64 s\n",
      "Loss: 60.1240, accuracy:  8.10%, batch time: 2.67 s\n",
      "Loss: 27940044800.0000, accuracy:  9.50%, batch time: 1.56 s\n",
      "=======================\n",
      "Iteration: 0\n",
      "Loss: 48.6314, accuracy:  7.50%, batch time: 3.18 s\n",
      "Loss: 75.2322, accuracy:  8.30%, batch time: 2.14 s\n",
      "Loss: 31904149536768.0000, accuracy:  7.40%, batch time: 1.55 s\n",
      "=======================\n",
      "Iteration: 1\n",
      "Loss: 108.3811, accuracy:  7.60%, batch time: 2.08 s\n",
      "Loss: 35.9016, accuracy:  9.20%, batch time: 1.47 s\n",
      "Loss: 17818394069156495360.0000, accuracy:  9.20%, batch time: 1.69 s\n",
      "=======================\n",
      "Iteration: 2\n",
      "Loss: 88.0546, accuracy:  10.50%, batch time: 3.14 s\n",
      "Loss: 41.6783, accuracy:  9.90%, batch time: 2.83 s\n",
      "Loss: 1089168004349952.0000, accuracy:  12.20%, batch time: 0.93 s\n",
      "=======================\n",
      "Iteration: 3\n"
     ]
    }
   ],
   "source": [
    "def callback(xk, y, z ,k, l):\n",
    "    print(\"=======================\")\n",
    "    print(\"Iteration:\", callback.iteration) #, \", fun: \", z) #, \", Loss: \", loss)\n",
    "    callback.iteration += 1\n",
    "\n",
    "callback.iteration = 0  # Initialize the iteration count\n",
    "\n",
    "spsa = SPSA(maxiter=500, callback=callback, blocking=True, learning_rate=1e-3, perturbation=1e-2)\n",
    "out = spsa.minimize(objective, x0 = initial_params)\n",
    "\n",
    "# out = spsa.optimize(\n",
    "#                 num_vars = len(initial_params),\n",
    "#                 objective_function = objective,\n",
    "#                 initial_point = initial_params\n",
    "#                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model parameters with optimized values\n",
    "optimized_params = out.x\n",
    "with torch.no_grad():\n",
    "    idx = 0\n",
    "    for param in model_qt.parameters():\n",
    "        numel = param.numel()\n",
    "        param.copy_(torch.from_numpy(optimized_params[idx:idx+numel]).view_as(param))\n",
    "        idx += numel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Testing train loop\n",
    "model_qt.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_qt(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy on the train set: {(100 * correct / total):.2f}%\")\n",
    "\n",
    "# Testing loop\n",
    "model_qt.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model_qt(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy on the test set: {(100 * correct / total):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# #############################################\n",
    "# ### Training loop ###########################\n",
    "\n",
    "# ### (Optional) Start from pretrained model ##\n",
    "# # model_qt = torch.load('L16/tq_mm_acc_99_bsf')\n",
    "# # model_qt.eval()  # Set the model to evaluation mode\n",
    "# #############################################\n",
    "\n",
    "# loss_list = [] \n",
    "# acc_list = [] \n",
    "# acc_best = 0\n",
    "# for epoch in range(num_epochs):\n",
    "#     model_qt.train()\n",
    "\n",
    "#     for i, (images, labels) in enumerate(train_loader):\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         since_batch = time.time()\n",
    "        \n",
    "#         images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model_qt(images)\n",
    "#         # print(\"output: \", outputs)\n",
    "#         labels_one_hot = F.one_hot(labels, num_classes=10).float()\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "        \n",
    "#         loss = criterion(outputs, labels_one_hot)\n",
    "#         loss_list.append(loss.cpu().detach().numpy())\n",
    "#         acc = 100 * correct / total\n",
    "#         acc_list.append(acc)\n",
    "        \n",
    "        \n",
    "#         if acc > acc_best:\n",
    "#             # torch.save(model_qt, 'L16/3/tq_mm_acc_'+str(int(acc))+'_bsf')\n",
    "#             acc_best = acc\n",
    "            \n",
    "#         spsa_step(model_qt, criterion, images, labels, lr=5e-5, epsilon=5e-4)\n",
    "        \n",
    "        \n",
    "#         # if (i+1) % 100 == 0:\n",
    "#         print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}, batch time: {time.time() - since_batch:.2f}, accuracy:  {(acc):.2f}%\")\n",
    "    \n",
    "    \n",
    "# #############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3), dpi = 150)\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"Quantum-Train training process\")\n",
    "plt.ylabel(\"cross-entropy loss\")\n",
    "plt.xlabel(\"training iteration\")\n",
    "plt.plot(loss_list ,color=plt.cm.Reds(0.6), marker = \"P\", markersize=2, lw = 1, alpha = 0.8, label = \"L\")\n",
    "plt.ylim(0, 5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
