{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Wei-Jia/anaconda3/envs/torchquantum/lib/python3.9/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11050). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# Basic Tools\n",
    "import time\n",
    "import numpy as np \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from QuantumTrain.util import *\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# TorchQuantum\n",
    "import torchquantum as tq\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "# np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Classical target model initialization ###\n",
    "\n",
    "# Define the CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        # Writing every operation as layer, so that the extraction function could read\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=5)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(8, 12, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()  \n",
    "        self.fc1 = nn.Linear(12*4*4, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.conv1(x))\n",
    "        x = self.pool2(self.conv2(x))\n",
    "        x = self.flatten(x)  # Use the Flatten layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the model and loss function\n",
    "model = CNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of NN parameters:  6690\n",
      "Required qubit number:  13\n"
     ]
    }
   ],
   "source": [
    "n_qubit, nw_list_normal = required_qubits_estimation(model)\n",
    "network_config          = network_config_extract(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Training setting ########################\n",
    "\n",
    "step       = 1e-4   # Learning rate\n",
    "batch_size = 1000    # Number of samples for each training step\n",
    "num_epochs = 10      # Number of training epochs\n",
    "q_depth    = 16     # Depth of the quantum circuit (number of variational layers)\n",
    "\n",
    "# Dataset setup\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Instantiate the model, move it to GPU, and set up loss function and optimizer\n",
    "model_qt = QuantumTrain(\n",
    "                        model,\n",
    "                        n_qubit,\n",
    "                        nw_list_normal,\n",
    "                        q_depth,\n",
    "                        device,\n",
    "                        network_config\n",
    "                        ).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = optim.Adam(model_qt.parameters(), lr=step, weight_decay=1e-5, eps=1e-6)\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model_qt.QuantumNN.parameters()},\n",
    "    {'params': model_qt.MappingNetwork.parameters()}\n",
    "], lr=step, weight_decay=1e-5, eps=1e-6)\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 5, verbose = True, factor = 0.5)  # 'min' because we're minimizing loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of trainable parameter in Mapping model:  249\n",
      "# of trainable parameter in QNN model:  1248\n",
      "# of trainable parameter in full model:  8187\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_trainable_params_MM = sum(p.numel() for p in model_qt.MappingNetwork.parameters() if p.requires_grad)\n",
    "num_trainable_params_QNN = sum(p.numel() for p in model_qt.QuantumNN.parameters() if p.requires_grad)\n",
    "num_trainable_params = sum(p.numel() for p in model_qt.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"# of trainable parameter in Mapping model: \", num_trainable_params_MM)\n",
    "print(\"# of trainable parameter in QNN model: \", num_trainable_params_QNN)\n",
    "print(\"# of trainable parameter in full model: \", num_trainable_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/60], Loss: 22.4013, batch time: 0.55, accuracy:  7.40%\n",
      "Epoch [1/10], Step [2/60], Loss: 22.1687, batch time: 0.38, accuracy:  7.40%\n",
      "Epoch [1/10], Step [3/60], Loss: 20.4757, batch time: 0.28, accuracy:  7.10%\n",
      "Epoch [1/10], Step [4/60], Loss: 20.7899, batch time: 0.29, accuracy:  7.90%\n",
      "Epoch [1/10], Step [5/60], Loss: 20.1095, batch time: 0.29, accuracy:  7.80%\n",
      "Epoch [1/10], Step [6/60], Loss: 18.8639, batch time: 0.29, accuracy:  7.90%\n",
      "Epoch [1/10], Step [7/60], Loss: 19.0451, batch time: 0.29, accuracy:  8.30%\n",
      "Epoch [1/10], Step [8/60], Loss: 19.0837, batch time: 0.29, accuracy:  7.70%\n",
      "Epoch [1/10], Step [9/60], Loss: 18.2830, batch time: 0.29, accuracy:  8.30%\n",
      "Epoch [1/10], Step [10/60], Loss: 18.5170, batch time: 0.30, accuracy:  6.70%\n",
      "Epoch [1/10], Step [11/60], Loss: 17.5140, batch time: 0.30, accuracy:  6.10%\n",
      "Epoch [1/10], Step [12/60], Loss: 17.3420, batch time: 0.29, accuracy:  8.40%\n",
      "Epoch [1/10], Step [13/60], Loss: 16.6696, batch time: 0.30, accuracy:  7.90%\n",
      "Epoch [1/10], Step [14/60], Loss: 16.5481, batch time: 0.31, accuracy:  6.90%\n",
      "Epoch [1/10], Step [15/60], Loss: 16.5487, batch time: 0.29, accuracy:  6.70%\n",
      "Epoch [1/10], Step [16/60], Loss: 15.0664, batch time: 0.30, accuracy:  8.10%\n",
      "Epoch [1/10], Step [17/60], Loss: 15.4072, batch time: 0.29, accuracy:  7.50%\n",
      "Epoch [1/10], Step [18/60], Loss: 15.0716, batch time: 0.29, accuracy:  7.70%\n",
      "Epoch [1/10], Step [19/60], Loss: 14.7950, batch time: 0.30, accuracy:  8.60%\n",
      "Epoch [1/10], Step [20/60], Loss: 14.1344, batch time: 0.29, accuracy:  8.10%\n",
      "Epoch [1/10], Step [21/60], Loss: 14.6898, batch time: 0.29, accuracy:  8.20%\n",
      "Epoch [1/10], Step [22/60], Loss: 14.0510, batch time: 0.30, accuracy:  8.40%\n",
      "Epoch [1/10], Step [23/60], Loss: 13.1759, batch time: 0.29, accuracy:  8.50%\n",
      "Epoch [1/10], Step [24/60], Loss: 13.5413, batch time: 0.30, accuracy:  7.50%\n",
      "Epoch [1/10], Step [25/60], Loss: 13.0035, batch time: 0.29, accuracy:  7.50%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#############################################\n",
    "### Training loop ###########################\n",
    "\n",
    "### (Optional) Start from pretrained model ##\n",
    "# model_qt = torch.load('L16/tq_mm_acc_99_bsf')\n",
    "# model_qt.eval()  # Set the model to evaluation mode\n",
    "#############################################\n",
    "\n",
    "loss_list = [] \n",
    "acc_list = [] \n",
    "acc_best = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model_qt.train()\n",
    "    train_loss = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        since_batch = time.time()\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model_qt(images)\n",
    "        # print(\"output: \", outputs)\n",
    "        labels_one_hot = F.one_hot(labels, num_classes=10).float()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels_one_hot)\n",
    "        # log_loss = torch.log(loss + 1e-6)\n",
    "        \n",
    "        loss_list.append(loss.cpu().detach().numpy())\n",
    "        acc = 100 * correct / total\n",
    "        acc_list.append(acc)\n",
    "        train_loss += loss.cpu().detach().numpy()\n",
    "        \n",
    "        # np.array(loss_list).dump(\"L16/3/loss_list.dat\")\n",
    "        # np.array(acc_list).dump(\"L16/3/acc_list.dat\")\n",
    "        if acc > acc_best:\n",
    "            # torch.save(model_qt, 'L16/3/tq_mm_acc_'+str(int(acc))+'_bsf')\n",
    "            acc_best = acc\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        # if (i+1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}, batch time: {time.time() - since_batch:.2f}, accuracy:  {(acc):.2f}%\")\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    scheduler.step(train_loss)\n",
    "    \n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print gradients of all parameters\n",
    "for name, param in model_qt.named_parameters():\n",
    "    print(f\"Gradient of {name}: {param.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
