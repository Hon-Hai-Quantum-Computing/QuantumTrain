{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Tools\n",
    "import time\n",
    "import numpy as np \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from QuantumTrain.util import *\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# TorchQuantum\n",
    "import torchquantum as tq\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "# np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Classical target model initialization ###\n",
    "\n",
    "# Define the CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        # Writing every operation as layer, so that the extraction function could read\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=5)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(8, 12, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()  \n",
    "        self.fc1 = nn.Linear(12*4*4, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.conv1(x))\n",
    "        x = self.pool2(self.conv2(x))\n",
    "        x = self.flatten(x)  # Use the Flatten layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the model and loss function\n",
    "model = CNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_qubit, nw_list_normal = required_qubits_estimation(model)\n",
    "# network_config          = network_config_extract(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of NN parameters:  6690\n",
      "Required qubit number:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenyu/anaconda3/envs/tq/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Training setting ########################\n",
    "\n",
    "step       = 1e-4   # Learning rate\n",
    "batch_size = 1000    # Number of samples for each training step\n",
    "num_epochs = 1      # Number of training epochs\n",
    "q_depth    = 16     # Depth of the quantum circuit (number of variational layers)\n",
    "\n",
    "# Dataset setup\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Instantiate the model, move it to GPU, and set up loss function and optimizer\n",
    "model_qt = QuantumTrain(\n",
    "                        model,\n",
    "                        q_depth,\n",
    "                        device,\n",
    "                        ).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = optim.Adam(model_qt.parameters(), lr=step, weight_decay=1e-5, eps=1e-6)\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model_qt.QuantumNN.parameters()},\n",
    "    {'params': model_qt.MappingNetwork.parameters()}\n",
    "], lr=step, weight_decay=1e-5, eps=1e-6)\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 5, verbose = True, factor = 0.5)  # 'min' because we're minimizing loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of trainable parameter in Mapping model:  249\n",
      "# of trainable parameter in QNN model:  1248\n",
      "# of trainable parameter in full model:  8187\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_trainable_params_MM = sum(p.numel() for p in model_qt.MappingNetwork.parameters() if p.requires_grad)\n",
    "num_trainable_params_QNN = sum(p.numel() for p in model_qt.QuantumNN.parameters() if p.requires_grad)\n",
    "num_trainable_params = sum(p.numel() for p in model_qt.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"# of trainable parameter in Mapping model: \", num_trainable_params_MM)\n",
    "print(\"# of trainable parameter in QNN model: \", num_trainable_params_QNN)\n",
    "print(\"# of trainable parameter in full model: \", num_trainable_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1/60], Loss: 71.1569, batch time: 0.56, accuracy:  10.80%\n",
      "Epoch [1/1], Step [2/60], Loss: 67.5563, batch time: 0.28, accuracy:  11.20%\n",
      "Epoch [1/1], Step [3/60], Loss: 68.5654, batch time: 0.28, accuracy:  9.40%\n",
      "Epoch [1/1], Step [4/60], Loss: 69.8510, batch time: 0.29, accuracy:  10.20%\n",
      "Epoch [1/1], Step [5/60], Loss: 65.9082, batch time: 0.32, accuracy:  8.80%\n",
      "Epoch [1/1], Step [6/60], Loss: 64.1576, batch time: 0.29, accuracy:  8.90%\n",
      "Epoch [1/1], Step [7/60], Loss: 61.6979, batch time: 0.29, accuracy:  10.10%\n",
      "Epoch [1/1], Step [8/60], Loss: 62.8467, batch time: 0.29, accuracy:  10.30%\n",
      "Epoch [1/1], Step [9/60], Loss: 58.9354, batch time: 0.29, accuracy:  9.50%\n",
      "Epoch [1/1], Step [10/60], Loss: 58.7592, batch time: 0.29, accuracy:  9.80%\n",
      "Epoch [1/1], Step [11/60], Loss: 59.6430, batch time: 0.29, accuracy:  10.00%\n",
      "Epoch [1/1], Step [12/60], Loss: 59.1256, batch time: 0.29, accuracy:  10.10%\n",
      "Epoch [1/1], Step [13/60], Loss: 55.7284, batch time: 0.29, accuracy:  10.20%\n",
      "Epoch [1/1], Step [14/60], Loss: 54.3799, batch time: 0.29, accuracy:  9.40%\n",
      "Epoch [1/1], Step [15/60], Loss: 52.0049, batch time: 0.29, accuracy:  10.90%\n",
      "Epoch [1/1], Step [16/60], Loss: 52.8817, batch time: 0.30, accuracy:  10.30%\n",
      "Epoch [1/1], Step [17/60], Loss: 52.8798, batch time: 0.29, accuracy:  10.70%\n",
      "Epoch [1/1], Step [18/60], Loss: 51.9154, batch time: 0.30, accuracy:  8.00%\n",
      "Epoch [1/1], Step [19/60], Loss: 49.7626, batch time: 0.29, accuracy:  10.70%\n",
      "Epoch [1/1], Step [20/60], Loss: 48.8716, batch time: 0.29, accuracy:  8.50%\n",
      "Epoch [1/1], Step [21/60], Loss: 45.5846, batch time: 0.30, accuracy:  10.80%\n",
      "Epoch [1/1], Step [22/60], Loss: 48.5990, batch time: 0.30, accuracy:  10.20%\n",
      "Epoch [1/1], Step [23/60], Loss: 47.7978, batch time: 0.29, accuracy:  9.10%\n",
      "Epoch [1/1], Step [24/60], Loss: 44.6982, batch time: 0.30, accuracy:  9.80%\n",
      "Epoch [1/1], Step [25/60], Loss: 44.4041, batch time: 0.29, accuracy:  9.20%\n",
      "Epoch [1/1], Step [26/60], Loss: 46.4829, batch time: 0.29, accuracy:  9.90%\n",
      "Epoch [1/1], Step [27/60], Loss: 46.1409, batch time: 0.29, accuracy:  10.40%\n",
      "Epoch [1/1], Step [28/60], Loss: 43.6995, batch time: 0.29, accuracy:  9.60%\n",
      "Epoch [1/1], Step [29/60], Loss: 41.4731, batch time: 0.29, accuracy:  10.30%\n",
      "Epoch [1/1], Step [30/60], Loss: 40.2534, batch time: 0.30, accuracy:  10.20%\n",
      "Epoch [1/1], Step [31/60], Loss: 42.6438, batch time: 0.29, accuracy:  9.80%\n",
      "Epoch [1/1], Step [32/60], Loss: 40.8358, batch time: 0.38, accuracy:  7.90%\n",
      "Epoch [1/1], Step [33/60], Loss: 41.7449, batch time: 0.29, accuracy:  10.10%\n",
      "Epoch [1/1], Step [34/60], Loss: 37.6868, batch time: 0.30, accuracy:  8.30%\n",
      "Epoch [1/1], Step [35/60], Loss: 37.1171, batch time: 0.30, accuracy:  9.20%\n",
      "Epoch [1/1], Step [36/60], Loss: 37.0605, batch time: 0.30, accuracy:  10.00%\n",
      "Epoch [1/1], Step [37/60], Loss: 36.9834, batch time: 0.29, accuracy:  10.30%\n",
      "Epoch [1/1], Step [38/60], Loss: 37.1835, batch time: 0.29, accuracy:  10.30%\n",
      "Epoch [1/1], Step [39/60], Loss: 36.2125, batch time: 0.30, accuracy:  10.80%\n",
      "Epoch [1/1], Step [40/60], Loss: 35.6192, batch time: 0.31, accuracy:  8.80%\n",
      "Epoch [1/1], Step [41/60], Loss: 35.8005, batch time: 0.29, accuracy:  9.50%\n",
      "Epoch [1/1], Step [42/60], Loss: 33.2544, batch time: 0.30, accuracy:  10.60%\n",
      "Epoch [1/1], Step [43/60], Loss: 33.0205, batch time: 0.29, accuracy:  9.90%\n",
      "Epoch [1/1], Step [44/60], Loss: 33.3612, batch time: 0.30, accuracy:  10.10%\n",
      "Epoch [1/1], Step [45/60], Loss: 32.7583, batch time: 0.29, accuracy:  9.40%\n",
      "Epoch [1/1], Step [46/60], Loss: 33.5841, batch time: 0.29, accuracy:  8.80%\n",
      "Epoch [1/1], Step [47/60], Loss: 32.1067, batch time: 0.30, accuracy:  8.80%\n",
      "Epoch [1/1], Step [48/60], Loss: 30.5596, batch time: 0.30, accuracy:  10.20%\n",
      "Epoch [1/1], Step [49/60], Loss: 30.6433, batch time: 0.29, accuracy:  10.60%\n",
      "Epoch [1/1], Step [50/60], Loss: 31.7149, batch time: 0.29, accuracy:  9.20%\n",
      "Epoch [1/1], Step [51/60], Loss: 30.0350, batch time: 0.29, accuracy:  10.60%\n",
      "Epoch [1/1], Step [52/60], Loss: 27.3768, batch time: 0.29, accuracy:  11.00%\n",
      "Epoch [1/1], Step [53/60], Loss: 28.5413, batch time: 0.30, accuracy:  10.30%\n",
      "Epoch [1/1], Step [54/60], Loss: 29.3816, batch time: 0.29, accuracy:  10.10%\n",
      "Epoch [1/1], Step [55/60], Loss: 29.0873, batch time: 0.29, accuracy:  11.00%\n",
      "Epoch [1/1], Step [56/60], Loss: 26.6263, batch time: 0.29, accuracy:  11.30%\n",
      "Epoch [1/1], Step [57/60], Loss: 26.9341, batch time: 0.29, accuracy:  10.50%\n",
      "Epoch [1/1], Step [58/60], Loss: 26.5472, batch time: 0.29, accuracy:  9.70%\n",
      "Epoch [1/1], Step [59/60], Loss: 27.1604, batch time: 0.29, accuracy:  10.70%\n",
      "Epoch [1/1], Step [60/60], Loss: 25.9609, batch time: 0.29, accuracy:  10.30%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#############################################\n",
    "### Training loop ###########################\n",
    "\n",
    "### (Optional) Start from pretrained model ##\n",
    "# model_qt = torch.load('L16/tq_mm_acc_99_bsf')\n",
    "# model_qt.eval()  # Set the model to evaluation mode\n",
    "#############################################\n",
    "\n",
    "loss_list = [] \n",
    "acc_list = [] \n",
    "acc_best = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model_qt.train()\n",
    "    train_loss = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        since_batch = time.time()\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model_qt(images)\n",
    "        # print(\"output: \", outputs)\n",
    "        labels_one_hot = F.one_hot(labels, num_classes=10).float()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels_one_hot)\n",
    "        # log_loss = torch.log(loss + 1e-6)\n",
    "        \n",
    "        loss_list.append(loss.cpu().detach().numpy())\n",
    "        acc = 100 * correct / total\n",
    "        acc_list.append(acc)\n",
    "        train_loss += loss.cpu().detach().numpy()\n",
    "        \n",
    "        # np.array(loss_list).dump(\"L16/3/loss_list.dat\")\n",
    "        # np.array(acc_list).dump(\"L16/3/acc_list.dat\")\n",
    "        if acc > acc_best:\n",
    "            # torch.save(model_qt, 'L16/3/tq_mm_acc_'+str(int(acc))+'_bsf')\n",
    "            acc_best = acc\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        # if (i+1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}, batch time: {time.time() - since_batch:.2f}, accuracy:  {(acc):.2f}%\")\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    scheduler.step(train_loss)\n",
    "    \n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
