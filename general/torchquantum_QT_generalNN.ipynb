{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Wei-Jia/anaconda3/envs/torchquantum/lib/python3.9/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11050). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# Basic Tools\n",
    "import time\n",
    "import numpy as np \n",
    "from util import *\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# TorchQuantum\n",
    "import torchquantum as tq\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "# np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Classical target model initialization ###\n",
    "\n",
    "# Define the CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        # Writing every operation as layer, so that the extraction function could read\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=5)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(8, 12, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()  \n",
    "        self.fc1 = nn.Linear(12*4*4, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.conv1(x))\n",
    "        x = self.pool2(self.conv2(x))\n",
    "        x = self.flatten(x)  # Use the Flatten layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the model and loss function\n",
    "model = CNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of NN parameters:  6690\n",
      "Required qubit number:  13\n"
     ]
    }
   ],
   "source": [
    "n_qubit, nw_list_normal = required_qubits_estimation(model)\n",
    "network_config          = network_config_extract(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m\n\u001b[1;32m     13\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset\u001b[38;5;241m=\u001b[39mtest_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Instantiate the model, move it to GPU, and set up loss function and optimizer\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# model_qt = QuantumTrain(\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#                         model,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#                         network_config\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#                         )().to(device)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m model_qt \u001b[38;5;241m=\u001b[39m \u001b[43mLewHybridNN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQuantumTrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mn_qubit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mnw_list_normal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mq_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mnetwork_config\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     34\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# optimizer = optim.Adam(model_qt.parameters(), lr=step, weight_decay=1e-5, eps=1e-6)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchquantum/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchquantum/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "\n",
    "### Training setting ########################\n",
    "\n",
    "step       = 1e-4   # Learning rate\n",
    "batch_size = 100    # Number of samples for each training step\n",
    "num_epochs = 10      # Number of training epochs\n",
    "q_depth    = 16     # Depth of the quantum circuit (number of variational layers)\n",
    "\n",
    "# Dataset setup\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Instantiate the model, move it to GPU, and set up loss function and optimizer\n",
    "# model_qt = QuantumTrain(\n",
    "#                         model,\n",
    "#                         n_qubit,\n",
    "#                         nw_list_normal,\n",
    "#                         q_depth,\n",
    "#                         device,\n",
    "#                         network_config\n",
    "#                         )().to(device)\n",
    "\n",
    "model_qt = LewHybridNN.QuantumTrain(\n",
    "                        model,\n",
    "                        n_qubit,\n",
    "                        nw_list_normal,\n",
    "                        q_depth,\n",
    "                        device,\n",
    "                        network_config\n",
    "                        )().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = optim.Adam(model_qt.parameters(), lr=step, weight_decay=1e-5, eps=1e-6)\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model_qt.QuantumNN.parameters()},\n",
    "    {'params': model_qt.MappingNetwork.parameters()}\n",
    "], lr=step, weight_decay=1e-5, eps=1e-6)\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 5, verbose = True, factor = 0.5)  # 'min' because we're minimizing loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of trainable parameter in Mapping model:  249\n",
      "# of trainable parameter in QNN model:  1248\n",
      "# of trainable parameter in full model:  1497\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_trainable_params_MM = sum(p.numel() for p in model_qt.MappingNetwork.parameters() if p.requires_grad)\n",
    "num_trainable_params_QNN = sum(p.numel() for p in model_qt.QuantumNN.parameters() if p.requires_grad)\n",
    "num_trainable_params = sum(p.numel() for p in model_qt.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"# of trainable parameter in Mapping model: \", num_trainable_params_MM)\n",
    "print(\"# of trainable parameter in QNN model: \", num_trainable_params_QNN)\n",
    "print(\"# of trainable parameter in full model: \", num_trainable_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/600], Loss: 16.0768, batch time: 1.87, accuracy:  3.00%\n",
      "Epoch [1/10], Step [2/600], Loss: 14.6499, batch time: 1.35, accuracy:  9.00%\n",
      "Epoch [1/10], Step [3/600], Loss: 15.9573, batch time: 1.34, accuracy:  6.00%\n",
      "Epoch [1/10], Step [4/600], Loss: 15.4101, batch time: 1.34, accuracy:  2.00%\n",
      "Epoch [1/10], Step [5/600], Loss: 15.2650, batch time: 1.18, accuracy:  3.00%\n",
      "Epoch [1/10], Step [6/600], Loss: 14.5986, batch time: 1.19, accuracy:  3.00%\n",
      "Epoch [1/10], Step [7/600], Loss: 14.3709, batch time: 0.88, accuracy:  5.00%\n",
      "Epoch [1/10], Step [8/600], Loss: 12.8227, batch time: 1.00, accuracy:  5.00%\n",
      "Epoch [1/10], Step [9/600], Loss: 12.5973, batch time: 0.90, accuracy:  7.00%\n",
      "Epoch [1/10], Step [10/600], Loss: 12.5266, batch time: 1.14, accuracy:  5.00%\n",
      "Epoch [1/10], Step [11/600], Loss: 13.9612, batch time: 1.36, accuracy:  6.00%\n",
      "Epoch [1/10], Step [12/600], Loss: 13.2155, batch time: 0.90, accuracy:  7.00%\n",
      "Epoch [1/10], Step [13/600], Loss: 12.5463, batch time: 0.84, accuracy:  8.00%\n",
      "Epoch [1/10], Step [14/600], Loss: 12.2447, batch time: 1.02, accuracy:  6.00%\n",
      "Epoch [1/10], Step [15/600], Loss: 12.9869, batch time: 0.98, accuracy:  13.00%\n",
      "Epoch [1/10], Step [16/600], Loss: 11.7630, batch time: 1.17, accuracy:  14.00%\n",
      "Epoch [1/10], Step [17/600], Loss: 11.2609, batch time: 0.91, accuracy:  9.00%\n",
      "Epoch [1/10], Step [18/600], Loss: 11.9328, batch time: 1.10, accuracy:  8.00%\n",
      "Epoch [1/10], Step [19/600], Loss: 11.5589, batch time: 1.21, accuracy:  6.00%\n",
      "Epoch [1/10], Step [20/600], Loss: 11.4626, batch time: 0.86, accuracy:  9.00%\n",
      "Epoch [1/10], Step [21/600], Loss: 10.8069, batch time: 1.04, accuracy:  8.00%\n",
      "Epoch [1/10], Step [22/600], Loss: 11.6487, batch time: 1.13, accuracy:  8.00%\n",
      "Epoch [1/10], Step [23/600], Loss: 10.3908, batch time: 1.07, accuracy:  7.00%\n",
      "Epoch [1/10], Step [24/600], Loss: 10.6695, batch time: 0.84, accuracy:  13.00%\n",
      "Epoch [1/10], Step [25/600], Loss: 10.6204, batch time: 0.99, accuracy:  5.00%\n",
      "Epoch [1/10], Step [26/600], Loss: 11.7062, batch time: 1.28, accuracy:  6.00%\n",
      "Epoch [1/10], Step [27/600], Loss: 10.8062, batch time: 1.30, accuracy:  13.00%\n",
      "Epoch [1/10], Step [28/600], Loss: 8.6728, batch time: 1.01, accuracy:  8.00%\n",
      "Epoch [1/10], Step [29/600], Loss: 8.8382, batch time: 1.63, accuracy:  7.00%\n",
      "Epoch [1/10], Step [30/600], Loss: 10.6300, batch time: 0.98, accuracy:  6.00%\n",
      "Epoch [1/10], Step [31/600], Loss: 10.3493, batch time: 1.07, accuracy:  5.00%\n",
      "Epoch [1/10], Step [32/600], Loss: 9.1670, batch time: 0.95, accuracy:  12.00%\n",
      "Epoch [1/10], Step [33/600], Loss: 9.0973, batch time: 1.40, accuracy:  13.00%\n",
      "Epoch [1/10], Step [34/600], Loss: 9.2464, batch time: 1.27, accuracy:  4.00%\n",
      "Epoch [1/10], Step [35/600], Loss: 9.6718, batch time: 1.04, accuracy:  5.00%\n",
      "Epoch [1/10], Step [36/600], Loss: 8.0611, batch time: 1.01, accuracy:  14.00%\n",
      "Epoch [1/10], Step [37/600], Loss: 8.6992, batch time: 1.12, accuracy:  6.00%\n",
      "Epoch [1/10], Step [38/600], Loss: 8.9837, batch time: 1.17, accuracy:  6.00%\n",
      "Epoch [1/10], Step [39/600], Loss: 7.9107, batch time: 0.94, accuracy:  12.00%\n",
      "Epoch [1/10], Step [40/600], Loss: 9.4347, batch time: 1.37, accuracy:  12.00%\n",
      "Epoch [1/10], Step [41/600], Loss: 7.6391, batch time: 1.05, accuracy:  9.00%\n",
      "Epoch [1/10], Step [42/600], Loss: 8.0746, batch time: 1.25, accuracy:  9.00%\n",
      "Epoch [1/10], Step [43/600], Loss: 9.3599, batch time: 1.12, accuracy:  4.00%\n",
      "Epoch [1/10], Step [44/600], Loss: 8.4129, batch time: 0.90, accuracy:  7.00%\n",
      "Epoch [1/10], Step [45/600], Loss: 9.0129, batch time: 0.98, accuracy:  6.00%\n",
      "Epoch [1/10], Step [46/600], Loss: 8.2387, batch time: 1.16, accuracy:  9.00%\n",
      "Epoch [1/10], Step [47/600], Loss: 7.3174, batch time: 1.17, accuracy:  16.00%\n",
      "Epoch [1/10], Step [48/600], Loss: 8.5974, batch time: 1.41, accuracy:  4.00%\n",
      "Epoch [1/10], Step [49/600], Loss: 6.9526, batch time: 0.99, accuracy:  12.00%\n",
      "Epoch [1/10], Step [50/600], Loss: 7.7172, batch time: 1.06, accuracy:  6.00%\n",
      "Epoch [1/10], Step [51/600], Loss: 6.7786, batch time: 0.89, accuracy:  11.00%\n",
      "Epoch [1/10], Step [52/600], Loss: 8.3616, batch time: 1.08, accuracy:  3.00%\n",
      "Epoch [1/10], Step [53/600], Loss: 7.5287, batch time: 0.86, accuracy:  6.00%\n",
      "Epoch [1/10], Step [54/600], Loss: 6.8137, batch time: 1.08, accuracy:  8.00%\n",
      "Epoch [1/10], Step [55/600], Loss: 6.9558, batch time: 1.21, accuracy:  5.00%\n",
      "Epoch [1/10], Step [56/600], Loss: 7.5503, batch time: 1.30, accuracy:  5.00%\n",
      "Epoch [1/10], Step [57/600], Loss: 8.0822, batch time: 1.06, accuracy:  4.00%\n",
      "Epoch [1/10], Step [58/600], Loss: 7.3444, batch time: 1.26, accuracy:  9.00%\n",
      "Epoch [1/10], Step [59/600], Loss: 7.0861, batch time: 1.05, accuracy:  5.00%\n",
      "Epoch [1/10], Step [60/600], Loss: 7.7341, batch time: 0.81, accuracy:  5.00%\n",
      "Epoch [1/10], Step [61/600], Loss: 6.1699, batch time: 0.94, accuracy:  7.00%\n",
      "Epoch [1/10], Step [62/600], Loss: 7.0261, batch time: 0.94, accuracy:  8.00%\n",
      "Epoch [1/10], Step [63/600], Loss: 6.8589, batch time: 0.86, accuracy:  6.00%\n",
      "Epoch [1/10], Step [64/600], Loss: 6.9719, batch time: 1.15, accuracy:  7.00%\n",
      "Epoch [1/10], Step [65/600], Loss: 6.3203, batch time: 1.23, accuracy:  3.00%\n",
      "Epoch [1/10], Step [66/600], Loss: 6.6144, batch time: 1.23, accuracy:  9.00%\n",
      "Epoch [1/10], Step [67/600], Loss: 6.3051, batch time: 1.26, accuracy:  8.00%\n",
      "Epoch [1/10], Step [68/600], Loss: 6.0849, batch time: 0.92, accuracy:  9.00%\n",
      "Epoch [1/10], Step [69/600], Loss: 6.2616, batch time: 1.29, accuracy:  8.00%\n",
      "Epoch [1/10], Step [70/600], Loss: 6.0348, batch time: 1.48, accuracy:  7.00%\n",
      "Epoch [1/10], Step [71/600], Loss: 6.5345, batch time: 1.96, accuracy:  7.00%\n",
      "Epoch [1/10], Step [72/600], Loss: 6.5195, batch time: 1.31, accuracy:  4.00%\n",
      "Epoch [1/10], Step [73/600], Loss: 6.1492, batch time: 1.15, accuracy:  8.00%\n",
      "Epoch [1/10], Step [74/600], Loss: 5.7982, batch time: 1.11, accuracy:  9.00%\n",
      "Epoch [1/10], Step [75/600], Loss: 5.7445, batch time: 1.36, accuracy:  6.00%\n",
      "Epoch [1/10], Step [76/600], Loss: 5.5191, batch time: 1.25, accuracy:  7.00%\n",
      "Epoch [1/10], Step [77/600], Loss: 5.1757, batch time: 1.09, accuracy:  9.00%\n",
      "Epoch [1/10], Step [78/600], Loss: 5.1790, batch time: 1.36, accuracy:  5.00%\n",
      "Epoch [1/10], Step [79/600], Loss: 5.9664, batch time: 1.99, accuracy:  9.00%\n",
      "Epoch [1/10], Step [80/600], Loss: 6.2767, batch time: 1.28, accuracy:  4.00%\n",
      "Epoch [1/10], Step [81/600], Loss: 5.2651, batch time: 1.22, accuracy:  4.00%\n",
      "Epoch [1/10], Step [82/600], Loss: 5.0259, batch time: 1.28, accuracy:  7.00%\n",
      "Epoch [1/10], Step [83/600], Loss: 5.2478, batch time: 1.04, accuracy:  7.00%\n",
      "Epoch [1/10], Step [84/600], Loss: 5.7136, batch time: 1.10, accuracy:  3.00%\n",
      "Epoch [1/10], Step [85/600], Loss: 5.2446, batch time: 2.29, accuracy:  8.00%\n",
      "Epoch [1/10], Step [86/600], Loss: 5.2423, batch time: 1.39, accuracy:  7.00%\n",
      "Epoch [1/10], Step [87/600], Loss: 5.1701, batch time: 1.49, accuracy:  6.00%\n",
      "Epoch [1/10], Step [88/600], Loss: 5.5159, batch time: 1.36, accuracy:  4.00%\n",
      "Epoch [1/10], Step [89/600], Loss: 5.4234, batch time: 1.26, accuracy:  4.00%\n",
      "Epoch [1/10], Step [90/600], Loss: 5.6115, batch time: 2.06, accuracy:  3.00%\n",
      "Epoch [1/10], Step [91/600], Loss: 5.5694, batch time: 1.31, accuracy:  1.00%\n",
      "Epoch [1/10], Step [92/600], Loss: 5.0745, batch time: 1.42, accuracy:  3.00%\n",
      "Epoch [1/10], Step [93/600], Loss: 5.4400, batch time: 1.60, accuracy:  7.00%\n",
      "Epoch [1/10], Step [94/600], Loss: 5.2225, batch time: 1.84, accuracy:  2.00%\n",
      "Epoch [1/10], Step [95/600], Loss: 5.0751, batch time: 1.36, accuracy:  6.00%\n",
      "Epoch [1/10], Step [96/600], Loss: 5.4164, batch time: 1.39, accuracy:  2.00%\n",
      "Epoch [1/10], Step [97/600], Loss: 4.8920, batch time: 1.37, accuracy:  9.00%\n",
      "Epoch [1/10], Step [98/600], Loss: 5.2005, batch time: 1.07, accuracy:  2.00%\n",
      "Epoch [1/10], Step [99/600], Loss: 5.6144, batch time: 1.08, accuracy:  4.00%\n",
      "Epoch [1/10], Step [100/600], Loss: 4.5438, batch time: 1.88, accuracy:  4.00%\n",
      "Epoch [1/10], Step [101/600], Loss: 4.9460, batch time: 1.49, accuracy:  5.00%\n",
      "Epoch [1/10], Step [102/600], Loss: 4.8047, batch time: 1.87, accuracy:  4.00%\n",
      "Epoch [1/10], Step [103/600], Loss: 4.6188, batch time: 1.55, accuracy:  7.00%\n",
      "Epoch [1/10], Step [104/600], Loss: 4.6574, batch time: 1.35, accuracy:  7.00%\n",
      "Epoch [1/10], Step [105/600], Loss: 4.9655, batch time: 1.37, accuracy:  3.00%\n",
      "Epoch [1/10], Step [106/600], Loss: 4.7095, batch time: 1.60, accuracy:  5.00%\n",
      "Epoch [1/10], Step [107/600], Loss: 4.6674, batch time: 1.29, accuracy:  5.00%\n",
      "Epoch [1/10], Step [108/600], Loss: 4.7981, batch time: 1.39, accuracy:  5.00%\n",
      "Epoch [1/10], Step [109/600], Loss: 4.9452, batch time: 1.63, accuracy:  4.00%\n",
      "Epoch [1/10], Step [110/600], Loss: 4.6202, batch time: 1.89, accuracy:  5.00%\n",
      "Epoch [1/10], Step [111/600], Loss: 4.7935, batch time: 1.45, accuracy:  4.00%\n",
      "Epoch [1/10], Step [112/600], Loss: 4.4038, batch time: 1.44, accuracy:  4.00%\n",
      "Epoch [1/10], Step [113/600], Loss: 4.7199, batch time: 1.44, accuracy:  4.00%\n",
      "Epoch [1/10], Step [114/600], Loss: 4.8062, batch time: 1.60, accuracy:  3.00%\n",
      "Epoch [1/10], Step [115/600], Loss: 4.4962, batch time: 2.12, accuracy:  7.00%\n",
      "Epoch [1/10], Step [116/600], Loss: 4.3472, batch time: 1.42, accuracy:  4.00%\n",
      "Epoch [1/10], Step [117/600], Loss: 4.6061, batch time: 1.25, accuracy:  1.00%\n",
      "Epoch [1/10], Step [118/600], Loss: 4.4391, batch time: 1.67, accuracy:  6.00%\n",
      "Epoch [1/10], Step [119/600], Loss: 4.3535, batch time: 1.50, accuracy:  6.00%\n",
      "Epoch [1/10], Step [120/600], Loss: 4.2762, batch time: 1.27, accuracy:  5.00%\n",
      "Epoch [1/10], Step [121/600], Loss: 4.3899, batch time: 1.43, accuracy:  5.00%\n",
      "Epoch [1/10], Step [122/600], Loss: 4.1098, batch time: 1.48, accuracy:  5.00%\n",
      "Epoch [1/10], Step [123/600], Loss: 4.2246, batch time: 1.39, accuracy:  7.00%\n",
      "Epoch [1/10], Step [124/600], Loss: 4.0196, batch time: 1.79, accuracy:  3.00%\n",
      "Epoch [1/10], Step [125/600], Loss: 4.1587, batch time: 1.30, accuracy:  7.00%\n",
      "Epoch [1/10], Step [126/600], Loss: 4.3328, batch time: 1.46, accuracy:  9.00%\n",
      "Epoch [1/10], Step [127/600], Loss: 3.7022, batch time: 1.37, accuracy:  9.00%\n",
      "Epoch [1/10], Step [128/600], Loss: 4.2615, batch time: 1.48, accuracy:  5.00%\n",
      "Epoch [1/10], Step [129/600], Loss: 3.8422, batch time: 1.17, accuracy:  7.00%\n",
      "Epoch [1/10], Step [130/600], Loss: 3.9707, batch time: 1.19, accuracy:  7.00%\n",
      "Epoch [1/10], Step [131/600], Loss: 3.8211, batch time: 1.36, accuracy:  10.00%\n",
      "Epoch [1/10], Step [132/600], Loss: 4.2393, batch time: 1.37, accuracy:  10.00%\n",
      "Epoch [1/10], Step [133/600], Loss: 3.9556, batch time: 1.64, accuracy:  6.00%\n",
      "Epoch [1/10], Step [134/600], Loss: 4.0226, batch time: 1.86, accuracy:  7.00%\n",
      "Epoch [1/10], Step [135/600], Loss: 4.1258, batch time: 1.38, accuracy:  7.00%\n",
      "Epoch [1/10], Step [136/600], Loss: 3.6441, batch time: 1.05, accuracy:  10.00%\n",
      "Epoch [1/10], Step [137/600], Loss: 3.9824, batch time: 1.22, accuracy:  6.00%\n",
      "Epoch [1/10], Step [138/600], Loss: 3.6819, batch time: 1.47, accuracy:  4.00%\n",
      "Epoch [1/10], Step [139/600], Loss: 4.1213, batch time: 1.35, accuracy:  5.00%\n",
      "Epoch [1/10], Step [140/600], Loss: 3.8760, batch time: 1.05, accuracy:  5.00%\n",
      "Epoch [1/10], Step [141/600], Loss: 3.9413, batch time: 1.40, accuracy:  6.00%\n",
      "Epoch [1/10], Step [142/600], Loss: 3.8590, batch time: 1.05, accuracy:  8.00%\n",
      "Epoch [1/10], Step [143/600], Loss: 4.0045, batch time: 1.21, accuracy:  5.00%\n",
      "Epoch [1/10], Step [144/600], Loss: 3.4273, batch time: 1.23, accuracy:  6.00%\n",
      "Epoch [1/10], Step [145/600], Loss: 3.5562, batch time: 1.12, accuracy:  5.00%\n",
      "Epoch [1/10], Step [146/600], Loss: 3.5935, batch time: 0.96, accuracy:  3.00%\n",
      "Epoch [1/10], Step [147/600], Loss: 3.6272, batch time: 1.08, accuracy:  4.00%\n",
      "Epoch [1/10], Step [148/600], Loss: 3.6930, batch time: 1.13, accuracy:  7.00%\n",
      "Epoch [1/10], Step [149/600], Loss: 3.5852, batch time: 1.28, accuracy:  9.00%\n",
      "Epoch [1/10], Step [150/600], Loss: 3.4528, batch time: 1.20, accuracy:  13.00%\n",
      "Epoch [1/10], Step [151/600], Loss: 3.7376, batch time: 1.10, accuracy:  8.00%\n",
      "Epoch [1/10], Step [152/600], Loss: 3.3385, batch time: 1.14, accuracy:  6.00%\n",
      "Epoch [1/10], Step [153/600], Loss: 3.6999, batch time: 1.41, accuracy:  3.00%\n",
      "Epoch [1/10], Step [154/600], Loss: 3.6352, batch time: 1.05, accuracy:  9.00%\n",
      "Epoch [1/10], Step [155/600], Loss: 3.1340, batch time: 1.36, accuracy:  8.00%\n",
      "Epoch [1/10], Step [156/600], Loss: 3.8737, batch time: 1.35, accuracy:  7.00%\n",
      "Epoch [1/10], Step [157/600], Loss: 3.9705, batch time: 1.30, accuracy:  4.00%\n",
      "Epoch [1/10], Step [158/600], Loss: 3.2858, batch time: 1.21, accuracy:  6.00%\n",
      "Epoch [1/10], Step [159/600], Loss: 3.7553, batch time: 1.11, accuracy:  3.00%\n",
      "Epoch [1/10], Step [160/600], Loss: 3.4777, batch time: 1.14, accuracy:  5.00%\n",
      "Epoch [1/10], Step [161/600], Loss: 3.6312, batch time: 1.22, accuracy:  7.00%\n",
      "Epoch [1/10], Step [162/600], Loss: 3.3353, batch time: 1.12, accuracy:  6.00%\n",
      "Epoch [1/10], Step [163/600], Loss: 3.3499, batch time: 1.19, accuracy:  5.00%\n",
      "Epoch [1/10], Step [164/600], Loss: 3.3993, batch time: 1.28, accuracy:  10.00%\n",
      "Epoch [1/10], Step [165/600], Loss: 3.2672, batch time: 1.25, accuracy:  6.00%\n",
      "Epoch [1/10], Step [166/600], Loss: 3.4748, batch time: 0.98, accuracy:  6.00%\n",
      "Epoch [1/10], Step [167/600], Loss: 3.3637, batch time: 0.93, accuracy:  5.00%\n",
      "Epoch [1/10], Step [168/600], Loss: 3.6554, batch time: 1.32, accuracy:  3.00%\n",
      "Epoch [1/10], Step [169/600], Loss: 3.3496, batch time: 1.17, accuracy:  4.00%\n",
      "Epoch [1/10], Step [170/600], Loss: 3.5548, batch time: 1.22, accuracy:  4.00%\n",
      "Epoch [1/10], Step [171/600], Loss: 3.4492, batch time: 1.15, accuracy:  8.00%\n",
      "Epoch [1/10], Step [172/600], Loss: 3.3707, batch time: 1.25, accuracy:  6.00%\n",
      "Epoch [1/10], Step [173/600], Loss: 3.2500, batch time: 1.03, accuracy:  7.00%\n",
      "Epoch [1/10], Step [174/600], Loss: 3.3068, batch time: 0.99, accuracy:  7.00%\n",
      "Epoch [1/10], Step [175/600], Loss: 3.2590, batch time: 1.14, accuracy:  12.00%\n",
      "Epoch [1/10], Step [176/600], Loss: 3.4600, batch time: 1.16, accuracy:  4.00%\n",
      "Epoch [1/10], Step [177/600], Loss: 3.3249, batch time: 1.45, accuracy:  5.00%\n",
      "Epoch [1/10], Step [178/600], Loss: 3.4023, batch time: 0.93, accuracy:  8.00%\n",
      "Epoch [1/10], Step [179/600], Loss: 3.2157, batch time: 1.10, accuracy:  11.00%\n",
      "Epoch [1/10], Step [180/600], Loss: 3.4204, batch time: 1.07, accuracy:  7.00%\n",
      "Epoch [1/10], Step [181/600], Loss: 3.2556, batch time: 1.06, accuracy:  7.00%\n",
      "Epoch [1/10], Step [182/600], Loss: 3.2153, batch time: 1.19, accuracy:  5.00%\n",
      "Epoch [1/10], Step [183/600], Loss: 3.5595, batch time: 1.08, accuracy:  7.00%\n",
      "Epoch [1/10], Step [184/600], Loss: 3.3235, batch time: 1.18, accuracy:  2.00%\n",
      "Epoch [1/10], Step [185/600], Loss: 3.1451, batch time: 1.41, accuracy:  4.00%\n",
      "Epoch [1/10], Step [186/600], Loss: 3.4103, batch time: 1.04, accuracy:  5.00%\n",
      "Epoch [1/10], Step [187/600], Loss: 3.4351, batch time: 0.87, accuracy:  5.00%\n",
      "Epoch [1/10], Step [188/600], Loss: 3.2684, batch time: 0.66, accuracy:  3.00%\n",
      "Epoch [1/10], Step [189/600], Loss: 3.2885, batch time: 0.64, accuracy:  6.00%\n",
      "Epoch [1/10], Step [190/600], Loss: 3.1832, batch time: 0.65, accuracy:  3.00%\n",
      "Epoch [1/10], Step [191/600], Loss: 3.3436, batch time: 0.70, accuracy:  5.00%\n",
      "Epoch [1/10], Step [192/600], Loss: 3.0111, batch time: 0.75, accuracy:  6.00%\n",
      "Epoch [1/10], Step [193/600], Loss: 3.2898, batch time: 0.74, accuracy:  4.00%\n",
      "Epoch [1/10], Step [194/600], Loss: 3.1718, batch time: 0.75, accuracy:  6.00%\n",
      "Epoch [1/10], Step [195/600], Loss: 3.4983, batch time: 0.76, accuracy:  7.00%\n",
      "Epoch [1/10], Step [196/600], Loss: 3.4793, batch time: 0.75, accuracy:  2.00%\n",
      "Epoch [1/10], Step [197/600], Loss: 2.9941, batch time: 0.75, accuracy:  5.00%\n",
      "Epoch [1/10], Step [198/600], Loss: 3.4707, batch time: 0.75, accuracy:  3.00%\n",
      "Epoch [1/10], Step [199/600], Loss: 2.9998, batch time: 0.75, accuracy:  11.00%\n",
      "Epoch [1/10], Step [200/600], Loss: 3.1513, batch time: 0.69, accuracy:  2.00%\n",
      "Epoch [1/10], Step [201/600], Loss: 3.2424, batch time: 0.73, accuracy:  8.00%\n",
      "Epoch [1/10], Step [202/600], Loss: 3.3687, batch time: 0.82, accuracy:  4.00%\n",
      "Epoch [1/10], Step [203/600], Loss: 3.1591, batch time: 0.75, accuracy:  4.00%\n",
      "Epoch [1/10], Step [204/600], Loss: 3.0602, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [1/10], Step [205/600], Loss: 3.2935, batch time: 0.73, accuracy:  5.00%\n",
      "Epoch [1/10], Step [206/600], Loss: 3.1538, batch time: 0.72, accuracy:  3.00%\n",
      "Epoch [1/10], Step [207/600], Loss: 3.1605, batch time: 0.67, accuracy:  3.00%\n",
      "Epoch [1/10], Step [208/600], Loss: 3.1228, batch time: 0.75, accuracy:  9.00%\n",
      "Epoch [1/10], Step [209/600], Loss: 3.0711, batch time: 0.74, accuracy:  3.00%\n",
      "Epoch [1/10], Step [210/600], Loss: 3.0136, batch time: 0.77, accuracy:  8.00%\n",
      "Epoch [1/10], Step [211/600], Loss: 3.0400, batch time: 0.77, accuracy:  6.00%\n",
      "Epoch [1/10], Step [212/600], Loss: 3.0416, batch time: 0.75, accuracy:  4.00%\n",
      "Epoch [1/10], Step [213/600], Loss: 2.8907, batch time: 0.76, accuracy:  4.00%\n",
      "Epoch [1/10], Step [214/600], Loss: 3.0379, batch time: 0.88, accuracy:  9.00%\n",
      "Epoch [1/10], Step [215/600], Loss: 3.2475, batch time: 0.73, accuracy:  2.00%\n",
      "Epoch [1/10], Step [216/600], Loss: 3.0286, batch time: 0.78, accuracy:  4.00%\n",
      "Epoch [1/10], Step [217/600], Loss: 3.2636, batch time: 0.74, accuracy:  5.00%\n",
      "Epoch [1/10], Step [218/600], Loss: 3.1202, batch time: 0.84, accuracy:  5.00%\n",
      "Epoch [1/10], Step [219/600], Loss: 2.8963, batch time: 0.65, accuracy:  3.00%\n",
      "Epoch [1/10], Step [220/600], Loss: 3.0672, batch time: 0.66, accuracy:  8.00%\n",
      "Epoch [1/10], Step [221/600], Loss: 2.9220, batch time: 0.68, accuracy:  5.00%\n",
      "Epoch [1/10], Step [222/600], Loss: 2.7854, batch time: 0.74, accuracy:  9.00%\n",
      "Epoch [1/10], Step [223/600], Loss: 3.0494, batch time: 0.75, accuracy:  7.00%\n",
      "Epoch [1/10], Step [224/600], Loss: 2.9790, batch time: 0.67, accuracy:  3.00%\n",
      "Epoch [1/10], Step [225/600], Loss: 2.9308, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [1/10], Step [226/600], Loss: 2.9569, batch time: 0.74, accuracy:  5.00%\n",
      "Epoch [1/10], Step [227/600], Loss: 3.0896, batch time: 0.75, accuracy:  5.00%\n",
      "Epoch [1/10], Step [228/600], Loss: 3.1429, batch time: 0.91, accuracy:  7.00%\n",
      "Epoch [1/10], Step [229/600], Loss: 3.2112, batch time: 0.76, accuracy:  5.00%\n",
      "Epoch [1/10], Step [230/600], Loss: 2.9024, batch time: 0.76, accuracy:  9.00%\n",
      "Epoch [1/10], Step [231/600], Loss: 2.9906, batch time: 0.65, accuracy:  5.00%\n",
      "Epoch [1/10], Step [232/600], Loss: 3.2579, batch time: 0.65, accuracy:  3.00%\n",
      "Epoch [1/10], Step [233/600], Loss: 3.0019, batch time: 0.73, accuracy:  3.00%\n",
      "Epoch [1/10], Step [234/600], Loss: 2.9681, batch time: 0.73, accuracy:  6.00%\n",
      "Epoch [1/10], Step [235/600], Loss: 2.8814, batch time: 0.73, accuracy:  10.00%\n",
      "Epoch [1/10], Step [236/600], Loss: 2.9726, batch time: 0.74, accuracy:  8.00%\n",
      "Epoch [1/10], Step [237/600], Loss: 2.9804, batch time: 0.73, accuracy:  3.00%\n",
      "Epoch [1/10], Step [238/600], Loss: 2.7781, batch time: 0.74, accuracy:  4.00%\n",
      "Epoch [1/10], Step [239/600], Loss: 3.0537, batch time: 0.74, accuracy:  7.00%\n",
      "Epoch [1/10], Step [240/600], Loss: 2.7294, batch time: 0.74, accuracy:  10.00%\n",
      "Epoch [1/10], Step [241/600], Loss: 2.9817, batch time: 0.75, accuracy:  2.00%\n",
      "Epoch [1/10], Step [242/600], Loss: 3.0554, batch time: 0.74, accuracy:  3.00%\n",
      "Epoch [1/10], Step [243/600], Loss: 2.8492, batch time: 0.75, accuracy:  8.00%\n",
      "Epoch [1/10], Step [244/600], Loss: 3.0010, batch time: 0.76, accuracy:  8.00%\n",
      "Epoch [1/10], Step [245/600], Loss: 2.8383, batch time: 0.80, accuracy:  6.00%\n",
      "Epoch [1/10], Step [246/600], Loss: 2.8403, batch time: 0.74, accuracy:  8.00%\n",
      "Epoch [1/10], Step [247/600], Loss: 2.9112, batch time: 0.75, accuracy:  3.00%\n",
      "Epoch [1/10], Step [248/600], Loss: 2.9664, batch time: 0.74, accuracy:  5.00%\n",
      "Epoch [1/10], Step [249/600], Loss: 3.0001, batch time: 0.74, accuracy:  5.00%\n",
      "Epoch [1/10], Step [250/600], Loss: 2.8099, batch time: 0.87, accuracy:  10.00%\n",
      "Epoch [1/10], Step [251/600], Loss: 2.8003, batch time: 0.72, accuracy:  4.00%\n",
      "Epoch [1/10], Step [252/600], Loss: 3.0609, batch time: 0.73, accuracy:  3.00%\n",
      "Epoch [1/10], Step [253/600], Loss: 2.9095, batch time: 0.73, accuracy:  9.00%\n",
      "Epoch [1/10], Step [254/600], Loss: 2.8326, batch time: 0.75, accuracy:  6.00%\n",
      "Epoch [1/10], Step [255/600], Loss: 2.8225, batch time: 0.75, accuracy:  7.00%\n",
      "Epoch [1/10], Step [256/600], Loss: 2.9910, batch time: 0.75, accuracy:  5.00%\n",
      "Epoch [1/10], Step [257/600], Loss: 2.8889, batch time: 0.80, accuracy:  6.00%\n",
      "Epoch [1/10], Step [258/600], Loss: 2.9137, batch time: 0.73, accuracy:  4.00%\n",
      "Epoch [1/10], Step [259/600], Loss: 2.8833, batch time: 0.73, accuracy:  5.00%\n",
      "Epoch [1/10], Step [260/600], Loss: 2.7690, batch time: 0.73, accuracy:  3.00%\n",
      "Epoch [1/10], Step [261/600], Loss: 2.6837, batch time: 0.70, accuracy:  8.00%\n",
      "Epoch [1/10], Step [262/600], Loss: 2.9720, batch time: 0.62, accuracy:  3.00%\n",
      "Epoch [1/10], Step [263/600], Loss: 2.8666, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [1/10], Step [264/600], Loss: 2.8315, batch time: 0.83, accuracy:  6.00%\n",
      "Epoch [1/10], Step [265/600], Loss: 2.7667, batch time: 0.74, accuracy:  6.00%\n",
      "Epoch [1/10], Step [266/600], Loss: 2.6718, batch time: 0.75, accuracy:  6.00%\n",
      "Epoch [1/10], Step [267/600], Loss: 2.8299, batch time: 0.74, accuracy:  1.00%\n",
      "Epoch [1/10], Step [268/600], Loss: 2.7939, batch time: 0.67, accuracy:  4.00%\n",
      "Epoch [1/10], Step [269/600], Loss: 2.7622, batch time: 0.73, accuracy:  6.00%\n",
      "Epoch [1/10], Step [270/600], Loss: 2.8576, batch time: 0.66, accuracy:  10.00%\n",
      "Epoch [1/10], Step [271/600], Loss: 2.8587, batch time: 0.65, accuracy:  4.00%\n",
      "Epoch [1/10], Step [272/600], Loss: 2.6592, batch time: 0.66, accuracy:  5.00%\n",
      "Epoch [1/10], Step [273/600], Loss: 2.8143, batch time: 0.72, accuracy:  6.00%\n",
      "Epoch [1/10], Step [274/600], Loss: 2.5468, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [1/10], Step [275/600], Loss: 2.7236, batch time: 0.74, accuracy:  8.00%\n",
      "Epoch [1/10], Step [276/600], Loss: 2.8054, batch time: 0.74, accuracy:  5.00%\n",
      "Epoch [1/10], Step [277/600], Loss: 2.8471, batch time: 0.82, accuracy:  5.00%\n",
      "Epoch [1/10], Step [278/600], Loss: 2.8062, batch time: 0.68, accuracy:  4.00%\n",
      "Epoch [1/10], Step [279/600], Loss: 2.6528, batch time: 0.78, accuracy:  11.00%\n",
      "Epoch [1/10], Step [280/600], Loss: 2.9407, batch time: 0.74, accuracy:  2.00%\n",
      "Epoch [1/10], Step [281/600], Loss: 2.7553, batch time: 0.74, accuracy:  5.00%\n",
      "Epoch [1/10], Step [282/600], Loss: 2.6661, batch time: 0.75, accuracy:  2.00%\n",
      "Epoch [1/10], Step [283/600], Loss: 2.8417, batch time: 0.74, accuracy:  4.00%\n",
      "Epoch [1/10], Step [284/600], Loss: 2.7117, batch time: 0.74, accuracy:  6.00%\n",
      "Epoch [1/10], Step [285/600], Loss: 2.6637, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [1/10], Step [286/600], Loss: 2.8450, batch time: 0.69, accuracy:  3.00%\n",
      "Epoch [1/10], Step [287/600], Loss: 2.5987, batch time: 0.66, accuracy:  13.00%\n",
      "Epoch [1/10], Step [288/600], Loss: 2.7323, batch time: 0.70, accuracy:  5.00%\n",
      "Epoch [1/10], Step [289/600], Loss: 2.7176, batch time: 0.73, accuracy:  8.00%\n",
      "Epoch [1/10], Step [290/600], Loss: 2.6560, batch time: 0.74, accuracy:  5.00%\n",
      "Epoch [1/10], Step [291/600], Loss: 2.7072, batch time: 0.75, accuracy:  4.00%\n",
      "Epoch [1/10], Step [292/600], Loss: 2.7543, batch time: 0.74, accuracy:  5.00%\n",
      "Epoch [1/10], Step [293/600], Loss: 2.5489, batch time: 0.73, accuracy:  12.00%\n",
      "Epoch [1/10], Step [294/600], Loss: 2.7883, batch time: 0.78, accuracy:  6.00%\n",
      "Epoch [1/10], Step [295/600], Loss: 2.8618, batch time: 0.72, accuracy:  4.00%\n",
      "Epoch [1/10], Step [296/600], Loss: 2.7136, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [1/10], Step [297/600], Loss: 2.7643, batch time: 0.70, accuracy:  3.00%\n",
      "Epoch [1/10], Step [298/600], Loss: 2.8348, batch time: 0.73, accuracy:  5.00%\n",
      "Epoch [1/10], Step [299/600], Loss: 2.6179, batch time: 0.73, accuracy:  8.00%\n",
      "Epoch [1/10], Step [300/600], Loss: 2.6345, batch time: 0.74, accuracy:  6.00%\n",
      "Epoch [1/10], Step [301/600], Loss: 2.6390, batch time: 0.75, accuracy:  3.00%\n",
      "Epoch [1/10], Step [302/600], Loss: 2.6425, batch time: 0.69, accuracy:  5.00%\n",
      "Epoch [1/10], Step [303/600], Loss: 2.7346, batch time: 0.64, accuracy:  4.00%\n",
      "Epoch [1/10], Step [304/600], Loss: 2.5956, batch time: 0.65, accuracy:  11.00%\n",
      "Epoch [1/10], Step [305/600], Loss: 2.6639, batch time: 0.71, accuracy:  6.00%\n",
      "Epoch [1/10], Step [306/600], Loss: 2.7038, batch time: 0.67, accuracy:  4.00%\n",
      "Epoch [1/10], Step [307/600], Loss: 2.6628, batch time: 0.64, accuracy:  1.00%\n",
      "Epoch [1/10], Step [308/600], Loss: 2.6584, batch time: 0.65, accuracy:  6.00%\n",
      "Epoch [1/10], Step [309/600], Loss: 2.7673, batch time: 0.71, accuracy:  6.00%\n",
      "Epoch [1/10], Step [310/600], Loss: 2.6822, batch time: 0.75, accuracy:  3.00%\n",
      "Epoch [1/10], Step [311/600], Loss: 2.6208, batch time: 0.74, accuracy:  6.00%\n",
      "Epoch [1/10], Step [312/600], Loss: 2.5448, batch time: 0.75, accuracy:  4.00%\n",
      "Epoch [1/10], Step [313/600], Loss: 2.7742, batch time: 0.75, accuracy:  4.00%\n",
      "Epoch [1/10], Step [314/600], Loss: 2.6516, batch time: 0.74, accuracy:  5.00%\n",
      "Epoch [1/10], Step [315/600], Loss: 2.6403, batch time: 0.64, accuracy:  5.00%\n",
      "Epoch [1/10], Step [316/600], Loss: 2.6390, batch time: 0.77, accuracy:  2.00%\n",
      "Epoch [1/10], Step [317/600], Loss: 2.7194, batch time: 0.73, accuracy:  7.00%\n",
      "Epoch [1/10], Step [318/600], Loss: 2.6627, batch time: 0.75, accuracy:  5.00%\n",
      "Epoch [1/10], Step [319/600], Loss: 2.6788, batch time: 0.75, accuracy:  3.00%\n",
      "Epoch [1/10], Step [320/600], Loss: 2.7422, batch time: 0.76, accuracy:  11.00%\n",
      "Epoch [1/10], Step [321/600], Loss: 2.6254, batch time: 0.74, accuracy:  4.00%\n",
      "Epoch [1/10], Step [322/600], Loss: 2.5866, batch time: 0.65, accuracy:  7.00%\n",
      "Epoch [1/10], Step [323/600], Loss: 2.6471, batch time: 0.65, accuracy:  5.00%\n",
      "Epoch [1/10], Step [324/600], Loss: 2.5507, batch time: 0.71, accuracy:  7.00%\n",
      "Epoch [1/10], Step [325/600], Loss: 2.6744, batch time: 0.67, accuracy:  5.00%\n",
      "Epoch [1/10], Step [326/600], Loss: 2.5605, batch time: 0.73, accuracy:  5.00%\n",
      "Epoch [1/10], Step [327/600], Loss: 2.6476, batch time: 0.74, accuracy:  5.00%\n",
      "Epoch [1/10], Step [328/600], Loss: 2.6292, batch time: 0.89, accuracy:  9.00%\n",
      "Epoch [1/10], Step [329/600], Loss: 2.6391, batch time: 0.74, accuracy:  8.00%\n",
      "Epoch [1/10], Step [330/600], Loss: 2.6798, batch time: 0.73, accuracy:  6.00%\n",
      "Epoch [1/10], Step [331/600], Loss: 2.5613, batch time: 0.74, accuracy:  10.00%\n",
      "Epoch [1/10], Step [332/600], Loss: 2.6612, batch time: 0.73, accuracy:  4.00%\n",
      "Epoch [1/10], Step [333/600], Loss: 2.6286, batch time: 0.75, accuracy:  8.00%\n",
      "Epoch [1/10], Step [334/600], Loss: 2.7108, batch time: 0.74, accuracy:  4.00%\n",
      "Epoch [1/10], Step [335/600], Loss: 2.6274, batch time: 0.75, accuracy:  7.00%\n",
      "Epoch [1/10], Step [336/600], Loss: 2.6857, batch time: 0.74, accuracy:  6.00%\n",
      "Epoch [1/10], Step [337/600], Loss: 2.6319, batch time: 0.78, accuracy:  8.00%\n",
      "Epoch [1/10], Step [338/600], Loss: 2.6008, batch time: 0.74, accuracy:  8.00%\n",
      "Epoch [1/10], Step [339/600], Loss: 2.6582, batch time: 0.74, accuracy:  4.00%\n",
      "Epoch [1/10], Step [340/600], Loss: 2.6235, batch time: 0.74, accuracy:  6.00%\n",
      "Epoch [1/10], Step [341/600], Loss: 2.5542, batch time: 0.74, accuracy:  6.00%\n",
      "Epoch [1/10], Step [342/600], Loss: 2.5665, batch time: 0.75, accuracy:  6.00%\n",
      "Epoch [1/10], Step [343/600], Loss: 2.6216, batch time: 0.74, accuracy:  5.00%\n",
      "Epoch [1/10], Step [344/600], Loss: 2.6290, batch time: 0.74, accuracy:  4.00%\n",
      "Epoch [1/10], Step [345/600], Loss: 2.4837, batch time: 0.88, accuracy:  12.00%\n",
      "Epoch [1/10], Step [346/600], Loss: 2.6592, batch time: 0.73, accuracy:  6.00%\n",
      "Epoch [1/10], Step [347/600], Loss: 2.5283, batch time: 0.73, accuracy:  5.00%\n",
      "Epoch [1/10], Step [348/600], Loss: 2.6241, batch time: 0.76, accuracy:  6.00%\n",
      "Epoch [1/10], Step [349/600], Loss: 2.6653, batch time: 0.71, accuracy:  3.00%\n",
      "Epoch [1/10], Step [350/600], Loss: 2.5323, batch time: 0.74, accuracy:  8.00%\n",
      "Epoch [1/10], Step [351/600], Loss: 2.5363, batch time: 0.75, accuracy:  8.00%\n",
      "Epoch [1/10], Step [352/600], Loss: 2.6081, batch time: 0.73, accuracy:  7.00%\n",
      "Epoch [1/10], Step [353/600], Loss: 2.5448, batch time: 0.75, accuracy:  7.00%\n",
      "Epoch [1/10], Step [354/600], Loss: 2.6363, batch time: 0.74, accuracy:  1.00%\n",
      "Epoch [1/10], Step [355/600], Loss: 2.5612, batch time: 0.88, accuracy:  8.00%\n",
      "Epoch [1/10], Step [356/600], Loss: 2.5310, batch time: 0.72, accuracy:  6.00%\n",
      "Epoch [1/10], Step [357/600], Loss: 2.7061, batch time: 0.66, accuracy:  4.00%\n",
      "Epoch [1/10], Step [358/600], Loss: 2.7093, batch time: 0.74, accuracy:  6.00%\n",
      "Epoch [1/10], Step [359/600], Loss: 2.6537, batch time: 0.64, accuracy:  5.00%\n",
      "Epoch [1/10], Step [360/600], Loss: 2.5600, batch time: 0.69, accuracy:  7.00%\n",
      "Epoch [1/10], Step [361/600], Loss: 2.5090, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [1/10], Step [362/600], Loss: 2.5269, batch time: 0.76, accuracy:  4.00%\n",
      "Epoch [1/10], Step [363/600], Loss: 2.5443, batch time: 0.74, accuracy:  4.00%\n",
      "Epoch [1/10], Step [364/600], Loss: 2.5998, batch time: 0.75, accuracy:  5.00%\n",
      "Epoch [1/10], Step [365/600], Loss: 2.6682, batch time: 0.74, accuracy:  4.00%\n",
      "Epoch [1/10], Step [366/600], Loss: 2.5921, batch time: 0.74, accuracy:  7.00%\n",
      "Epoch [1/10], Step [367/600], Loss: 2.5762, batch time: 0.79, accuracy:  3.00%\n",
      "Epoch [1/10], Step [368/600], Loss: 2.5487, batch time: 0.74, accuracy:  7.00%\n",
      "Epoch [1/10], Step [369/600], Loss: 2.5369, batch time: 0.73, accuracy:  6.00%\n",
      "Epoch [1/10], Step [370/600], Loss: 2.5770, batch time: 0.66, accuracy:  5.00%\n",
      "Epoch [1/10], Step [371/600], Loss: 2.5360, batch time: 0.79, accuracy:  9.00%\n",
      "Epoch [1/10], Step [372/600], Loss: 2.5331, batch time: 0.73, accuracy:  7.00%\n",
      "Epoch [1/10], Step [373/600], Loss: 2.5809, batch time: 0.74, accuracy:  8.00%\n",
      "Epoch [1/10], Step [374/600], Loss: 2.4811, batch time: 0.75, accuracy:  11.00%\n",
      "Epoch [1/10], Step [375/600], Loss: 2.5896, batch time: 0.75, accuracy:  7.00%\n",
      "Epoch [1/10], Step [376/600], Loss: 2.4969, batch time: 0.74, accuracy:  7.00%\n",
      "Epoch [1/10], Step [377/600], Loss: 2.5577, batch time: 0.75, accuracy:  4.00%\n",
      "Epoch [1/10], Step [378/600], Loss: 2.5228, batch time: 0.74, accuracy:  9.00%\n",
      "Epoch [1/10], Step [379/600], Loss: 2.5858, batch time: 0.72, accuracy:  7.00%\n",
      "Epoch [1/10], Step [380/600], Loss: 2.5814, batch time: 0.72, accuracy:  5.00%\n",
      "Epoch [1/10], Step [381/600], Loss: 2.6486, batch time: 0.73, accuracy:  2.00%\n",
      "Epoch [1/10], Step [382/600], Loss: 2.6253, batch time: 0.69, accuracy:  6.00%\n",
      "Epoch [1/10], Step [383/600], Loss: 2.5799, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [1/10], Step [384/600], Loss: 2.4847, batch time: 0.72, accuracy:  15.00%\n",
      "Epoch [1/10], Step [385/600], Loss: 2.5114, batch time: 0.65, accuracy:  4.00%\n",
      "Epoch [1/10], Step [386/600], Loss: 2.5853, batch time: 0.73, accuracy:  6.00%\n",
      "Epoch [1/10], Step [387/600], Loss: 2.5827, batch time: 0.77, accuracy:  3.00%\n",
      "Epoch [1/10], Step [388/600], Loss: 2.5711, batch time: 0.87, accuracy:  5.00%\n",
      "Epoch [1/10], Step [389/600], Loss: 2.6076, batch time: 0.74, accuracy:  3.00%\n",
      "Epoch [1/10], Step [390/600], Loss: 2.5767, batch time: 0.74, accuracy:  6.00%\n",
      "Epoch [1/10], Step [391/600], Loss: 2.4547, batch time: 0.73, accuracy:  8.00%\n",
      "Epoch [1/10], Step [392/600], Loss: 2.5622, batch time: 0.75, accuracy:  9.00%\n",
      "Epoch [1/10], Step [393/600], Loss: 2.5080, batch time: 0.73, accuracy:  5.00%\n",
      "Epoch [1/10], Step [394/600], Loss: 2.5407, batch time: 0.73, accuracy:  5.00%\n",
      "Epoch [1/10], Step [395/600], Loss: 2.5878, batch time: 0.74, accuracy:  9.00%\n",
      "Epoch [1/10], Step [396/600], Loss: 2.5176, batch time: 0.80, accuracy:  2.00%\n",
      "Epoch [1/10], Step [397/600], Loss: 2.4864, batch time: 0.63, accuracy:  6.00%\n",
      "Epoch [1/10], Step [398/600], Loss: 2.4599, batch time: 0.64, accuracy:  7.00%\n",
      "Epoch [1/10], Step [399/600], Loss: 2.5623, batch time: 0.65, accuracy:  6.00%\n",
      "Epoch [1/10], Step [400/600], Loss: 2.5281, batch time: 0.78, accuracy:  5.00%\n",
      "Epoch [1/10], Step [401/600], Loss: 2.6034, batch time: 0.84, accuracy:  5.00%\n",
      "Epoch [1/10], Step [402/600], Loss: 2.5366, batch time: 0.64, accuracy:  4.00%\n",
      "Epoch [1/10], Step [403/600], Loss: 2.5689, batch time: 0.63, accuracy:  6.00%\n",
      "Epoch [1/10], Step [404/600], Loss: 2.5070, batch time: 0.64, accuracy:  3.00%\n",
      "Epoch [1/10], Step [405/600], Loss: 2.5324, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [1/10], Step [406/600], Loss: 2.5478, batch time: 0.85, accuracy:  6.00%\n",
      "Epoch [1/10], Step [407/600], Loss: 2.5849, batch time: 0.73, accuracy:  4.00%\n",
      "Epoch [1/10], Step [408/600], Loss: 2.5654, batch time: 0.73, accuracy:  7.00%\n",
      "Epoch [1/10], Step [409/600], Loss: 2.5404, batch time: 0.73, accuracy:  5.00%\n",
      "Epoch [1/10], Step [410/600], Loss: 2.5260, batch time: 0.73, accuracy:  9.00%\n",
      "Epoch [1/10], Step [411/600], Loss: 2.4335, batch time: 0.65, accuracy:  8.00%\n",
      "Epoch [1/10], Step [412/600], Loss: 2.5749, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [1/10], Step [413/600], Loss: 2.5616, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [1/10], Step [414/600], Loss: 2.5663, batch time: 0.63, accuracy:  2.00%\n",
      "Epoch [1/10], Step [415/600], Loss: 2.5603, batch time: 0.64, accuracy:  0.00%\n",
      "Epoch [1/10], Step [416/600], Loss: 2.5826, batch time: 0.63, accuracy:  2.00%\n",
      "Epoch [1/10], Step [417/600], Loss: 2.5696, batch time: 0.62, accuracy:  1.00%\n",
      "Epoch [1/10], Step [418/600], Loss: 2.4996, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [1/10], Step [419/600], Loss: 2.4363, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [1/10], Step [420/600], Loss: 2.4832, batch time: 0.63, accuracy:  5.00%\n",
      "Epoch [1/10], Step [421/600], Loss: 2.6862, batch time: 0.69, accuracy:  2.00%\n",
      "Epoch [1/10], Step [422/600], Loss: 2.4437, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [1/10], Step [423/600], Loss: 2.4405, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [1/10], Step [424/600], Loss: 2.5194, batch time: 0.63, accuracy:  3.00%\n",
      "Epoch [1/10], Step [425/600], Loss: 2.5597, batch time: 0.63, accuracy:  1.00%\n",
      "Epoch [1/10], Step [426/600], Loss: 2.5348, batch time: 0.64, accuracy:  4.00%\n",
      "Epoch [1/10], Step [427/600], Loss: 2.4769, batch time: 0.64, accuracy:  7.00%\n",
      "Epoch [1/10], Step [428/600], Loss: 2.4953, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [1/10], Step [429/600], Loss: 2.5621, batch time: 0.63, accuracy:  5.00%\n",
      "Epoch [1/10], Step [430/600], Loss: 2.5485, batch time: 0.63, accuracy:  3.00%\n",
      "Epoch [1/10], Step [431/600], Loss: 2.4607, batch time: 0.63, accuracy:  5.00%\n",
      "Epoch [1/10], Step [432/600], Loss: 2.4197, batch time: 0.70, accuracy:  5.00%\n",
      "Epoch [1/10], Step [433/600], Loss: 2.4650, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [1/10], Step [434/600], Loss: 2.5193, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [1/10], Step [435/600], Loss: 2.4848, batch time: 0.61, accuracy:  10.00%\n",
      "Epoch [1/10], Step [436/600], Loss: 2.5134, batch time: 0.63, accuracy:  6.00%\n",
      "Epoch [1/10], Step [437/600], Loss: 2.5063, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [1/10], Step [438/600], Loss: 2.4902, batch time: 0.62, accuracy:  5.00%\n",
      "Epoch [1/10], Step [439/600], Loss: 2.4958, batch time: 0.62, accuracy:  4.00%\n",
      "Epoch [1/10], Step [440/600], Loss: 2.4078, batch time: 0.66, accuracy:  4.00%\n",
      "Epoch [1/10], Step [441/600], Loss: 2.4325, batch time: 0.63, accuracy:  6.00%\n",
      "Epoch [1/10], Step [442/600], Loss: 2.4394, batch time: 0.63, accuracy:  10.00%\n",
      "Epoch [1/10], Step [443/600], Loss: 2.4394, batch time: 0.64, accuracy:  5.00%\n",
      "Epoch [1/10], Step [444/600], Loss: 2.4458, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [1/10], Step [445/600], Loss: 2.4664, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [1/10], Step [446/600], Loss: 2.5042, batch time: 0.64, accuracy:  6.00%\n",
      "Epoch [1/10], Step [447/600], Loss: 2.5403, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [1/10], Step [448/600], Loss: 2.4597, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [1/10], Step [449/600], Loss: 2.4986, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [1/10], Step [450/600], Loss: 2.5607, batch time: 0.63, accuracy:  4.00%\n",
      "Epoch [1/10], Step [451/600], Loss: 2.4476, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [1/10], Step [452/600], Loss: 2.5126, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [1/10], Step [453/600], Loss: 2.4240, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [1/10], Step [454/600], Loss: 2.5450, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [1/10], Step [455/600], Loss: 2.5185, batch time: 0.62, accuracy:  4.00%\n",
      "Epoch [1/10], Step [456/600], Loss: 2.4753, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [1/10], Step [457/600], Loss: 2.5361, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [1/10], Step [458/600], Loss: 2.4588, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [1/10], Step [459/600], Loss: 2.4593, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [1/10], Step [460/600], Loss: 2.5442, batch time: 0.66, accuracy:  5.00%\n",
      "Epoch [1/10], Step [461/600], Loss: 2.4730, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [1/10], Step [462/600], Loss: 2.5231, batch time: 0.62, accuracy:  2.00%\n",
      "Epoch [1/10], Step [463/600], Loss: 2.4624, batch time: 0.63, accuracy:  6.00%\n",
      "Epoch [1/10], Step [464/600], Loss: 2.5265, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [1/10], Step [465/600], Loss: 2.4376, batch time: 0.62, accuracy:  5.00%\n",
      "Epoch [1/10], Step [466/600], Loss: 2.4778, batch time: 0.61, accuracy:  4.00%\n",
      "Epoch [1/10], Step [467/600], Loss: 2.4208, batch time: 0.62, accuracy:  13.00%\n",
      "Epoch [1/10], Step [468/600], Loss: 2.5115, batch time: 0.62, accuracy:  5.00%\n",
      "Epoch [1/10], Step [469/600], Loss: 2.4725, batch time: 0.64, accuracy:  7.00%\n",
      "Epoch [1/10], Step [470/600], Loss: 2.4711, batch time: 0.64, accuracy:  8.00%\n",
      "Epoch [1/10], Step [471/600], Loss: 2.5135, batch time: 0.63, accuracy:  6.00%\n",
      "Epoch [1/10], Step [472/600], Loss: 2.4298, batch time: 0.62, accuracy:  4.00%\n",
      "Epoch [1/10], Step [473/600], Loss: 2.4628, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [1/10], Step [474/600], Loss: 2.5380, batch time: 0.64, accuracy:  6.00%\n",
      "Epoch [1/10], Step [475/600], Loss: 2.4221, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [1/10], Step [476/600], Loss: 2.4246, batch time: 0.63, accuracy:  2.00%\n",
      "Epoch [1/10], Step [477/600], Loss: 2.4786, batch time: 0.63, accuracy:  6.00%\n",
      "Epoch [1/10], Step [478/600], Loss: 2.4767, batch time: 0.62, accuracy:  3.00%\n",
      "Epoch [1/10], Step [479/600], Loss: 2.4583, batch time: 0.67, accuracy:  5.00%\n",
      "Epoch [1/10], Step [480/600], Loss: 2.4986, batch time: 0.64, accuracy:  8.00%\n",
      "Epoch [1/10], Step [481/600], Loss: 2.4391, batch time: 0.61, accuracy:  6.00%\n",
      "Epoch [1/10], Step [482/600], Loss: 2.4502, batch time: 0.62, accuracy:  5.00%\n",
      "Epoch [1/10], Step [483/600], Loss: 2.5516, batch time: 0.62, accuracy:  4.00%\n",
      "Epoch [1/10], Step [484/600], Loss: 2.4363, batch time: 0.77, accuracy:  6.00%\n",
      "Epoch [1/10], Step [485/600], Loss: 2.5039, batch time: 0.62, accuracy:  3.00%\n",
      "Epoch [1/10], Step [486/600], Loss: 2.5015, batch time: 0.62, accuracy:  5.00%\n",
      "Epoch [1/10], Step [487/600], Loss: 2.4480, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [1/10], Step [488/600], Loss: 2.4269, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [1/10], Step [489/600], Loss: 2.4405, batch time: 0.63, accuracy:  4.00%\n",
      "Epoch [1/10], Step [490/600], Loss: 2.4542, batch time: 0.64, accuracy:  4.00%\n",
      "Epoch [1/10], Step [491/600], Loss: 2.4305, batch time: 0.63, accuracy:  5.00%\n",
      "Epoch [1/10], Step [492/600], Loss: 2.4622, batch time: 0.71, accuracy:  4.00%\n",
      "Epoch [1/10], Step [493/600], Loss: 2.4729, batch time: 0.64, accuracy:  15.00%\n",
      "Epoch [1/10], Step [494/600], Loss: 2.4481, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [1/10], Step [495/600], Loss: 2.4140, batch time: 0.64, accuracy:  8.00%\n",
      "Epoch [1/10], Step [496/600], Loss: 2.4337, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [1/10], Step [497/600], Loss: 2.4660, batch time: 0.67, accuracy:  8.00%\n",
      "Epoch [1/10], Step [498/600], Loss: 2.3858, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [1/10], Step [499/600], Loss: 2.4648, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [1/10], Step [500/600], Loss: 2.4563, batch time: 0.63, accuracy:  3.00%\n",
      "Epoch [1/10], Step [501/600], Loss: 2.4914, batch time: 0.62, accuracy:  5.00%\n",
      "Epoch [1/10], Step [502/600], Loss: 2.4696, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [1/10], Step [503/600], Loss: 2.4662, batch time: 0.61, accuracy:  5.00%\n",
      "Epoch [1/10], Step [504/600], Loss: 2.4775, batch time: 0.66, accuracy:  6.00%\n",
      "Epoch [1/10], Step [505/600], Loss: 2.5291, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [1/10], Step [506/600], Loss: 2.4165, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [1/10], Step [507/600], Loss: 2.4408, batch time: 0.62, accuracy:  4.00%\n",
      "Epoch [1/10], Step [508/600], Loss: 2.4946, batch time: 0.61, accuracy:  3.00%\n",
      "Epoch [1/10], Step [509/600], Loss: 2.4647, batch time: 0.61, accuracy:  7.00%\n",
      "Epoch [1/10], Step [510/600], Loss: 2.3967, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [1/10], Step [511/600], Loss: 2.4560, batch time: 0.63, accuracy:  5.00%\n",
      "Epoch [1/10], Step [512/600], Loss: 2.4389, batch time: 0.64, accuracy:  8.00%\n",
      "Epoch [1/10], Step [513/600], Loss: 2.4742, batch time: 0.64, accuracy:  4.00%\n",
      "Epoch [1/10], Step [514/600], Loss: 2.3777, batch time: 0.61, accuracy:  12.00%\n",
      "Epoch [1/10], Step [515/600], Loss: 2.4745, batch time: 0.61, accuracy:  8.00%\n",
      "Epoch [1/10], Step [516/600], Loss: 2.4838, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [1/10], Step [517/600], Loss: 2.4826, batch time: 0.61, accuracy:  4.00%\n",
      "Epoch [1/10], Step [518/600], Loss: 2.5196, batch time: 0.61, accuracy:  3.00%\n",
      "Epoch [1/10], Step [519/600], Loss: 2.4863, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [1/10], Step [520/600], Loss: 2.4409, batch time: 0.65, accuracy:  4.00%\n",
      "Epoch [1/10], Step [521/600], Loss: 2.4714, batch time: 0.62, accuracy:  5.00%\n",
      "Epoch [1/10], Step [522/600], Loss: 2.4659, batch time: 0.63, accuracy:  4.00%\n",
      "Epoch [1/10], Step [523/600], Loss: 2.4086, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [1/10], Step [524/600], Loss: 2.4227, batch time: 0.62, accuracy:  4.00%\n",
      "Epoch [1/10], Step [525/600], Loss: 2.4060, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [1/10], Step [526/600], Loss: 2.4666, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [1/10], Step [527/600], Loss: 2.4401, batch time: 0.73, accuracy:  7.00%\n",
      "Epoch [1/10], Step [528/600], Loss: 2.4337, batch time: 0.61, accuracy:  6.00%\n",
      "Epoch [1/10], Step [529/600], Loss: 2.4627, batch time: 0.63, accuracy:  6.00%\n",
      "Epoch [1/10], Step [530/600], Loss: 2.4397, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [1/10], Step [531/600], Loss: 2.4405, batch time: 0.61, accuracy:  7.00%\n",
      "Epoch [1/10], Step [532/600], Loss: 2.4376, batch time: 0.63, accuracy:  5.00%\n",
      "Epoch [1/10], Step [533/600], Loss: 2.5279, batch time: 0.62, accuracy:  3.00%\n",
      "Epoch [1/10], Step [534/600], Loss: 2.3809, batch time: 0.61, accuracy:  6.00%\n",
      "Epoch [1/10], Step [535/600], Loss: 2.4272, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [1/10], Step [536/600], Loss: 2.4642, batch time: 0.66, accuracy:  6.00%\n",
      "Epoch [1/10], Step [537/600], Loss: 2.4232, batch time: 0.74, accuracy:  7.00%\n",
      "Epoch [1/10], Step [538/600], Loss: 2.4932, batch time: 0.69, accuracy:  7.00%\n",
      "Epoch [1/10], Step [539/600], Loss: 2.3927, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [1/10], Step [540/600], Loss: 2.4260, batch time: 0.63, accuracy:  5.00%\n",
      "Epoch [1/10], Step [541/600], Loss: 2.4342, batch time: 0.64, accuracy:  7.00%\n",
      "Epoch [1/10], Step [542/600], Loss: 2.4160, batch time: 0.65, accuracy:  5.00%\n",
      "Epoch [1/10], Step [543/600], Loss: 2.4060, batch time: 0.64, accuracy:  5.00%\n",
      "Epoch [1/10], Step [544/600], Loss: 2.4372, batch time: 0.64, accuracy:  2.00%\n",
      "Epoch [1/10], Step [545/600], Loss: 2.4138, batch time: 0.64, accuracy:  3.00%\n",
      "Epoch [1/10], Step [546/600], Loss: 2.4793, batch time: 0.63, accuracy:  4.00%\n",
      "Epoch [1/10], Step [547/600], Loss: 2.3749, batch time: 0.61, accuracy:  5.00%\n",
      "Epoch [1/10], Step [548/600], Loss: 2.4089, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [1/10], Step [549/600], Loss: 2.3861, batch time: 0.61, accuracy:  6.00%\n",
      "Epoch [1/10], Step [550/600], Loss: 2.4012, batch time: 0.62, accuracy:  5.00%\n",
      "Epoch [1/10], Step [551/600], Loss: 2.4391, batch time: 0.63, accuracy:  4.00%\n",
      "Epoch [1/10], Step [552/600], Loss: 2.4218, batch time: 0.61, accuracy:  8.00%\n",
      "Epoch [1/10], Step [553/600], Loss: 2.4164, batch time: 0.63, accuracy:  5.00%\n",
      "Epoch [1/10], Step [554/600], Loss: 2.4342, batch time: 0.69, accuracy:  5.00%\n",
      "Epoch [1/10], Step [555/600], Loss: 2.4527, batch time: 0.66, accuracy:  8.00%\n",
      "Epoch [1/10], Step [556/600], Loss: 2.4449, batch time: 0.63, accuracy:  3.00%\n",
      "Epoch [1/10], Step [557/600], Loss: 2.4235, batch time: 0.64, accuracy:  9.00%\n",
      "Epoch [1/10], Step [558/600], Loss: 2.4600, batch time: 0.70, accuracy:  1.00%\n",
      "Epoch [1/10], Step [559/600], Loss: 2.4289, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [1/10], Step [560/600], Loss: 2.4050, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [1/10], Step [561/600], Loss: 2.4664, batch time: 0.64, accuracy:  5.00%\n",
      "Epoch [1/10], Step [562/600], Loss: 2.4667, batch time: 0.84, accuracy:  3.00%\n",
      "Epoch [1/10], Step [563/600], Loss: 2.4033, batch time: 0.73, accuracy:  6.00%\n",
      "Epoch [1/10], Step [564/600], Loss: 2.3829, batch time: 0.72, accuracy:  5.00%\n",
      "Epoch [1/10], Step [565/600], Loss: 2.3597, batch time: 0.70, accuracy:  5.00%\n",
      "Epoch [1/10], Step [566/600], Loss: 2.3937, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [1/10], Step [567/600], Loss: 2.3748, batch time: 0.61, accuracy:  8.00%\n",
      "Epoch [1/10], Step [568/600], Loss: 2.4393, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [1/10], Step [569/600], Loss: 2.4295, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [1/10], Step [570/600], Loss: 2.4796, batch time: 0.72, accuracy:  5.00%\n",
      "Epoch [1/10], Step [571/600], Loss: 2.3972, batch time: 0.72, accuracy:  10.00%\n",
      "Epoch [1/10], Step [572/600], Loss: 2.4293, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [1/10], Step [573/600], Loss: 2.4267, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [1/10], Step [574/600], Loss: 2.3898, batch time: 0.68, accuracy:  5.00%\n",
      "Epoch [1/10], Step [575/600], Loss: 2.3488, batch time: 0.61, accuracy:  10.00%\n",
      "Epoch [1/10], Step [576/600], Loss: 2.3988, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [1/10], Step [577/600], Loss: 2.4597, batch time: 0.73, accuracy:  6.00%\n",
      "Epoch [1/10], Step [578/600], Loss: 2.4074, batch time: 0.65, accuracy:  10.00%\n",
      "Epoch [1/10], Step [579/600], Loss: 2.4187, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [1/10], Step [580/600], Loss: 2.4347, batch time: 0.63, accuracy:  13.00%\n",
      "Epoch [1/10], Step [581/600], Loss: 2.4433, batch time: 0.66, accuracy:  6.00%\n",
      "Epoch [1/10], Step [582/600], Loss: 2.3961, batch time: 0.64, accuracy:  9.00%\n",
      "Epoch [1/10], Step [583/600], Loss: 2.3916, batch time: 0.64, accuracy:  5.00%\n",
      "Epoch [1/10], Step [584/600], Loss: 2.3919, batch time: 0.68, accuracy:  4.00%\n",
      "Epoch [1/10], Step [585/600], Loss: 2.4289, batch time: 0.64, accuracy:  7.00%\n",
      "Epoch [1/10], Step [586/600], Loss: 2.3231, batch time: 0.71, accuracy:  11.00%\n",
      "Epoch [1/10], Step [587/600], Loss: 2.4039, batch time: 0.71, accuracy:  9.00%\n",
      "Epoch [1/10], Step [588/600], Loss: 2.4122, batch time: 0.64, accuracy:  12.00%\n",
      "Epoch [1/10], Step [589/600], Loss: 2.4292, batch time: 0.72, accuracy:  12.00%\n",
      "Epoch [1/10], Step [590/600], Loss: 2.3918, batch time: 0.73, accuracy:  9.00%\n",
      "Epoch [1/10], Step [591/600], Loss: 2.3981, batch time: 0.73, accuracy:  8.00%\n",
      "Epoch [1/10], Step [592/600], Loss: 2.4122, batch time: 0.73, accuracy:  8.00%\n",
      "Epoch [1/10], Step [593/600], Loss: 2.4351, batch time: 0.74, accuracy:  7.00%\n",
      "Epoch [1/10], Step [594/600], Loss: 2.4434, batch time: 0.73, accuracy:  8.00%\n",
      "Epoch [1/10], Step [595/600], Loss: 2.4349, batch time: 0.73, accuracy:  6.00%\n",
      "Epoch [1/10], Step [596/600], Loss: 2.4022, batch time: 0.74, accuracy:  6.00%\n",
      "Epoch [1/10], Step [597/600], Loss: 2.4418, batch time: 0.73, accuracy:  2.00%\n",
      "Epoch [1/10], Step [598/600], Loss: 2.4304, batch time: 0.67, accuracy:  4.00%\n",
      "Epoch [1/10], Step [599/600], Loss: 2.4285, batch time: 0.62, accuracy:  5.00%\n",
      "Epoch [1/10], Step [600/600], Loss: 2.4193, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [2/10], Step [1/600], Loss: 2.3888, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [2/10], Step [2/600], Loss: 2.3631, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [2/10], Step [3/600], Loss: 2.4220, batch time: 0.63, accuracy:  4.00%\n",
      "Epoch [2/10], Step [4/600], Loss: 2.4231, batch time: 0.62, accuracy:  5.00%\n",
      "Epoch [2/10], Step [5/600], Loss: 2.3801, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [2/10], Step [6/600], Loss: 2.4461, batch time: 0.73, accuracy:  8.00%\n",
      "Epoch [2/10], Step [7/600], Loss: 2.3905, batch time: 0.73, accuracy:  12.00%\n",
      "Epoch [2/10], Step [8/600], Loss: 2.4042, batch time: 0.74, accuracy:  6.00%\n",
      "Epoch [2/10], Step [9/600], Loss: 2.3827, batch time: 0.73, accuracy:  9.00%\n",
      "Epoch [2/10], Step [10/600], Loss: 2.3779, batch time: 0.74, accuracy:  7.00%\n",
      "Epoch [2/10], Step [11/600], Loss: 2.4421, batch time: 0.73, accuracy:  3.00%\n",
      "Epoch [2/10], Step [12/600], Loss: 2.4098, batch time: 0.74, accuracy:  5.00%\n",
      "Epoch [2/10], Step [13/600], Loss: 2.4531, batch time: 0.74, accuracy:  6.00%\n",
      "Epoch [2/10], Step [14/600], Loss: 2.4155, batch time: 0.66, accuracy:  8.00%\n",
      "Epoch [2/10], Step [15/600], Loss: 2.4317, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [2/10], Step [16/600], Loss: 2.3954, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [2/10], Step [17/600], Loss: 2.4159, batch time: 0.63, accuracy:  5.00%\n",
      "Epoch [2/10], Step [18/600], Loss: 2.4044, batch time: 0.63, accuracy:  6.00%\n",
      "Epoch [2/10], Step [19/600], Loss: 2.3941, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [2/10], Step [20/600], Loss: 2.4002, batch time: 0.93, accuracy:  8.00%\n",
      "Epoch [2/10], Step [21/600], Loss: 2.3791, batch time: 0.73, accuracy:  13.00%\n",
      "Epoch [2/10], Step [22/600], Loss: 2.4019, batch time: 0.74, accuracy:  4.00%\n",
      "Epoch [2/10], Step [23/600], Loss: 2.3168, batch time: 0.73, accuracy:  12.00%\n",
      "Epoch [2/10], Step [24/600], Loss: 2.4219, batch time: 0.74, accuracy:  8.00%\n",
      "Epoch [2/10], Step [25/600], Loss: 2.4749, batch time: 0.73, accuracy:  3.00%\n",
      "Epoch [2/10], Step [26/600], Loss: 2.4733, batch time: 0.73, accuracy:  6.00%\n",
      "Epoch [2/10], Step [27/600], Loss: 2.4171, batch time: 0.75, accuracy:  7.00%\n",
      "Epoch [2/10], Step [28/600], Loss: 2.3835, batch time: 0.72, accuracy:  4.00%\n",
      "Epoch [2/10], Step [29/600], Loss: 2.4112, batch time: 0.73, accuracy:  7.00%\n",
      "Epoch [2/10], Step [30/600], Loss: 2.3936, batch time: 0.73, accuracy:  8.00%\n",
      "Epoch [2/10], Step [31/600], Loss: 2.4100, batch time: 0.73, accuracy:  8.00%\n",
      "Epoch [2/10], Step [32/600], Loss: 2.3461, batch time: 0.73, accuracy:  12.00%\n",
      "Epoch [2/10], Step [33/600], Loss: 2.4262, batch time: 0.73, accuracy:  11.00%\n",
      "Epoch [2/10], Step [34/600], Loss: 2.3952, batch time: 0.73, accuracy:  4.00%\n",
      "Epoch [2/10], Step [35/600], Loss: 2.3962, batch time: 0.72, accuracy:  4.00%\n",
      "Epoch [2/10], Step [36/600], Loss: 2.3575, batch time: 0.74, accuracy:  10.00%\n",
      "Epoch [2/10], Step [37/600], Loss: 2.4192, batch time: 0.73, accuracy:  7.00%\n",
      "Epoch [2/10], Step [38/600], Loss: 2.3691, batch time: 0.74, accuracy:  5.00%\n",
      "Epoch [2/10], Step [39/600], Loss: 2.4335, batch time: 0.72, accuracy:  4.00%\n",
      "Epoch [2/10], Step [40/600], Loss: 2.4035, batch time: 0.88, accuracy:  6.00%\n",
      "Epoch [2/10], Step [41/600], Loss: 2.3453, batch time: 0.73, accuracy:  5.00%\n",
      "Epoch [2/10], Step [42/600], Loss: 2.4008, batch time: 0.72, accuracy:  9.00%\n",
      "Epoch [2/10], Step [43/600], Loss: 2.3684, batch time: 0.72, accuracy:  11.00%\n",
      "Epoch [2/10], Step [44/600], Loss: 2.3952, batch time: 0.72, accuracy:  8.00%\n",
      "Epoch [2/10], Step [45/600], Loss: 2.3511, batch time: 0.73, accuracy:  5.00%\n",
      "Epoch [2/10], Step [46/600], Loss: 2.3852, batch time: 0.73, accuracy:  11.00%\n",
      "Epoch [2/10], Step [47/600], Loss: 2.4078, batch time: 0.72, accuracy:  9.00%\n",
      "Epoch [2/10], Step [48/600], Loss: 2.4177, batch time: 0.73, accuracy:  9.00%\n",
      "Epoch [2/10], Step [49/600], Loss: 2.4174, batch time: 0.73, accuracy:  7.00%\n",
      "Epoch [2/10], Step [50/600], Loss: 2.3671, batch time: 0.74, accuracy:  10.00%\n",
      "Epoch [2/10], Step [51/600], Loss: 2.3859, batch time: 0.74, accuracy:  7.00%\n",
      "Epoch [2/10], Step [52/600], Loss: 2.3442, batch time: 0.73, accuracy:  6.00%\n",
      "Epoch [2/10], Step [53/600], Loss: 2.4150, batch time: 0.73, accuracy:  8.00%\n",
      "Epoch [2/10], Step [54/600], Loss: 2.4288, batch time: 0.74, accuracy:  11.00%\n",
      "Epoch [2/10], Step [55/600], Loss: 2.3900, batch time: 0.73, accuracy:  6.00%\n",
      "Epoch [2/10], Step [56/600], Loss: 2.3948, batch time: 0.72, accuracy:  5.00%\n",
      "Epoch [2/10], Step [57/600], Loss: 2.4072, batch time: 0.67, accuracy:  6.00%\n",
      "Epoch [2/10], Step [58/600], Loss: 2.4245, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [2/10], Step [59/600], Loss: 2.4124, batch time: 0.64, accuracy:  10.00%\n",
      "Epoch [2/10], Step [60/600], Loss: 2.3631, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [2/10], Step [61/600], Loss: 2.4282, batch time: 0.72, accuracy:  7.00%\n",
      "Epoch [2/10], Step [62/600], Loss: 2.4364, batch time: 0.74, accuracy:  9.00%\n",
      "Epoch [2/10], Step [63/600], Loss: 2.3903, batch time: 0.74, accuracy:  12.00%\n",
      "Epoch [2/10], Step [64/600], Loss: 2.3481, batch time: 0.75, accuracy:  10.00%\n",
      "Epoch [2/10], Step [65/600], Loss: 2.3639, batch time: 0.64, accuracy:  5.00%\n",
      "Epoch [2/10], Step [66/600], Loss: 2.3785, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [2/10], Step [67/600], Loss: 2.3451, batch time: 0.63, accuracy:  11.00%\n",
      "Epoch [2/10], Step [68/600], Loss: 2.3831, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [2/10], Step [69/600], Loss: 2.4074, batch time: 0.64, accuracy:  6.00%\n",
      "Epoch [2/10], Step [70/600], Loss: 2.3664, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [2/10], Step [71/600], Loss: 2.4199, batch time: 0.63, accuracy:  6.00%\n",
      "Epoch [2/10], Step [72/600], Loss: 2.3950, batch time: 0.64, accuracy:  11.00%\n",
      "Epoch [2/10], Step [73/600], Loss: 2.3620, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [2/10], Step [74/600], Loss: 2.4176, batch time: 0.63, accuracy:  11.00%\n",
      "Epoch [2/10], Step [75/600], Loss: 2.3838, batch time: 0.63, accuracy:  13.00%\n",
      "Epoch [2/10], Step [76/600], Loss: 2.4112, batch time: 0.62, accuracy:  5.00%\n",
      "Epoch [2/10], Step [77/600], Loss: 2.3681, batch time: 0.63, accuracy:  11.00%\n",
      "Epoch [2/10], Step [78/600], Loss: 2.4109, batch time: 0.71, accuracy:  8.00%\n",
      "Epoch [2/10], Step [79/600], Loss: 2.3837, batch time: 0.69, accuracy:  6.00%\n",
      "Epoch [2/10], Step [80/600], Loss: 2.4282, batch time: 0.64, accuracy:  4.00%\n",
      "Epoch [2/10], Step [81/600], Loss: 2.3613, batch time: 0.63, accuracy:  4.00%\n",
      "Epoch [2/10], Step [82/600], Loss: 2.3699, batch time: 0.72, accuracy:  6.00%\n",
      "Epoch [2/10], Step [83/600], Loss: 2.4541, batch time: 0.70, accuracy:  6.00%\n",
      "Epoch [2/10], Step [84/600], Loss: 2.3292, batch time: 0.74, accuracy:  13.00%\n",
      "Epoch [2/10], Step [85/600], Loss: 2.3681, batch time: 0.73, accuracy:  10.00%\n",
      "Epoch [2/10], Step [86/600], Loss: 2.4599, batch time: 0.62, accuracy:  4.00%\n",
      "Epoch [2/10], Step [87/600], Loss: 2.4272, batch time: 0.63, accuracy:  12.00%\n",
      "Epoch [2/10], Step [88/600], Loss: 2.3711, batch time: 0.63, accuracy:  12.00%\n",
      "Epoch [2/10], Step [89/600], Loss: 2.3658, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [2/10], Step [90/600], Loss: 2.3432, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [2/10], Step [91/600], Loss: 2.3796, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [2/10], Step [92/600], Loss: 2.4153, batch time: 0.62, accuracy:  5.00%\n",
      "Epoch [2/10], Step [93/600], Loss: 2.3735, batch time: 0.73, accuracy:  11.00%\n",
      "Epoch [2/10], Step [94/600], Loss: 2.3581, batch time: 0.75, accuracy:  5.00%\n",
      "Epoch [2/10], Step [95/600], Loss: 2.3870, batch time: 0.74, accuracy:  9.00%\n",
      "Epoch [2/10], Step [96/600], Loss: 2.3940, batch time: 0.73, accuracy:  8.00%\n",
      "Epoch [2/10], Step [97/600], Loss: 2.3958, batch time: 0.73, accuracy:  8.00%\n",
      "Epoch [2/10], Step [98/600], Loss: 2.3692, batch time: 0.73, accuracy:  7.00%\n",
      "Epoch [2/10], Step [99/600], Loss: 2.3579, batch time: 0.74, accuracy:  11.00%\n",
      "Epoch [2/10], Step [100/600], Loss: 2.3404, batch time: 0.72, accuracy:  10.00%\n",
      "Epoch [2/10], Step [101/600], Loss: 2.4152, batch time: 0.74, accuracy:  12.00%\n",
      "Epoch [2/10], Step [102/600], Loss: 2.4315, batch time: 0.72, accuracy:  2.00%\n",
      "Epoch [2/10], Step [103/600], Loss: 2.4273, batch time: 0.74, accuracy:  4.00%\n",
      "Epoch [2/10], Step [104/600], Loss: 2.3235, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [2/10], Step [105/600], Loss: 2.3843, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [2/10], Step [106/600], Loss: 2.3588, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [2/10], Step [107/600], Loss: 2.4222, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [2/10], Step [108/600], Loss: 2.3944, batch time: 0.64, accuracy:  8.00%\n",
      "Epoch [2/10], Step [109/600], Loss: 2.3774, batch time: 0.63, accuracy:  12.00%\n",
      "Epoch [2/10], Step [110/600], Loss: 2.3734, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [2/10], Step [111/600], Loss: 2.4389, batch time: 0.62, accuracy:  5.00%\n",
      "Epoch [2/10], Step [112/600], Loss: 2.3707, batch time: 0.62, accuracy:  5.00%\n",
      "Epoch [2/10], Step [113/600], Loss: 2.3452, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [2/10], Step [114/600], Loss: 2.3597, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [2/10], Step [115/600], Loss: 2.4057, batch time: 0.63, accuracy:  5.00%\n",
      "Epoch [2/10], Step [116/600], Loss: 2.3813, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [2/10], Step [117/600], Loss: 2.3936, batch time: 0.64, accuracy:  5.00%\n",
      "Epoch [2/10], Step [118/600], Loss: 2.3927, batch time: 0.79, accuracy:  7.00%\n",
      "Epoch [2/10], Step [119/600], Loss: 2.3979, batch time: 0.63, accuracy:  12.00%\n",
      "Epoch [2/10], Step [120/600], Loss: 2.3944, batch time: 0.64, accuracy:  6.00%\n",
      "Epoch [2/10], Step [121/600], Loss: 2.4354, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [2/10], Step [122/600], Loss: 2.3406, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [2/10], Step [123/600], Loss: 2.4119, batch time: 0.64, accuracy:  7.00%\n",
      "Epoch [2/10], Step [124/600], Loss: 2.3749, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [2/10], Step [125/600], Loss: 2.3677, batch time: 0.64, accuracy:  9.00%\n",
      "Epoch [2/10], Step [126/600], Loss: 2.3371, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [2/10], Step [127/600], Loss: 2.3369, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [2/10], Step [128/600], Loss: 2.4216, batch time: 0.61, accuracy:  6.00%\n",
      "Epoch [2/10], Step [129/600], Loss: 2.4077, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [2/10], Step [130/600], Loss: 2.3972, batch time: 0.63, accuracy:  4.00%\n",
      "Epoch [2/10], Step [131/600], Loss: 2.3685, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [2/10], Step [132/600], Loss: 2.3612, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [2/10], Step [133/600], Loss: 2.3701, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [2/10], Step [134/600], Loss: 2.3328, batch time: 0.62, accuracy:  16.00%\n",
      "Epoch [2/10], Step [135/600], Loss: 2.3537, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [2/10], Step [136/600], Loss: 2.3561, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [2/10], Step [137/600], Loss: 2.3732, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [2/10], Step [138/600], Loss: 2.3949, batch time: 0.63, accuracy:  15.00%\n",
      "Epoch [2/10], Step [139/600], Loss: 2.3670, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [2/10], Step [140/600], Loss: 2.4091, batch time: 0.65, accuracy:  8.00%\n",
      "Epoch [2/10], Step [141/600], Loss: 2.3713, batch time: 0.64, accuracy:  7.00%\n",
      "Epoch [2/10], Step [142/600], Loss: 2.3937, batch time: 0.66, accuracy:  9.00%\n",
      "Epoch [2/10], Step [143/600], Loss: 2.3187, batch time: 0.73, accuracy:  11.00%\n",
      "Epoch [2/10], Step [144/600], Loss: 2.3935, batch time: 0.73, accuracy:  8.00%\n",
      "Epoch [2/10], Step [145/600], Loss: 2.3506, batch time: 0.74, accuracy:  12.00%\n",
      "Epoch [2/10], Step [146/600], Loss: 2.3669, batch time: 0.66, accuracy:  9.00%\n",
      "Epoch [2/10], Step [147/600], Loss: 2.3839, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [2/10], Step [148/600], Loss: 2.4112, batch time: 0.64, accuracy:  8.00%\n",
      "Epoch [2/10], Step [149/600], Loss: 2.3624, batch time: 0.64, accuracy:  7.00%\n",
      "Epoch [2/10], Step [150/600], Loss: 2.3892, batch time: 0.64, accuracy:  11.00%\n",
      "Epoch [2/10], Step [151/600], Loss: 2.3374, batch time: 0.75, accuracy:  12.00%\n",
      "Epoch [2/10], Step [152/600], Loss: 2.3809, batch time: 0.76, accuracy:  6.00%\n",
      "Epoch [2/10], Step [153/600], Loss: 2.3831, batch time: 0.75, accuracy:  6.00%\n",
      "Epoch [2/10], Step [154/600], Loss: 2.3339, batch time: 0.73, accuracy:  12.00%\n",
      "Epoch [2/10], Step [155/600], Loss: 2.4018, batch time: 0.76, accuracy:  5.00%\n",
      "Epoch [2/10], Step [156/600], Loss: 2.3342, batch time: 0.75, accuracy:  11.00%\n",
      "Epoch [2/10], Step [157/600], Loss: 2.3564, batch time: 0.72, accuracy:  11.00%\n",
      "Epoch [2/10], Step [158/600], Loss: 2.3379, batch time: 0.73, accuracy:  8.00%\n",
      "Epoch [2/10], Step [159/600], Loss: 2.3870, batch time: 0.73, accuracy:  6.00%\n",
      "Epoch [2/10], Step [160/600], Loss: 2.3622, batch time: 0.73, accuracy:  11.00%\n",
      "Epoch [2/10], Step [161/600], Loss: 2.3659, batch time: 0.66, accuracy:  6.00%\n",
      "Epoch [2/10], Step [162/600], Loss: 2.3588, batch time: 0.61, accuracy:  12.00%\n",
      "Epoch [2/10], Step [163/600], Loss: 2.3817, batch time: 0.62, accuracy:  13.00%\n",
      "Epoch [2/10], Step [164/600], Loss: 2.3869, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [2/10], Step [165/600], Loss: 2.3733, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [2/10], Step [166/600], Loss: 2.3780, batch time: 0.62, accuracy:  4.00%\n",
      "Epoch [2/10], Step [167/600], Loss: 2.3812, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [2/10], Step [168/600], Loss: 2.3876, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [2/10], Step [169/600], Loss: 2.3531, batch time: 0.63, accuracy:  10.00%\n",
      "Epoch [2/10], Step [170/600], Loss: 2.3375, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [2/10], Step [171/600], Loss: 2.4009, batch time: 0.61, accuracy:  5.00%\n",
      "Epoch [2/10], Step [172/600], Loss: 2.3771, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [2/10], Step [173/600], Loss: 2.4025, batch time: 0.63, accuracy:  4.00%\n",
      "Epoch [2/10], Step [174/600], Loss: 2.3579, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [2/10], Step [175/600], Loss: 2.3599, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [2/10], Step [176/600], Loss: 2.3362, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [2/10], Step [177/600], Loss: 2.3664, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [2/10], Step [178/600], Loss: 2.3752, batch time: 0.61, accuracy:  8.00%\n",
      "Epoch [2/10], Step [179/600], Loss: 2.3631, batch time: 0.62, accuracy:  5.00%\n",
      "Epoch [2/10], Step [180/600], Loss: 2.3590, batch time: 0.73, accuracy:  11.00%\n",
      "Epoch [2/10], Step [181/600], Loss: 2.3714, batch time: 0.73, accuracy:  7.00%\n",
      "Epoch [2/10], Step [182/600], Loss: 2.4139, batch time: 0.74, accuracy:  4.00%\n",
      "Epoch [2/10], Step [183/600], Loss: 2.3788, batch time: 0.73, accuracy:  11.00%\n",
      "Epoch [2/10], Step [184/600], Loss: 2.3661, batch time: 0.73, accuracy:  6.00%\n",
      "Epoch [2/10], Step [185/600], Loss: 2.3858, batch time: 0.74, accuracy:  6.00%\n",
      "Epoch [2/10], Step [186/600], Loss: 2.3727, batch time: 0.73, accuracy:  4.00%\n",
      "Epoch [2/10], Step [187/600], Loss: 2.3736, batch time: 0.66, accuracy:  10.00%\n",
      "Epoch [2/10], Step [188/600], Loss: 2.3727, batch time: 0.62, accuracy:  12.00%\n",
      "Epoch [2/10], Step [189/600], Loss: 2.3915, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [2/10], Step [190/600], Loss: 2.3221, batch time: 0.64, accuracy:  12.00%\n",
      "Epoch [2/10], Step [191/600], Loss: 2.3795, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [2/10], Step [192/600], Loss: 2.3423, batch time: 0.73, accuracy:  10.00%\n",
      "Epoch [2/10], Step [193/600], Loss: 2.4220, batch time: 0.68, accuracy:  4.00%\n",
      "Epoch [2/10], Step [194/600], Loss: 2.3524, batch time: 0.64, accuracy:  7.00%\n",
      "Epoch [2/10], Step [195/600], Loss: 2.3781, batch time: 0.63, accuracy:  10.00%\n",
      "Epoch [2/10], Step [196/600], Loss: 2.3672, batch time: 0.86, accuracy:  8.00%\n",
      "Epoch [2/10], Step [197/600], Loss: 2.3518, batch time: 0.62, accuracy:  5.00%\n",
      "Epoch [2/10], Step [198/600], Loss: 2.3804, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [2/10], Step [199/600], Loss: 2.3280, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [2/10], Step [200/600], Loss: 2.3722, batch time: 0.63, accuracy:  12.00%\n",
      "Epoch [2/10], Step [201/600], Loss: 2.3645, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [2/10], Step [202/600], Loss: 2.3940, batch time: 0.63, accuracy:  10.00%\n",
      "Epoch [2/10], Step [203/600], Loss: 2.3950, batch time: 0.62, accuracy:  2.00%\n",
      "Epoch [2/10], Step [204/600], Loss: 2.4344, batch time: 0.62, accuracy:  5.00%\n",
      "Epoch [2/10], Step [205/600], Loss: 2.3630, batch time: 0.64, accuracy:  12.00%\n",
      "Epoch [2/10], Step [206/600], Loss: 2.3486, batch time: 0.63, accuracy:  6.00%\n",
      "Epoch [2/10], Step [207/600], Loss: 2.3555, batch time: 0.72, accuracy:  8.00%\n",
      "Epoch [2/10], Step [208/600], Loss: 2.3275, batch time: 0.72, accuracy:  13.00%\n",
      "Epoch [2/10], Step [209/600], Loss: 2.3671, batch time: 0.71, accuracy:  8.00%\n",
      "Epoch [2/10], Step [210/600], Loss: 2.3669, batch time: 0.74, accuracy:  8.00%\n",
      "Epoch [2/10], Step [211/600], Loss: 2.4298, batch time: 0.73, accuracy:  6.00%\n",
      "Epoch [2/10], Step [212/600], Loss: 2.3588, batch time: 0.72, accuracy:  8.00%\n",
      "Epoch [2/10], Step [213/600], Loss: 2.3531, batch time: 0.73, accuracy:  11.00%\n",
      "Epoch [2/10], Step [214/600], Loss: 2.3360, batch time: 0.72, accuracy:  14.00%\n",
      "Epoch [2/10], Step [215/600], Loss: 2.4016, batch time: 0.73, accuracy:  12.00%\n",
      "Epoch [2/10], Step [216/600], Loss: 2.3542, batch time: 0.72, accuracy:  13.00%\n",
      "Epoch [2/10], Step [217/600], Loss: 2.3682, batch time: 0.72, accuracy:  7.00%\n",
      "Epoch [2/10], Step [218/600], Loss: 2.3848, batch time: 0.72, accuracy:  11.00%\n",
      "Epoch [2/10], Step [219/600], Loss: 2.3916, batch time: 0.74, accuracy:  4.00%\n",
      "Epoch [2/10], Step [220/600], Loss: 2.3534, batch time: 0.72, accuracy:  13.00%\n",
      "Epoch [2/10], Step [221/600], Loss: 2.2999, batch time: 0.73, accuracy:  11.00%\n",
      "Epoch [2/10], Step [222/600], Loss: 2.3563, batch time: 0.72, accuracy:  6.00%\n",
      "Epoch [2/10], Step [223/600], Loss: 2.3616, batch time: 0.72, accuracy:  11.00%\n",
      "Epoch [2/10], Step [224/600], Loss: 2.3935, batch time: 0.74, accuracy:  7.00%\n",
      "Epoch [2/10], Step [225/600], Loss: 2.3808, batch time: 0.73, accuracy:  10.00%\n",
      "Epoch [2/10], Step [226/600], Loss: 2.3306, batch time: 0.72, accuracy:  9.00%\n",
      "Epoch [2/10], Step [227/600], Loss: 2.3683, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [2/10], Step [228/600], Loss: 2.3715, batch time: 0.71, accuracy:  5.00%\n",
      "Epoch [2/10], Step [229/600], Loss: 2.3792, batch time: 0.74, accuracy:  5.00%\n",
      "Epoch [2/10], Step [230/600], Loss: 2.3982, batch time: 0.72, accuracy:  8.00%\n",
      "Epoch [2/10], Step [231/600], Loss: 2.3422, batch time: 0.72, accuracy:  9.00%\n",
      "Epoch [2/10], Step [232/600], Loss: 2.3721, batch time: 0.72, accuracy:  9.00%\n",
      "Epoch [2/10], Step [233/600], Loss: 2.3575, batch time: 0.74, accuracy:  11.00%\n",
      "Epoch [2/10], Step [234/600], Loss: 2.3825, batch time: 0.72, accuracy:  7.00%\n",
      "Epoch [2/10], Step [235/600], Loss: 2.3239, batch time: 0.74, accuracy:  13.00%\n",
      "Epoch [2/10], Step [236/600], Loss: 2.3516, batch time: 0.74, accuracy:  8.00%\n",
      "Epoch [2/10], Step [237/600], Loss: 2.3634, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [2/10], Step [238/600], Loss: 2.3523, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [2/10], Step [239/600], Loss: 2.3201, batch time: 0.63, accuracy:  14.00%\n",
      "Epoch [2/10], Step [240/600], Loss: 2.3376, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [2/10], Step [241/600], Loss: 2.3773, batch time: 0.63, accuracy:  13.00%\n",
      "Epoch [2/10], Step [242/600], Loss: 2.3971, batch time: 0.63, accuracy:  11.00%\n",
      "Epoch [2/10], Step [243/600], Loss: 2.3513, batch time: 0.64, accuracy:  13.00%\n",
      "Epoch [2/10], Step [244/600], Loss: 2.3627, batch time: 0.73, accuracy:  6.00%\n",
      "Epoch [2/10], Step [245/600], Loss: 2.3647, batch time: 0.61, accuracy:  6.00%\n",
      "Epoch [2/10], Step [246/600], Loss: 2.3951, batch time: 0.61, accuracy:  3.00%\n",
      "Epoch [2/10], Step [247/600], Loss: 2.3381, batch time: 0.61, accuracy:  11.00%\n",
      "Epoch [2/10], Step [248/600], Loss: 2.3305, batch time: 0.66, accuracy:  11.00%\n",
      "Epoch [2/10], Step [249/600], Loss: 2.3920, batch time: 0.66, accuracy:  5.00%\n",
      "Epoch [2/10], Step [250/600], Loss: 2.3758, batch time: 0.63, accuracy:  6.00%\n",
      "Epoch [2/10], Step [251/600], Loss: 2.3599, batch time: 0.61, accuracy:  8.00%\n",
      "Epoch [2/10], Step [252/600], Loss: 2.3934, batch time: 0.62, accuracy:  3.00%\n",
      "Epoch [2/10], Step [253/600], Loss: 2.3436, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [2/10], Step [254/600], Loss: 2.3826, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [2/10], Step [255/600], Loss: 2.3277, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [2/10], Step [256/600], Loss: 2.3848, batch time: 0.61, accuracy:  7.00%\n",
      "Epoch [2/10], Step [257/600], Loss: 2.3725, batch time: 0.61, accuracy:  5.00%\n",
      "Epoch [2/10], Step [258/600], Loss: 2.3461, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [2/10], Step [259/600], Loss: 2.3349, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [2/10], Step [260/600], Loss: 2.3575, batch time: 0.66, accuracy:  11.00%\n",
      "Epoch [2/10], Step [261/600], Loss: 2.3477, batch time: 0.61, accuracy:  9.00%\n",
      "Epoch [2/10], Step [262/600], Loss: 2.3569, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [2/10], Step [263/600], Loss: 2.3671, batch time: 0.61, accuracy:  5.00%\n",
      "Epoch [2/10], Step [264/600], Loss: 2.3618, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [2/10], Step [265/600], Loss: 2.3734, batch time: 0.63, accuracy:  4.00%\n",
      "Epoch [2/10], Step [266/600], Loss: 2.3808, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [2/10], Step [267/600], Loss: 2.3477, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [2/10], Step [268/600], Loss: 2.3042, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [2/10], Step [269/600], Loss: 2.3565, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [2/10], Step [270/600], Loss: 2.3297, batch time: 0.63, accuracy:  14.00%\n",
      "Epoch [2/10], Step [271/600], Loss: 2.3366, batch time: 0.61, accuracy:  14.00%\n",
      "Epoch [2/10], Step [272/600], Loss: 2.3451, batch time: 0.61, accuracy:  13.00%\n",
      "Epoch [2/10], Step [273/600], Loss: 2.3580, batch time: 0.61, accuracy:  7.00%\n",
      "Epoch [2/10], Step [274/600], Loss: 2.3645, batch time: 0.76, accuracy:  8.00%\n",
      "Epoch [2/10], Step [275/600], Loss: 2.3648, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [2/10], Step [276/600], Loss: 2.3517, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [2/10], Step [277/600], Loss: 2.3529, batch time: 0.63, accuracy:  11.00%\n",
      "Epoch [2/10], Step [278/600], Loss: 2.3423, batch time: 0.63, accuracy:  10.00%\n",
      "Epoch [2/10], Step [279/600], Loss: 2.3535, batch time: 0.66, accuracy:  9.00%\n",
      "Epoch [2/10], Step [280/600], Loss: 2.3542, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [2/10], Step [281/600], Loss: 2.3688, batch time: 0.61, accuracy:  4.00%\n",
      "Epoch [2/10], Step [282/600], Loss: 2.3407, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [2/10], Step [283/600], Loss: 2.3357, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [2/10], Step [284/600], Loss: 2.3757, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [2/10], Step [285/600], Loss: 2.3548, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [2/10], Step [286/600], Loss: 2.3403, batch time: 0.63, accuracy:  11.00%\n",
      "Epoch [2/10], Step [287/600], Loss: 2.3357, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [2/10], Step [288/600], Loss: 2.3550, batch time: 0.61, accuracy:  12.00%\n",
      "Epoch [2/10], Step [289/600], Loss: 2.3651, batch time: 0.61, accuracy:  9.00%\n",
      "Epoch [2/10], Step [290/600], Loss: 2.3148, batch time: 0.62, accuracy:  12.00%\n",
      "Epoch [2/10], Step [291/600], Loss: 2.3100, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [2/10], Step [292/600], Loss: 2.3504, batch time: 0.61, accuracy:  7.00%\n",
      "Epoch [2/10], Step [293/600], Loss: 2.3625, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [2/10], Step [294/600], Loss: 2.3432, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [2/10], Step [295/600], Loss: 2.3673, batch time: 0.66, accuracy:  9.00%\n",
      "Epoch [2/10], Step [296/600], Loss: 2.3143, batch time: 0.63, accuracy:  12.00%\n",
      "Epoch [2/10], Step [297/600], Loss: 2.3431, batch time: 0.61, accuracy:  9.00%\n",
      "Epoch [2/10], Step [298/600], Loss: 2.3387, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [2/10], Step [299/600], Loss: 2.3610, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [2/10], Step [300/600], Loss: 2.3256, batch time: 0.61, accuracy:  10.00%\n",
      "Epoch [2/10], Step [301/600], Loss: 2.3287, batch time: 0.62, accuracy:  12.00%\n",
      "Epoch [2/10], Step [302/600], Loss: 2.3122, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [2/10], Step [303/600], Loss: 2.3190, batch time: 0.61, accuracy:  11.00%\n",
      "Epoch [2/10], Step [304/600], Loss: 2.3218, batch time: 0.61, accuracy:  16.00%\n",
      "Epoch [2/10], Step [305/600], Loss: 2.3432, batch time: 0.61, accuracy:  13.00%\n",
      "Epoch [2/10], Step [306/600], Loss: 2.3802, batch time: 0.61, accuracy:  5.00%\n",
      "Epoch [2/10], Step [307/600], Loss: 2.3519, batch time: 0.62, accuracy:  13.00%\n",
      "Epoch [2/10], Step [308/600], Loss: 2.3504, batch time: 0.61, accuracy:  7.00%\n",
      "Epoch [2/10], Step [309/600], Loss: 2.3364, batch time: 0.61, accuracy:  9.00%\n",
      "Epoch [2/10], Step [310/600], Loss: 2.3351, batch time: 0.61, accuracy:  10.00%\n",
      "Epoch [2/10], Step [311/600], Loss: 2.3252, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [2/10], Step [312/600], Loss: 2.3499, batch time: 0.62, accuracy:  13.00%\n",
      "Epoch [2/10], Step [313/600], Loss: 2.3682, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [2/10], Step [314/600], Loss: 2.3203, batch time: 0.61, accuracy:  12.00%\n",
      "Epoch [2/10], Step [315/600], Loss: 2.3337, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [2/10], Step [316/600], Loss: 2.3734, batch time: 0.64, accuracy:  9.00%\n",
      "Epoch [2/10], Step [317/600], Loss: 2.3767, batch time: 0.73, accuracy:  6.00%\n",
      "Epoch [2/10], Step [318/600], Loss: 2.3235, batch time: 0.72, accuracy:  16.00%\n",
      "Epoch [2/10], Step [319/600], Loss: 2.3511, batch time: 0.65, accuracy:  7.00%\n",
      "Epoch [2/10], Step [320/600], Loss: 2.3420, batch time: 0.61, accuracy:  6.00%\n",
      "Epoch [2/10], Step [321/600], Loss: 2.3874, batch time: 0.61, accuracy:  8.00%\n",
      "Epoch [2/10], Step [322/600], Loss: 2.3152, batch time: 0.61, accuracy:  8.00%\n",
      "Epoch [2/10], Step [323/600], Loss: 2.3798, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [2/10], Step [324/600], Loss: 2.3501, batch time: 0.61, accuracy:  7.00%\n",
      "Epoch [2/10], Step [325/600], Loss: 2.3261, batch time: 0.63, accuracy:  10.00%\n",
      "Epoch [2/10], Step [326/600], Loss: 2.3909, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [2/10], Step [327/600], Loss: 2.3497, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [2/10], Step [328/600], Loss: 2.3430, batch time: 0.63, accuracy:  11.00%\n",
      "Epoch [2/10], Step [329/600], Loss: 2.3544, batch time: 0.63, accuracy:  6.00%\n",
      "Epoch [2/10], Step [330/600], Loss: 2.3685, batch time: 0.64, accuracy:  6.00%\n",
      "Epoch [2/10], Step [331/600], Loss: 2.3031, batch time: 0.62, accuracy:  16.00%\n",
      "Epoch [2/10], Step [332/600], Loss: 2.3708, batch time: 0.62, accuracy:  5.00%\n",
      "Epoch [2/10], Step [333/600], Loss: 2.3774, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [2/10], Step [334/600], Loss: 2.3811, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [2/10], Step [335/600], Loss: 2.3509, batch time: 0.61, accuracy:  8.00%\n",
      "Epoch [2/10], Step [336/600], Loss: 2.3409, batch time: 0.62, accuracy:  12.00%\n",
      "Epoch [2/10], Step [337/600], Loss: 2.3256, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [2/10], Step [338/600], Loss: 2.3495, batch time: 0.61, accuracy:  7.00%\n",
      "Epoch [2/10], Step [339/600], Loss: 2.3290, batch time: 0.63, accuracy:  10.00%\n",
      "Epoch [2/10], Step [340/600], Loss: 2.3179, batch time: 0.61, accuracy:  8.00%\n",
      "Epoch [2/10], Step [341/600], Loss: 2.3641, batch time: 0.61, accuracy:  6.00%\n",
      "Epoch [2/10], Step [342/600], Loss: 2.3351, batch time: 0.61, accuracy:  9.00%\n",
      "Epoch [2/10], Step [343/600], Loss: 2.3369, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [2/10], Step [344/600], Loss: 2.3412, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [2/10], Step [345/600], Loss: 2.3606, batch time: 0.64, accuracy:  7.00%\n",
      "Epoch [2/10], Step [346/600], Loss: 2.3739, batch time: 0.62, accuracy:  3.00%\n",
      "Epoch [2/10], Step [347/600], Loss: 2.3293, batch time: 0.71, accuracy:  10.00%\n",
      "Epoch [2/10], Step [348/600], Loss: 2.3732, batch time: 0.74, accuracy:  10.00%\n",
      "Epoch [2/10], Step [349/600], Loss: 2.3447, batch time: 0.73, accuracy:  4.00%\n",
      "Epoch [2/10], Step [350/600], Loss: 2.3245, batch time: 0.71, accuracy:  12.00%\n",
      "Epoch [2/10], Step [351/600], Loss: 2.3615, batch time: 0.72, accuracy:  10.00%\n",
      "Epoch [2/10], Step [352/600], Loss: 2.3173, batch time: 0.82, accuracy:  11.00%\n",
      "Epoch [2/10], Step [353/600], Loss: 2.3633, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [2/10], Step [354/600], Loss: 2.3230, batch time: 0.64, accuracy:  11.00%\n",
      "Epoch [2/10], Step [355/600], Loss: 2.3043, batch time: 0.67, accuracy:  14.00%\n",
      "Epoch [2/10], Step [356/600], Loss: 2.3564, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [2/10], Step [357/600], Loss: 2.3369, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [2/10], Step [358/600], Loss: 2.3234, batch time: 0.61, accuracy:  13.00%\n",
      "Epoch [2/10], Step [359/600], Loss: 2.3511, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [2/10], Step [360/600], Loss: 2.3055, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [2/10], Step [361/600], Loss: 2.3489, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [2/10], Step [362/600], Loss: 2.3393, batch time: 0.62, accuracy:  12.00%\n",
      "Epoch [2/10], Step [363/600], Loss: 2.3374, batch time: 0.62, accuracy:  12.00%\n",
      "Epoch [2/10], Step [364/600], Loss: 2.3501, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [2/10], Step [365/600], Loss: 2.3349, batch time: 0.63, accuracy:  13.00%\n",
      "Epoch [2/10], Step [366/600], Loss: 2.3471, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [2/10], Step [367/600], Loss: 2.3543, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [2/10], Step [368/600], Loss: 2.3034, batch time: 0.62, accuracy:  13.00%\n",
      "Epoch [2/10], Step [369/600], Loss: 2.3193, batch time: 0.64, accuracy:  10.00%\n",
      "Epoch [2/10], Step [370/600], Loss: 2.3395, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [2/10], Step [371/600], Loss: 2.3334, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [2/10], Step [372/600], Loss: 2.3445, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [2/10], Step [373/600], Loss: 2.3660, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [2/10], Step [374/600], Loss: 2.3305, batch time: 0.74, accuracy:  11.00%\n",
      "Epoch [2/10], Step [375/600], Loss: 2.3285, batch time: 0.74, accuracy:  8.00%\n",
      "Epoch [2/10], Step [376/600], Loss: 2.3114, batch time: 0.72, accuracy:  10.00%\n",
      "Epoch [2/10], Step [377/600], Loss: 2.2926, batch time: 0.73, accuracy:  12.00%\n",
      "Epoch [2/10], Step [378/600], Loss: 2.3380, batch time: 0.75, accuracy:  11.00%\n",
      "Epoch [2/10], Step [379/600], Loss: 2.3258, batch time: 0.64, accuracy:  14.00%\n",
      "Epoch [2/10], Step [380/600], Loss: 2.3233, batch time: 0.63, accuracy:  11.00%\n",
      "Epoch [2/10], Step [381/600], Loss: 2.3402, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [2/10], Step [382/600], Loss: 2.3766, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [2/10], Step [383/600], Loss: 2.3264, batch time: 0.63, accuracy:  21.00%\n",
      "Epoch [2/10], Step [384/600], Loss: 2.3444, batch time: 0.63, accuracy:  13.00%\n",
      "Epoch [2/10], Step [385/600], Loss: 2.3495, batch time: 0.63, accuracy:  11.00%\n",
      "Epoch [2/10], Step [386/600], Loss: 2.3455, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [2/10], Step [387/600], Loss: 2.3292, batch time: 0.71, accuracy:  11.00%\n",
      "Epoch [2/10], Step [388/600], Loss: 2.3251, batch time: 0.62, accuracy:  12.00%\n",
      "Epoch [2/10], Step [389/600], Loss: 2.3512, batch time: 0.65, accuracy:  10.00%\n",
      "Epoch [2/10], Step [390/600], Loss: 2.3561, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [2/10], Step [391/600], Loss: 2.3687, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [2/10], Step [392/600], Loss: 2.3385, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [2/10], Step [393/600], Loss: 2.3365, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [2/10], Step [394/600], Loss: 2.3143, batch time: 0.62, accuracy:  15.00%\n",
      "Epoch [2/10], Step [395/600], Loss: 2.3257, batch time: 0.64, accuracy:  14.00%\n",
      "Epoch [2/10], Step [396/600], Loss: 2.3448, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [2/10], Step [397/600], Loss: 2.3589, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [2/10], Step [398/600], Loss: 2.3341, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [2/10], Step [399/600], Loss: 2.3457, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [2/10], Step [400/600], Loss: 2.3219, batch time: 0.63, accuracy:  12.00%\n",
      "Epoch [2/10], Step [401/600], Loss: 2.3438, batch time: 0.63, accuracy:  14.00%\n",
      "Epoch [2/10], Step [402/600], Loss: 2.3226, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [2/10], Step [403/600], Loss: 2.3304, batch time: 0.63, accuracy:  12.00%\n",
      "Epoch [2/10], Step [404/600], Loss: 2.3259, batch time: 0.63, accuracy:  13.00%\n",
      "Epoch [2/10], Step [405/600], Loss: 2.3488, batch time: 0.64, accuracy:  10.00%\n",
      "Epoch [2/10], Step [406/600], Loss: 2.3121, batch time: 0.63, accuracy:  13.00%\n",
      "Epoch [2/10], Step [407/600], Loss: 2.3408, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [2/10], Step [408/600], Loss: 2.3411, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [2/10], Step [409/600], Loss: 2.3548, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [2/10], Step [410/600], Loss: 2.3167, batch time: 0.74, accuracy:  9.00%\n",
      "Epoch [2/10], Step [411/600], Loss: 2.3471, batch time: 0.73, accuracy:  11.00%\n",
      "Epoch [2/10], Step [412/600], Loss: 2.3199, batch time: 0.72, accuracy:  9.00%\n",
      "Epoch [2/10], Step [413/600], Loss: 2.3604, batch time: 0.71, accuracy:  7.00%\n",
      "Epoch [2/10], Step [414/600], Loss: 2.3262, batch time: 0.62, accuracy:  17.00%\n",
      "Epoch [2/10], Step [415/600], Loss: 2.3397, batch time: 0.73, accuracy:  12.00%\n",
      "Epoch [2/10], Step [416/600], Loss: 2.3204, batch time: 0.73, accuracy:  11.00%\n",
      "Epoch [2/10], Step [417/600], Loss: 2.3318, batch time: 0.72, accuracy:  8.00%\n",
      "Epoch [2/10], Step [418/600], Loss: 2.3543, batch time: 0.74, accuracy:  11.00%\n",
      "Epoch [2/10], Step [419/600], Loss: 2.3475, batch time: 0.73, accuracy:  9.00%\n",
      "Epoch [2/10], Step [420/600], Loss: 2.3317, batch time: 0.75, accuracy:  11.00%\n",
      "Epoch [2/10], Step [421/600], Loss: 2.3174, batch time: 0.75, accuracy:  13.00%\n",
      "Epoch [2/10], Step [422/600], Loss: 2.3610, batch time: 0.74, accuracy:  6.00%\n",
      "Epoch [2/10], Step [423/600], Loss: 2.3127, batch time: 0.72, accuracy:  9.00%\n",
      "Epoch [2/10], Step [424/600], Loss: 2.2995, batch time: 0.73, accuracy:  14.00%\n",
      "Epoch [2/10], Step [425/600], Loss: 2.3307, batch time: 0.73, accuracy:  10.00%\n",
      "Epoch [2/10], Step [426/600], Loss: 2.3400, batch time: 0.72, accuracy:  13.00%\n",
      "Epoch [2/10], Step [427/600], Loss: 2.3404, batch time: 0.64, accuracy:  9.00%\n",
      "Epoch [2/10], Step [428/600], Loss: 2.2892, batch time: 0.61, accuracy:  14.00%\n",
      "Epoch [2/10], Step [429/600], Loss: 2.3341, batch time: 0.62, accuracy:  14.00%\n",
      "Epoch [2/10], Step [430/600], Loss: 2.3291, batch time: 0.79, accuracy:  10.00%\n",
      "Epoch [2/10], Step [431/600], Loss: 2.3564, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [2/10], Step [432/600], Loss: 2.3419, batch time: 0.63, accuracy:  11.00%\n",
      "Epoch [2/10], Step [433/600], Loss: 2.3193, batch time: 0.63, accuracy:  16.00%\n",
      "Epoch [2/10], Step [434/600], Loss: 2.3300, batch time: 0.73, accuracy:  8.00%\n",
      "Epoch [2/10], Step [435/600], Loss: 2.3330, batch time: 0.73, accuracy:  9.00%\n",
      "Epoch [2/10], Step [436/600], Loss: 2.3310, batch time: 0.72, accuracy:  17.00%\n",
      "Epoch [2/10], Step [437/600], Loss: 2.3309, batch time: 0.72, accuracy:  12.00%\n",
      "Epoch [2/10], Step [438/600], Loss: 2.3284, batch time: 0.73, accuracy:  16.00%\n",
      "Epoch [2/10], Step [439/600], Loss: 2.3226, batch time: 0.73, accuracy:  10.00%\n",
      "Epoch [2/10], Step [440/600], Loss: 2.3219, batch time: 0.73, accuracy:  15.00%\n",
      "Epoch [2/10], Step [441/600], Loss: 2.3228, batch time: 0.65, accuracy:  11.00%\n",
      "Epoch [2/10], Step [442/600], Loss: 2.3484, batch time: 0.61, accuracy:  17.00%\n",
      "Epoch [2/10], Step [443/600], Loss: 2.3312, batch time: 0.62, accuracy:  12.00%\n",
      "Epoch [2/10], Step [444/600], Loss: 2.2972, batch time: 0.68, accuracy:  17.00%\n",
      "Epoch [2/10], Step [445/600], Loss: 2.3756, batch time: 0.73, accuracy:  9.00%\n",
      "Epoch [2/10], Step [446/600], Loss: 2.3373, batch time: 0.73, accuracy:  10.00%\n",
      "Epoch [2/10], Step [447/600], Loss: 2.3357, batch time: 0.72, accuracy:  13.00%\n",
      "Epoch [2/10], Step [448/600], Loss: 2.3256, batch time: 0.72, accuracy:  9.00%\n",
      "Epoch [2/10], Step [449/600], Loss: 2.3130, batch time: 0.73, accuracy:  11.00%\n",
      "Epoch [2/10], Step [450/600], Loss: 2.3463, batch time: 0.71, accuracy:  10.00%\n",
      "Epoch [2/10], Step [451/600], Loss: 2.3196, batch time: 0.72, accuracy:  12.00%\n",
      "Epoch [2/10], Step [452/600], Loss: 2.3446, batch time: 0.72, accuracy:  12.00%\n",
      "Epoch [2/10], Step [453/600], Loss: 2.3395, batch time: 0.72, accuracy:  16.00%\n",
      "Epoch [2/10], Step [454/600], Loss: 2.3077, batch time: 0.73, accuracy:  12.00%\n",
      "Epoch [2/10], Step [455/600], Loss: 2.3253, batch time: 0.74, accuracy:  13.00%\n",
      "Epoch [2/10], Step [456/600], Loss: 2.3259, batch time: 0.72, accuracy:  10.00%\n",
      "Epoch [2/10], Step [457/600], Loss: 2.3421, batch time: 0.71, accuracy:  9.00%\n",
      "Epoch [2/10], Step [458/600], Loss: 2.3218, batch time: 0.73, accuracy:  9.00%\n",
      "Epoch [2/10], Step [459/600], Loss: 2.3190, batch time: 0.72, accuracy:  10.00%\n",
      "Epoch [2/10], Step [460/600], Loss: 2.3215, batch time: 0.73, accuracy:  10.00%\n",
      "Epoch [2/10], Step [461/600], Loss: 2.3555, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [2/10], Step [462/600], Loss: 2.3578, batch time: 0.63, accuracy:  10.00%\n",
      "Epoch [2/10], Step [463/600], Loss: 2.3323, batch time: 0.65, accuracy:  11.00%\n",
      "Epoch [2/10], Step [464/600], Loss: 2.3215, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [2/10], Step [465/600], Loss: 2.3077, batch time: 0.62, accuracy:  15.00%\n",
      "Epoch [2/10], Step [466/600], Loss: 2.3023, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [2/10], Step [467/600], Loss: 2.3408, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [2/10], Step [468/600], Loss: 2.3434, batch time: 0.71, accuracy:  7.00%\n",
      "Epoch [2/10], Step [469/600], Loss: 2.3485, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [2/10], Step [470/600], Loss: 2.3505, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [2/10], Step [471/600], Loss: 2.3296, batch time: 0.62, accuracy:  14.00%\n",
      "Epoch [2/10], Step [472/600], Loss: 2.3481, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [2/10], Step [473/600], Loss: 2.3615, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [2/10], Step [474/600], Loss: 2.3380, batch time: 0.73, accuracy:  15.00%\n",
      "Epoch [2/10], Step [475/600], Loss: 2.3377, batch time: 0.73, accuracy:  14.00%\n",
      "Epoch [2/10], Step [476/600], Loss: 2.3449, batch time: 0.73, accuracy:  8.00%\n",
      "Epoch [2/10], Step [477/600], Loss: 2.3569, batch time: 0.73, accuracy:  7.00%\n",
      "Epoch [2/10], Step [478/600], Loss: 2.3392, batch time: 0.73, accuracy:  14.00%\n",
      "Epoch [2/10], Step [479/600], Loss: 2.3693, batch time: 0.72, accuracy:  5.00%\n",
      "Epoch [2/10], Step [480/600], Loss: 2.3119, batch time: 0.72, accuracy:  9.00%\n",
      "Epoch [2/10], Step [481/600], Loss: 2.3598, batch time: 0.73, accuracy:  9.00%\n",
      "Epoch [2/10], Step [482/600], Loss: 2.3455, batch time: 0.72, accuracy:  10.00%\n",
      "Epoch [2/10], Step [483/600], Loss: 2.3143, batch time: 0.73, accuracy:  12.00%\n",
      "Epoch [2/10], Step [484/600], Loss: 2.2954, batch time: 0.64, accuracy:  13.00%\n",
      "Epoch [2/10], Step [485/600], Loss: 2.3307, batch time: 0.62, accuracy:  14.00%\n",
      "Epoch [2/10], Step [486/600], Loss: 2.3158, batch time: 0.61, accuracy:  10.00%\n",
      "Epoch [2/10], Step [487/600], Loss: 2.3130, batch time: 0.61, accuracy:  14.00%\n",
      "Epoch [2/10], Step [488/600], Loss: 2.3381, batch time: 0.63, accuracy:  13.00%\n",
      "Epoch [2/10], Step [489/600], Loss: 2.3498, batch time: 0.64, accuracy:  10.00%\n",
      "Epoch [2/10], Step [490/600], Loss: 2.3397, batch time: 0.72, accuracy:  7.00%\n",
      "Epoch [2/10], Step [491/600], Loss: 2.3191, batch time: 0.66, accuracy:  12.00%\n",
      "Epoch [2/10], Step [492/600], Loss: 2.3321, batch time: 0.61, accuracy:  7.00%\n",
      "Epoch [2/10], Step [493/600], Loss: 2.3262, batch time: 0.62, accuracy:  12.00%\n",
      "Epoch [2/10], Step [494/600], Loss: 2.3707, batch time: 0.62, accuracy:  14.00%\n",
      "Epoch [2/10], Step [495/600], Loss: 2.3167, batch time: 0.61, accuracy:  18.00%\n",
      "Epoch [2/10], Step [496/600], Loss: 2.3431, batch time: 0.61, accuracy:  10.00%\n",
      "Epoch [2/10], Step [497/600], Loss: 2.3209, batch time: 0.61, accuracy:  6.00%\n",
      "Epoch [2/10], Step [498/600], Loss: 2.3558, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [2/10], Step [499/600], Loss: 2.3443, batch time: 0.61, accuracy:  11.00%\n",
      "Epoch [2/10], Step [500/600], Loss: 2.3370, batch time: 0.61, accuracy:  9.00%\n",
      "Epoch [2/10], Step [501/600], Loss: 2.3149, batch time: 0.60, accuracy:  5.00%\n",
      "Epoch [2/10], Step [502/600], Loss: 2.3214, batch time: 0.61, accuracy:  12.00%\n",
      "Epoch [2/10], Step [503/600], Loss: 2.3300, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [2/10], Step [504/600], Loss: 2.3492, batch time: 0.73, accuracy:  7.00%\n",
      "Epoch [2/10], Step [505/600], Loss: 2.3214, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [2/10], Step [506/600], Loss: 2.3282, batch time: 0.60, accuracy:  9.00%\n",
      "Epoch [2/10], Step [507/600], Loss: 2.3629, batch time: 0.64, accuracy:  7.00%\n",
      "Epoch [2/10], Step [508/600], Loss: 2.3507, batch time: 0.86, accuracy:  12.00%\n",
      "Epoch [2/10], Step [509/600], Loss: 2.3286, batch time: 0.73, accuracy:  10.00%\n",
      "Epoch [2/10], Step [510/600], Loss: 2.3332, batch time: 0.71, accuracy:  6.00%\n",
      "Epoch [2/10], Step [511/600], Loss: 2.3011, batch time: 0.71, accuracy:  12.00%\n",
      "Epoch [2/10], Step [512/600], Loss: 2.3476, batch time: 0.64, accuracy:  12.00%\n",
      "Epoch [2/10], Step [513/600], Loss: 2.3428, batch time: 0.70, accuracy:  11.00%\n",
      "Epoch [2/10], Step [514/600], Loss: 2.3138, batch time: 0.72, accuracy:  12.00%\n",
      "Epoch [2/10], Step [515/600], Loss: 2.3183, batch time: 0.71, accuracy:  10.00%\n",
      "Epoch [2/10], Step [516/600], Loss: 2.3232, batch time: 0.72, accuracy:  14.00%\n",
      "Epoch [2/10], Step [517/600], Loss: 2.3084, batch time: 0.73, accuracy:  18.00%\n",
      "Epoch [2/10], Step [518/600], Loss: 2.3391, batch time: 0.74, accuracy:  9.00%\n",
      "Epoch [2/10], Step [519/600], Loss: 2.3278, batch time: 0.72, accuracy:  10.00%\n",
      "Epoch [2/10], Step [520/600], Loss: 2.3080, batch time: 0.73, accuracy:  15.00%\n",
      "Epoch [2/10], Step [521/600], Loss: 2.3563, batch time: 0.71, accuracy:  8.00%\n",
      "Epoch [2/10], Step [522/600], Loss: 2.3172, batch time: 0.71, accuracy:  10.00%\n",
      "Epoch [2/10], Step [523/600], Loss: 2.3253, batch time: 0.73, accuracy:  10.00%\n",
      "Epoch [2/10], Step [524/600], Loss: 2.3303, batch time: 0.72, accuracy:  11.00%\n",
      "Epoch [2/10], Step [525/600], Loss: 2.3626, batch time: 0.66, accuracy:  7.00%\n",
      "Epoch [2/10], Step [526/600], Loss: 2.3366, batch time: 0.60, accuracy:  7.00%\n",
      "Epoch [2/10], Step [527/600], Loss: 2.3226, batch time: 0.60, accuracy:  11.00%\n",
      "Epoch [2/10], Step [528/600], Loss: 2.3091, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [2/10], Step [529/600], Loss: 2.3475, batch time: 0.61, accuracy:  9.00%\n",
      "Epoch [2/10], Step [530/600], Loss: 2.2979, batch time: 0.62, accuracy:  12.00%\n",
      "Epoch [2/10], Step [531/600], Loss: 2.3216, batch time: 0.63, accuracy:  13.00%\n",
      "Epoch [2/10], Step [532/600], Loss: 2.3421, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [2/10], Step [533/600], Loss: 2.3027, batch time: 0.63, accuracy:  12.00%\n",
      "Epoch [2/10], Step [534/600], Loss: 2.2938, batch time: 0.73, accuracy:  14.00%\n",
      "Epoch [2/10], Step [535/600], Loss: 2.3133, batch time: 0.71, accuracy:  10.00%\n",
      "Epoch [2/10], Step [536/600], Loss: 2.3126, batch time: 0.73, accuracy:  17.00%\n",
      "Epoch [2/10], Step [537/600], Loss: 2.3393, batch time: 0.71, accuracy:  12.00%\n",
      "Epoch [2/10], Step [538/600], Loss: 2.3090, batch time: 0.73, accuracy:  18.00%\n",
      "Epoch [2/10], Step [539/600], Loss: 2.3215, batch time: 0.71, accuracy:  14.00%\n",
      "Epoch [2/10], Step [540/600], Loss: 2.3250, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [2/10], Step [541/600], Loss: 2.3378, batch time: 0.89, accuracy:  10.00%\n",
      "Epoch [2/10], Step [542/600], Loss: 2.3442, batch time: 0.73, accuracy:  8.00%\n",
      "Epoch [2/10], Step [543/600], Loss: 2.3203, batch time: 0.64, accuracy:  8.00%\n",
      "Epoch [2/10], Step [544/600], Loss: 2.3452, batch time: 0.70, accuracy:  8.00%\n",
      "Epoch [2/10], Step [545/600], Loss: 2.3365, batch time: 0.72, accuracy:  5.00%\n",
      "Epoch [2/10], Step [546/600], Loss: 2.3409, batch time: 0.65, accuracy:  10.00%\n",
      "Epoch [2/10], Step [547/600], Loss: 2.3279, batch time: 0.61, accuracy:  16.00%\n",
      "Epoch [2/10], Step [548/600], Loss: 2.3366, batch time: 0.63, accuracy:  12.00%\n",
      "Epoch [2/10], Step [549/600], Loss: 2.3297, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [2/10], Step [550/600], Loss: 2.3297, batch time: 0.61, accuracy:  15.00%\n",
      "Epoch [2/10], Step [551/600], Loss: 2.3391, batch time: 0.61, accuracy:  8.00%\n",
      "Epoch [2/10], Step [552/600], Loss: 2.3243, batch time: 0.61, accuracy:  10.00%\n",
      "Epoch [2/10], Step [553/600], Loss: 2.3056, batch time: 0.62, accuracy:  14.00%\n",
      "Epoch [2/10], Step [554/600], Loss: 2.3206, batch time: 0.62, accuracy:  12.00%\n",
      "Epoch [2/10], Step [555/600], Loss: 2.3234, batch time: 0.61, accuracy:  10.00%\n",
      "Epoch [2/10], Step [556/600], Loss: 2.3105, batch time: 0.61, accuracy:  7.00%\n",
      "Epoch [2/10], Step [557/600], Loss: 2.3041, batch time: 0.70, accuracy:  9.00%\n",
      "Epoch [2/10], Step [558/600], Loss: 2.3218, batch time: 0.73, accuracy:  10.00%\n",
      "Epoch [2/10], Step [559/600], Loss: 2.3206, batch time: 0.71, accuracy:  8.00%\n",
      "Epoch [2/10], Step [560/600], Loss: 2.3539, batch time: 0.71, accuracy:  8.00%\n",
      "Epoch [2/10], Step [561/600], Loss: 2.3261, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [2/10], Step [562/600], Loss: 2.2957, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [2/10], Step [563/600], Loss: 2.3280, batch time: 0.63, accuracy:  11.00%\n",
      "Epoch [2/10], Step [564/600], Loss: 2.3331, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [2/10], Step [565/600], Loss: 2.3401, batch time: 0.61, accuracy:  8.00%\n",
      "Epoch [2/10], Step [566/600], Loss: 2.3271, batch time: 0.61, accuracy:  14.00%\n",
      "Epoch [2/10], Step [567/600], Loss: 2.2996, batch time: 0.61, accuracy:  13.00%\n",
      "Epoch [2/10], Step [568/600], Loss: 2.3067, batch time: 0.61, accuracy:  10.00%\n",
      "Epoch [2/10], Step [569/600], Loss: 2.3455, batch time: 0.62, accuracy:  3.00%\n",
      "Epoch [2/10], Step [570/600], Loss: 2.3259, batch time: 0.71, accuracy:  16.00%\n",
      "Epoch [2/10], Step [571/600], Loss: 2.3101, batch time: 0.72, accuracy:  12.00%\n",
      "Epoch [2/10], Step [572/600], Loss: 2.2956, batch time: 0.72, accuracy:  22.00%\n",
      "Epoch [2/10], Step [573/600], Loss: 2.3446, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [2/10], Step [574/600], Loss: 2.3291, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [2/10], Step [575/600], Loss: 2.3304, batch time: 0.61, accuracy:  8.00%\n",
      "Epoch [2/10], Step [576/600], Loss: 2.3178, batch time: 0.63, accuracy:  10.00%\n",
      "Epoch [2/10], Step [577/600], Loss: 2.3132, batch time: 0.61, accuracy:  8.00%\n",
      "Epoch [2/10], Step [578/600], Loss: 2.3490, batch time: 0.62, accuracy:  3.00%\n",
      "Epoch [2/10], Step [579/600], Loss: 2.3064, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [2/10], Step [580/600], Loss: 2.3303, batch time: 0.61, accuracy:  16.00%\n",
      "Epoch [2/10], Step [581/600], Loss: 2.3235, batch time: 0.61, accuracy:  7.00%\n",
      "Epoch [2/10], Step [582/600], Loss: 2.3254, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [2/10], Step [583/600], Loss: 2.3160, batch time: 0.61, accuracy:  11.00%\n",
      "Epoch [2/10], Step [584/600], Loss: 2.3251, batch time: 0.62, accuracy:  14.00%\n",
      "Epoch [2/10], Step [585/600], Loss: 2.3143, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [2/10], Step [586/600], Loss: 2.3468, batch time: 0.86, accuracy:  6.00%\n",
      "Epoch [2/10], Step [587/600], Loss: 2.3418, batch time: 0.72, accuracy:  14.00%\n",
      "Epoch [2/10], Step [588/600], Loss: 2.3177, batch time: 0.72, accuracy:  9.00%\n",
      "Epoch [2/10], Step [589/600], Loss: 2.3170, batch time: 0.73, accuracy:  10.00%\n",
      "Epoch [2/10], Step [590/600], Loss: 2.3016, batch time: 0.73, accuracy:  13.00%\n",
      "Epoch [2/10], Step [591/600], Loss: 2.3253, batch time: 0.72, accuracy:  11.00%\n",
      "Epoch [2/10], Step [592/600], Loss: 2.3312, batch time: 0.72, accuracy:  11.00%\n",
      "Epoch [2/10], Step [593/600], Loss: 2.2983, batch time: 0.73, accuracy:  17.00%\n",
      "Epoch [2/10], Step [594/600], Loss: 2.3168, batch time: 0.73, accuracy:  13.00%\n",
      "Epoch [2/10], Step [595/600], Loss: 2.3326, batch time: 0.72, accuracy:  9.00%\n",
      "Epoch [2/10], Step [596/600], Loss: 2.3717, batch time: 0.71, accuracy:  8.00%\n",
      "Epoch [2/10], Step [597/600], Loss: 2.3292, batch time: 0.71, accuracy:  7.00%\n",
      "Epoch [2/10], Step [598/600], Loss: 2.3108, batch time: 0.72, accuracy:  13.00%\n",
      "Epoch [2/10], Step [599/600], Loss: 2.3155, batch time: 0.71, accuracy:  11.00%\n",
      "Epoch [2/10], Step [600/600], Loss: 2.3390, batch time: 0.71, accuracy:  12.00%\n",
      "Epoch [3/10], Step [1/600], Loss: 2.3097, batch time: 0.72, accuracy:  13.00%\n",
      "Epoch [3/10], Step [2/600], Loss: 2.3021, batch time: 0.72, accuracy:  10.00%\n",
      "Epoch [3/10], Step [3/600], Loss: 2.3381, batch time: 0.73, accuracy:  9.00%\n",
      "Epoch [3/10], Step [4/600], Loss: 2.3410, batch time: 0.73, accuracy:  9.00%\n",
      "Epoch [3/10], Step [5/600], Loss: 2.3114, batch time: 0.72, accuracy:  12.00%\n",
      "Epoch [3/10], Step [6/600], Loss: 2.3182, batch time: 0.72, accuracy:  9.00%\n",
      "Epoch [3/10], Step [7/600], Loss: 2.3321, batch time: 0.73, accuracy:  6.00%\n",
      "Epoch [3/10], Step [8/600], Loss: 2.3149, batch time: 0.71, accuracy:  16.00%\n",
      "Epoch [3/10], Step [9/600], Loss: 2.3241, batch time: 0.66, accuracy:  9.00%\n",
      "Epoch [3/10], Step [10/600], Loss: 2.2927, batch time: 0.59, accuracy:  14.00%\n",
      "Epoch [3/10], Step [11/600], Loss: 2.3154, batch time: 0.64, accuracy:  11.00%\n",
      "Epoch [3/10], Step [12/600], Loss: 2.3087, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [3/10], Step [13/600], Loss: 2.3079, batch time: 0.61, accuracy:  8.00%\n",
      "Epoch [3/10], Step [14/600], Loss: 2.3250, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [3/10], Step [15/600], Loss: 2.3047, batch time: 0.60, accuracy:  15.00%\n",
      "Epoch [3/10], Step [16/600], Loss: 2.3223, batch time: 0.61, accuracy:  10.00%\n",
      "Epoch [3/10], Step [17/600], Loss: 2.3064, batch time: 0.60, accuracy:  10.00%\n",
      "Epoch [3/10], Step [18/600], Loss: 2.3514, batch time: 0.61, accuracy:  7.00%\n",
      "Epoch [3/10], Step [19/600], Loss: 2.3015, batch time: 0.61, accuracy:  11.00%\n",
      "Epoch [3/10], Step [20/600], Loss: 2.3447, batch time: 0.59, accuracy:  9.00%\n",
      "Epoch [3/10], Step [21/600], Loss: 2.2990, batch time: 0.65, accuracy:  11.00%\n",
      "Epoch [3/10], Step [22/600], Loss: 2.2958, batch time: 0.61, accuracy:  11.00%\n",
      "Epoch [3/10], Step [23/600], Loss: 2.3123, batch time: 0.60, accuracy:  11.00%\n",
      "Epoch [3/10], Step [24/600], Loss: 2.2996, batch time: 0.61, accuracy:  13.00%\n",
      "Epoch [3/10], Step [25/600], Loss: 2.3047, batch time: 0.61, accuracy:  9.00%\n",
      "Epoch [3/10], Step [26/600], Loss: 2.3234, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [3/10], Step [27/600], Loss: 2.3192, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [3/10], Step [28/600], Loss: 2.3349, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [3/10], Step [29/600], Loss: 2.3187, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [3/10], Step [30/600], Loss: 2.2938, batch time: 0.60, accuracy:  11.00%\n",
      "Epoch [3/10], Step [31/600], Loss: 2.3152, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [3/10], Step [32/600], Loss: 2.3661, batch time: 0.60, accuracy:  5.00%\n",
      "Epoch [3/10], Step [33/600], Loss: 2.3194, batch time: 0.59, accuracy:  9.00%\n",
      "Epoch [3/10], Step [34/600], Loss: 2.3205, batch time: 0.59, accuracy:  5.00%\n",
      "Epoch [3/10], Step [35/600], Loss: 2.3204, batch time: 0.61, accuracy:  12.00%\n",
      "Epoch [3/10], Step [36/600], Loss: 2.3486, batch time: 0.59, accuracy:  8.00%\n",
      "Epoch [3/10], Step [37/600], Loss: 2.3028, batch time: 0.65, accuracy:  14.00%\n",
      "Epoch [3/10], Step [38/600], Loss: 2.3133, batch time: 0.60, accuracy:  10.00%\n",
      "Epoch [3/10], Step [39/600], Loss: 2.3130, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [3/10], Step [40/600], Loss: 2.2827, batch time: 0.59, accuracy:  20.00%\n",
      "Epoch [3/10], Step [41/600], Loss: 2.3027, batch time: 0.59, accuracy:  14.00%\n",
      "Epoch [3/10], Step [42/600], Loss: 2.3227, batch time: 0.59, accuracy:  12.00%\n",
      "Epoch [3/10], Step [43/600], Loss: 2.3178, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [3/10], Step [44/600], Loss: 2.3267, batch time: 0.65, accuracy:  6.00%\n",
      "Epoch [3/10], Step [45/600], Loss: 2.3013, batch time: 0.61, accuracy:  16.00%\n",
      "Epoch [3/10], Step [46/600], Loss: 2.3194, batch time: 0.60, accuracy:  13.00%\n",
      "Epoch [3/10], Step [47/600], Loss: 2.3238, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [3/10], Step [48/600], Loss: 2.3265, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [3/10], Step [49/600], Loss: 2.3027, batch time: 0.58, accuracy:  17.00%\n",
      "Epoch [3/10], Step [50/600], Loss: 2.3558, batch time: 0.59, accuracy:  8.00%\n",
      "Epoch [3/10], Step [51/600], Loss: 2.3321, batch time: 0.60, accuracy:  7.00%\n",
      "Epoch [3/10], Step [52/600], Loss: 2.3133, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [3/10], Step [53/600], Loss: 2.3139, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [3/10], Step [54/600], Loss: 2.3315, batch time: 0.59, accuracy:  8.00%\n",
      "Epoch [3/10], Step [55/600], Loss: 2.3053, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [3/10], Step [56/600], Loss: 2.3337, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [3/10], Step [57/600], Loss: 2.3235, batch time: 0.61, accuracy:  7.00%\n",
      "Epoch [3/10], Step [58/600], Loss: 2.3115, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [3/10], Step [59/600], Loss: 2.3313, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [3/10], Step [60/600], Loss: 2.3163, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [3/10], Step [61/600], Loss: 2.3196, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [3/10], Step [62/600], Loss: 2.3204, batch time: 0.69, accuracy:  7.00%\n",
      "Epoch [3/10], Step [63/600], Loss: 2.3359, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [3/10], Step [64/600], Loss: 2.2999, batch time: 0.72, accuracy:  9.00%\n",
      "Epoch [3/10], Step [65/600], Loss: 2.3147, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [3/10], Step [66/600], Loss: 2.3305, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [3/10], Step [67/600], Loss: 2.3315, batch time: 0.59, accuracy:  6.00%\n",
      "Epoch [3/10], Step [68/600], Loss: 2.3185, batch time: 0.60, accuracy:  6.00%\n",
      "Epoch [3/10], Step [69/600], Loss: 2.3206, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [3/10], Step [70/600], Loss: 2.3253, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [3/10], Step [71/600], Loss: 2.3396, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [3/10], Step [72/600], Loss: 2.3122, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [3/10], Step [73/600], Loss: 2.3053, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [3/10], Step [74/600], Loss: 2.3398, batch time: 0.65, accuracy:  6.00%\n",
      "Epoch [3/10], Step [75/600], Loss: 2.3210, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [3/10], Step [76/600], Loss: 2.3325, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [3/10], Step [77/600], Loss: 2.3275, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [3/10], Step [78/600], Loss: 2.3181, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [3/10], Step [79/600], Loss: 2.3130, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [3/10], Step [80/600], Loss: 2.3298, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [3/10], Step [81/600], Loss: 2.3173, batch time: 0.58, accuracy:  17.00%\n",
      "Epoch [3/10], Step [82/600], Loss: 2.3190, batch time: 0.58, accuracy:  5.00%\n",
      "Epoch [3/10], Step [83/600], Loss: 2.3098, batch time: 0.63, accuracy:  12.00%\n",
      "Epoch [3/10], Step [84/600], Loss: 2.3059, batch time: 0.61, accuracy:  12.00%\n",
      "Epoch [3/10], Step [85/600], Loss: 2.3038, batch time: 0.60, accuracy:  12.00%\n",
      "Epoch [3/10], Step [86/600], Loss: 2.3121, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [3/10], Step [87/600], Loss: 2.3312, batch time: 0.60, accuracy:  10.00%\n",
      "Epoch [3/10], Step [88/600], Loss: 2.3365, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [3/10], Step [89/600], Loss: 2.3182, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [3/10], Step [90/600], Loss: 2.3270, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [3/10], Step [91/600], Loss: 2.3195, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [3/10], Step [92/600], Loss: 2.3227, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [3/10], Step [93/600], Loss: 2.3203, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [3/10], Step [94/600], Loss: 2.3348, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [3/10], Step [95/600], Loss: 2.3149, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [3/10], Step [96/600], Loss: 2.3266, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [3/10], Step [97/600], Loss: 2.3413, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [3/10], Step [98/600], Loss: 2.3212, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [3/10], Step [99/600], Loss: 2.2869, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [3/10], Step [100/600], Loss: 2.3199, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [3/10], Step [101/600], Loss: 2.3176, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [3/10], Step [102/600], Loss: 2.3359, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [3/10], Step [103/600], Loss: 2.3340, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [3/10], Step [104/600], Loss: 2.2864, batch time: 0.59, accuracy:  17.00%\n",
      "Epoch [3/10], Step [105/600], Loss: 2.3286, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [3/10], Step [106/600], Loss: 2.3108, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [3/10], Step [107/600], Loss: 2.3098, batch time: 0.64, accuracy:  10.00%\n",
      "Epoch [3/10], Step [108/600], Loss: 2.3431, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [3/10], Step [109/600], Loss: 2.3279, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [3/10], Step [110/600], Loss: 2.3341, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [3/10], Step [111/600], Loss: 2.3524, batch time: 0.58, accuracy:  4.00%\n",
      "Epoch [3/10], Step [112/600], Loss: 2.3274, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [3/10], Step [113/600], Loss: 2.3381, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [3/10], Step [114/600], Loss: 2.3147, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [3/10], Step [115/600], Loss: 2.3438, batch time: 0.61, accuracy:  6.00%\n",
      "Epoch [3/10], Step [116/600], Loss: 2.3309, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [3/10], Step [117/600], Loss: 2.3085, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [3/10], Step [118/600], Loss: 2.3467, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [3/10], Step [119/600], Loss: 2.3248, batch time: 0.70, accuracy:  9.00%\n",
      "Epoch [3/10], Step [120/600], Loss: 2.3058, batch time: 0.68, accuracy:  5.00%\n",
      "Epoch [3/10], Step [121/600], Loss: 2.2883, batch time: 0.67, accuracy:  15.00%\n",
      "Epoch [3/10], Step [122/600], Loss: 2.3623, batch time: 0.68, accuracy:  5.00%\n",
      "Epoch [3/10], Step [123/600], Loss: 2.3115, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [3/10], Step [124/600], Loss: 2.3062, batch time: 0.64, accuracy:  10.00%\n",
      "Epoch [3/10], Step [125/600], Loss: 2.3001, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [3/10], Step [126/600], Loss: 2.3356, batch time: 0.60, accuracy:  9.00%\n",
      "Epoch [3/10], Step [127/600], Loss: 2.3211, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [3/10], Step [128/600], Loss: 2.3252, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [3/10], Step [129/600], Loss: 2.3127, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [3/10], Step [130/600], Loss: 2.3160, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [3/10], Step [131/600], Loss: 2.3283, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [3/10], Step [132/600], Loss: 2.2974, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [3/10], Step [133/600], Loss: 2.3242, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [3/10], Step [134/600], Loss: 2.3171, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [3/10], Step [135/600], Loss: 2.3164, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [3/10], Step [136/600], Loss: 2.3160, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [3/10], Step [137/600], Loss: 2.3213, batch time: 0.65, accuracy:  6.00%\n",
      "Epoch [3/10], Step [138/600], Loss: 2.3144, batch time: 0.69, accuracy:  6.00%\n",
      "Epoch [3/10], Step [139/600], Loss: 2.3358, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [3/10], Step [140/600], Loss: 2.3370, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [3/10], Step [141/600], Loss: 2.3108, batch time: 0.60, accuracy:  9.00%\n",
      "Epoch [3/10], Step [142/600], Loss: 2.3345, batch time: 0.80, accuracy:  9.00%\n",
      "Epoch [3/10], Step [143/600], Loss: 2.3250, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [3/10], Step [144/600], Loss: 2.3019, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [3/10], Step [145/600], Loss: 2.3377, batch time: 0.69, accuracy:  4.00%\n",
      "Epoch [3/10], Step [146/600], Loss: 2.3313, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [3/10], Step [147/600], Loss: 2.3220, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [3/10], Step [148/600], Loss: 2.3185, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [3/10], Step [149/600], Loss: 2.3058, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [3/10], Step [150/600], Loss: 2.3357, batch time: 0.59, accuracy:  8.00%\n",
      "Epoch [3/10], Step [151/600], Loss: 2.3129, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [3/10], Step [152/600], Loss: 2.2923, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [3/10], Step [153/600], Loss: 2.2902, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [3/10], Step [154/600], Loss: 2.3152, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [3/10], Step [155/600], Loss: 2.3087, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [3/10], Step [156/600], Loss: 2.2961, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [3/10], Step [157/600], Loss: 2.2999, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [3/10], Step [158/600], Loss: 2.3366, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [3/10], Step [159/600], Loss: 2.3154, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [3/10], Step [160/600], Loss: 2.3126, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [3/10], Step [161/600], Loss: 2.3312, batch time: 0.65, accuracy:  4.00%\n",
      "Epoch [3/10], Step [162/600], Loss: 2.3231, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [3/10], Step [163/600], Loss: 2.2956, batch time: 0.60, accuracy:  10.00%\n",
      "Epoch [3/10], Step [164/600], Loss: 2.3078, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [3/10], Step [165/600], Loss: 2.3342, batch time: 0.57, accuracy:  5.00%\n",
      "Epoch [3/10], Step [166/600], Loss: 2.3285, batch time: 0.56, accuracy:  3.00%\n",
      "Epoch [3/10], Step [167/600], Loss: 2.3102, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [3/10], Step [168/600], Loss: 2.3026, batch time: 0.56, accuracy:  16.00%\n",
      "Epoch [3/10], Step [169/600], Loss: 2.3090, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [3/10], Step [170/600], Loss: 2.3238, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [3/10], Step [171/600], Loss: 2.2895, batch time: 0.65, accuracy:  19.00%\n",
      "Epoch [3/10], Step [172/600], Loss: 2.3317, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [3/10], Step [173/600], Loss: 2.3057, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [3/10], Step [174/600], Loss: 2.3244, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [3/10], Step [175/600], Loss: 2.3069, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [3/10], Step [176/600], Loss: 2.2865, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [3/10], Step [177/600], Loss: 2.3260, batch time: 0.56, accuracy:  7.00%\n",
      "Epoch [3/10], Step [178/600], Loss: 2.2929, batch time: 0.61, accuracy:  10.00%\n",
      "Epoch [3/10], Step [179/600], Loss: 2.3065, batch time: 0.70, accuracy:  14.00%\n",
      "Epoch [3/10], Step [180/600], Loss: 2.3141, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [3/10], Step [181/600], Loss: 2.3224, batch time: 0.65, accuracy:  6.00%\n",
      "Epoch [3/10], Step [182/600], Loss: 2.2957, batch time: 0.58, accuracy:  15.00%\n",
      "Epoch [3/10], Step [183/600], Loss: 2.3212, batch time: 0.59, accuracy:  9.00%\n",
      "Epoch [3/10], Step [184/600], Loss: 2.3380, batch time: 0.60, accuracy:  9.00%\n",
      "Epoch [3/10], Step [185/600], Loss: 2.3043, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [3/10], Step [186/600], Loss: 2.3199, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [3/10], Step [187/600], Loss: 2.3113, batch time: 0.56, accuracy:  6.00%\n",
      "Epoch [3/10], Step [188/600], Loss: 2.3119, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [3/10], Step [189/600], Loss: 2.3114, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [3/10], Step [190/600], Loss: 2.3005, batch time: 0.66, accuracy:  9.00%\n",
      "Epoch [3/10], Step [191/600], Loss: 2.3103, batch time: 0.67, accuracy:  14.00%\n",
      "Epoch [3/10], Step [192/600], Loss: 2.3173, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [3/10], Step [193/600], Loss: 2.3101, batch time: 0.68, accuracy:  5.00%\n",
      "Epoch [3/10], Step [194/600], Loss: 2.3162, batch time: 0.68, accuracy:  15.00%\n",
      "Epoch [3/10], Step [195/600], Loss: 2.3111, batch time: 0.70, accuracy:  15.00%\n",
      "Epoch [3/10], Step [196/600], Loss: 2.3130, batch time: 0.67, accuracy:  14.00%\n",
      "Epoch [3/10], Step [197/600], Loss: 2.3296, batch time: 0.69, accuracy:  4.00%\n",
      "Epoch [3/10], Step [198/600], Loss: 2.3205, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [3/10], Step [199/600], Loss: 2.3048, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [3/10], Step [200/600], Loss: 2.3151, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [3/10], Step [201/600], Loss: 2.3095, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [3/10], Step [202/600], Loss: 2.3267, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [3/10], Step [203/600], Loss: 2.3125, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [3/10], Step [204/600], Loss: 2.3363, batch time: 0.68, accuracy:  4.00%\n",
      "Epoch [3/10], Step [205/600], Loss: 2.3141, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [3/10], Step [206/600], Loss: 2.3173, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [3/10], Step [207/600], Loss: 2.2955, batch time: 0.60, accuracy:  7.00%\n",
      "Epoch [3/10], Step [208/600], Loss: 2.3071, batch time: 0.58, accuracy:  15.00%\n",
      "Epoch [3/10], Step [209/600], Loss: 2.2910, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [3/10], Step [210/600], Loss: 2.3074, batch time: 0.62, accuracy:  12.00%\n",
      "Epoch [3/10], Step [211/600], Loss: 2.2779, batch time: 0.57, accuracy:  17.00%\n",
      "Epoch [3/10], Step [212/600], Loss: 2.3196, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [3/10], Step [213/600], Loss: 2.3169, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [3/10], Step [214/600], Loss: 2.3120, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [3/10], Step [215/600], Loss: 2.3055, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [3/10], Step [216/600], Loss: 2.3095, batch time: 0.64, accuracy:  12.00%\n",
      "Epoch [3/10], Step [217/600], Loss: 2.3198, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [3/10], Step [218/600], Loss: 2.2948, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [3/10], Step [219/600], Loss: 2.3206, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [3/10], Step [220/600], Loss: 2.3122, batch time: 0.72, accuracy:  15.00%\n",
      "Epoch [3/10], Step [221/600], Loss: 2.3040, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [3/10], Step [222/600], Loss: 2.2944, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [3/10], Step [223/600], Loss: 2.2958, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [3/10], Step [224/600], Loss: 2.3370, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [3/10], Step [225/600], Loss: 2.3160, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [3/10], Step [226/600], Loss: 2.3163, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [3/10], Step [227/600], Loss: 2.3033, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [3/10], Step [228/600], Loss: 2.3109, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [3/10], Step [229/600], Loss: 2.3304, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [3/10], Step [230/600], Loss: 2.3357, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [3/10], Step [231/600], Loss: 2.3117, batch time: 0.70, accuracy:  8.00%\n",
      "Epoch [3/10], Step [232/600], Loss: 2.3074, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [3/10], Step [233/600], Loss: 2.3125, batch time: 0.70, accuracy:  7.00%\n",
      "Epoch [3/10], Step [234/600], Loss: 2.3267, batch time: 0.69, accuracy:  5.00%\n",
      "Epoch [3/10], Step [235/600], Loss: 2.3187, batch time: 0.69, accuracy:  6.00%\n",
      "Epoch [3/10], Step [236/600], Loss: 2.3503, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [3/10], Step [237/600], Loss: 2.3126, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [3/10], Step [238/600], Loss: 2.3271, batch time: 0.72, accuracy:  8.00%\n",
      "Epoch [3/10], Step [239/600], Loss: 2.3034, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [3/10], Step [240/600], Loss: 2.3098, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [3/10], Step [241/600], Loss: 2.3122, batch time: 0.70, accuracy:  9.00%\n",
      "Epoch [3/10], Step [242/600], Loss: 2.2790, batch time: 0.69, accuracy:  15.00%\n",
      "Epoch [3/10], Step [243/600], Loss: 2.3017, batch time: 0.69, accuracy:  16.00%\n",
      "Epoch [3/10], Step [244/600], Loss: 2.3121, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [3/10], Step [245/600], Loss: 2.3126, batch time: 0.68, accuracy:  15.00%\n",
      "Epoch [3/10], Step [246/600], Loss: 2.3112, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [3/10], Step [247/600], Loss: 2.3185, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [3/10], Step [248/600], Loss: 2.3098, batch time: 0.69, accuracy:  7.00%\n",
      "Epoch [3/10], Step [249/600], Loss: 2.3190, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [3/10], Step [250/600], Loss: 2.3144, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [3/10], Step [251/600], Loss: 2.3319, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [3/10], Step [252/600], Loss: 2.3161, batch time: 0.69, accuracy:  7.00%\n",
      "Epoch [3/10], Step [253/600], Loss: 2.3070, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [3/10], Step [254/600], Loss: 2.3098, batch time: 0.69, accuracy:  16.00%\n",
      "Epoch [3/10], Step [255/600], Loss: 2.2944, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [3/10], Step [256/600], Loss: 2.3066, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [3/10], Step [257/600], Loss: 2.3067, batch time: 0.70, accuracy:  12.00%\n",
      "Epoch [3/10], Step [258/600], Loss: 2.3024, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [3/10], Step [259/600], Loss: 2.3033, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [3/10], Step [260/600], Loss: 2.3138, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [3/10], Step [261/600], Loss: 2.3198, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [3/10], Step [262/600], Loss: 2.3156, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [3/10], Step [263/600], Loss: 2.3039, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [3/10], Step [264/600], Loss: 2.3119, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [3/10], Step [265/600], Loss: 2.3261, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [3/10], Step [266/600], Loss: 2.3064, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [3/10], Step [267/600], Loss: 2.2886, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [3/10], Step [268/600], Loss: 2.3114, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [3/10], Step [269/600], Loss: 2.2908, batch time: 0.69, accuracy:  14.00%\n",
      "Epoch [3/10], Step [270/600], Loss: 2.2894, batch time: 0.69, accuracy:  14.00%\n",
      "Epoch [3/10], Step [271/600], Loss: 2.3026, batch time: 0.69, accuracy:  15.00%\n",
      "Epoch [3/10], Step [272/600], Loss: 2.3127, batch time: 0.70, accuracy:  14.00%\n",
      "Epoch [3/10], Step [273/600], Loss: 2.3199, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [3/10], Step [274/600], Loss: 2.3180, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [3/10], Step [275/600], Loss: 2.3079, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [3/10], Step [276/600], Loss: 2.3085, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [3/10], Step [277/600], Loss: 2.3136, batch time: 0.70, accuracy:  13.00%\n",
      "Epoch [3/10], Step [278/600], Loss: 2.3190, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [3/10], Step [279/600], Loss: 2.3016, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [3/10], Step [280/600], Loss: 2.2969, batch time: 0.68, accuracy:  15.00%\n",
      "Epoch [3/10], Step [281/600], Loss: 2.3142, batch time: 0.69, accuracy:  15.00%\n",
      "Epoch [3/10], Step [282/600], Loss: 2.2796, batch time: 0.69, accuracy:  17.00%\n",
      "Epoch [3/10], Step [283/600], Loss: 2.3112, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [3/10], Step [284/600], Loss: 2.3118, batch time: 0.58, accuracy:  18.00%\n",
      "Epoch [3/10], Step [285/600], Loss: 2.2838, batch time: 0.56, accuracy:  23.00%\n",
      "Epoch [3/10], Step [286/600], Loss: 2.3099, batch time: 0.61, accuracy:  13.00%\n",
      "Epoch [3/10], Step [287/600], Loss: 2.3302, batch time: 0.61, accuracy:  14.00%\n",
      "Epoch [3/10], Step [288/600], Loss: 2.3066, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [3/10], Step [289/600], Loss: 2.3115, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [3/10], Step [290/600], Loss: 2.3101, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [3/10], Step [291/600], Loss: 2.3139, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [3/10], Step [292/600], Loss: 2.2980, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [3/10], Step [293/600], Loss: 2.3245, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [3/10], Step [294/600], Loss: 2.3024, batch time: 0.58, accuracy:  16.00%\n",
      "Epoch [3/10], Step [295/600], Loss: 2.2850, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [3/10], Step [296/600], Loss: 2.2995, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [3/10], Step [297/600], Loss: 2.2906, batch time: 0.61, accuracy:  12.00%\n",
      "Epoch [3/10], Step [298/600], Loss: 2.3341, batch time: 0.71, accuracy:  15.00%\n",
      "Epoch [3/10], Step [299/600], Loss: 2.3129, batch time: 0.61, accuracy:  14.00%\n",
      "Epoch [3/10], Step [300/600], Loss: 2.3145, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [3/10], Step [301/600], Loss: 2.3082, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [3/10], Step [302/600], Loss: 2.3047, batch time: 0.57, accuracy:  17.00%\n",
      "Epoch [3/10], Step [303/600], Loss: 2.3028, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [3/10], Step [304/600], Loss: 2.3105, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [3/10], Step [305/600], Loss: 2.3096, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [3/10], Step [306/600], Loss: 2.3092, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [3/10], Step [307/600], Loss: 2.3144, batch time: 0.69, accuracy:  7.00%\n",
      "Epoch [3/10], Step [308/600], Loss: 2.3021, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [3/10], Step [309/600], Loss: 2.3107, batch time: 0.68, accuracy:  15.00%\n",
      "Epoch [3/10], Step [310/600], Loss: 2.3010, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [3/10], Step [311/600], Loss: 2.3009, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [3/10], Step [312/600], Loss: 2.3299, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [3/10], Step [313/600], Loss: 2.3078, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [3/10], Step [314/600], Loss: 2.3140, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [3/10], Step [315/600], Loss: 2.3285, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [3/10], Step [316/600], Loss: 2.3079, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [3/10], Step [317/600], Loss: 2.3060, batch time: 0.59, accuracy:  17.00%\n",
      "Epoch [3/10], Step [318/600], Loss: 2.3263, batch time: 0.57, accuracy:  5.00%\n",
      "Epoch [3/10], Step [319/600], Loss: 2.2967, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [3/10], Step [320/600], Loss: 2.3187, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [3/10], Step [321/600], Loss: 2.3071, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [3/10], Step [322/600], Loss: 2.2997, batch time: 0.57, accuracy:  18.00%\n",
      "Epoch [3/10], Step [323/600], Loss: 2.3177, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [3/10], Step [324/600], Loss: 2.2975, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [3/10], Step [325/600], Loss: 2.3077, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [3/10], Step [326/600], Loss: 2.3180, batch time: 0.62, accuracy:  14.00%\n",
      "Epoch [3/10], Step [327/600], Loss: 2.3006, batch time: 0.61, accuracy:  16.00%\n",
      "Epoch [3/10], Step [328/600], Loss: 2.3241, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [3/10], Step [329/600], Loss: 2.3024, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [3/10], Step [330/600], Loss: 2.3040, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [3/10], Step [331/600], Loss: 2.3014, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [3/10], Step [332/600], Loss: 2.3256, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [3/10], Step [333/600], Loss: 2.2866, batch time: 0.58, accuracy:  15.00%\n",
      "Epoch [3/10], Step [334/600], Loss: 2.3100, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [3/10], Step [335/600], Loss: 2.3142, batch time: 0.59, accuracy:  5.00%\n",
      "Epoch [3/10], Step [336/600], Loss: 2.3017, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [3/10], Step [337/600], Loss: 2.3074, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [3/10], Step [338/600], Loss: 2.3036, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [3/10], Step [339/600], Loss: 2.3176, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [3/10], Step [340/600], Loss: 2.3154, batch time: 0.64, accuracy:  11.00%\n",
      "Epoch [3/10], Step [341/600], Loss: 2.2966, batch time: 0.67, accuracy:  19.00%\n",
      "Epoch [3/10], Step [342/600], Loss: 2.3289, batch time: 0.61, accuracy:  10.00%\n",
      "Epoch [3/10], Step [343/600], Loss: 2.3226, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [3/10], Step [344/600], Loss: 2.3089, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [3/10], Step [345/600], Loss: 2.3173, batch time: 0.56, accuracy:  15.00%\n",
      "Epoch [3/10], Step [346/600], Loss: 2.3355, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [3/10], Step [347/600], Loss: 2.2940, batch time: 0.57, accuracy:  18.00%\n",
      "Epoch [3/10], Step [348/600], Loss: 2.3184, batch time: 0.58, accuracy:  16.00%\n",
      "Epoch [3/10], Step [349/600], Loss: 2.3092, batch time: 0.59, accuracy:  15.00%\n",
      "Epoch [3/10], Step [350/600], Loss: 2.2959, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [3/10], Step [351/600], Loss: 2.3181, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [3/10], Step [352/600], Loss: 2.2924, batch time: 0.57, accuracy:  17.00%\n",
      "Epoch [3/10], Step [353/600], Loss: 2.3034, batch time: 0.58, accuracy:  15.00%\n",
      "Epoch [3/10], Step [354/600], Loss: 2.3114, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [3/10], Step [355/600], Loss: 2.3036, batch time: 0.59, accuracy:  17.00%\n",
      "Epoch [3/10], Step [356/600], Loss: 2.3307, batch time: 0.60, accuracy:  9.00%\n",
      "Epoch [3/10], Step [357/600], Loss: 2.2943, batch time: 0.67, accuracy:  20.00%\n",
      "Epoch [3/10], Step [358/600], Loss: 2.2891, batch time: 0.69, accuracy:  19.00%\n",
      "Epoch [3/10], Step [359/600], Loss: 2.3355, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [3/10], Step [360/600], Loss: 2.3051, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [3/10], Step [361/600], Loss: 2.3212, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [3/10], Step [362/600], Loss: 2.3043, batch time: 0.68, accuracy:  20.00%\n",
      "Epoch [3/10], Step [363/600], Loss: 2.3076, batch time: 0.68, accuracy:  17.00%\n",
      "Epoch [3/10], Step [364/600], Loss: 2.3004, batch time: 0.70, accuracy:  23.00%\n",
      "Epoch [3/10], Step [365/600], Loss: 2.2990, batch time: 0.68, accuracy:  17.00%\n",
      "Epoch [3/10], Step [366/600], Loss: 2.3202, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [3/10], Step [367/600], Loss: 2.3065, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [3/10], Step [368/600], Loss: 2.3049, batch time: 0.56, accuracy:  16.00%\n",
      "Epoch [3/10], Step [369/600], Loss: 2.3176, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [3/10], Step [370/600], Loss: 2.3003, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [3/10], Step [371/600], Loss: 2.3115, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [3/10], Step [372/600], Loss: 2.2990, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [3/10], Step [373/600], Loss: 2.3123, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [3/10], Step [374/600], Loss: 2.3033, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [3/10], Step [375/600], Loss: 2.3113, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [3/10], Step [376/600], Loss: 2.2963, batch time: 0.71, accuracy:  11.00%\n",
      "Epoch [3/10], Step [377/600], Loss: 2.3224, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [3/10], Step [378/600], Loss: 2.3239, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [3/10], Step [379/600], Loss: 2.2999, batch time: 0.57, accuracy:  18.00%\n",
      "Epoch [3/10], Step [380/600], Loss: 2.3137, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [3/10], Step [381/600], Loss: 2.2926, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [3/10], Step [382/600], Loss: 2.3066, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [3/10], Step [383/600], Loss: 2.3070, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [3/10], Step [384/600], Loss: 2.3074, batch time: 0.57, accuracy:  20.00%\n",
      "Epoch [3/10], Step [385/600], Loss: 2.3138, batch time: 0.61, accuracy:  15.00%\n",
      "Epoch [3/10], Step [386/600], Loss: 2.3098, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [3/10], Step [387/600], Loss: 2.3175, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [3/10], Step [388/600], Loss: 2.3215, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [3/10], Step [389/600], Loss: 2.3324, batch time: 0.56, accuracy:  6.00%\n",
      "Epoch [3/10], Step [390/600], Loss: 2.3056, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [3/10], Step [391/600], Loss: 2.3330, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [3/10], Step [392/600], Loss: 2.2870, batch time: 0.56, accuracy:  19.00%\n",
      "Epoch [3/10], Step [393/600], Loss: 2.3163, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [3/10], Step [394/600], Loss: 2.3282, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [3/10], Step [395/600], Loss: 2.3025, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [3/10], Step [396/600], Loss: 2.3002, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [3/10], Step [397/600], Loss: 2.3111, batch time: 0.61, accuracy:  9.00%\n",
      "Epoch [3/10], Step [398/600], Loss: 2.3090, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [3/10], Step [399/600], Loss: 2.3004, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [3/10], Step [400/600], Loss: 2.3113, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [3/10], Step [401/600], Loss: 2.3148, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [3/10], Step [402/600], Loss: 2.2931, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [3/10], Step [403/600], Loss: 2.3131, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [3/10], Step [404/600], Loss: 2.2913, batch time: 0.57, accuracy:  17.00%\n",
      "Epoch [3/10], Step [405/600], Loss: 2.2999, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [3/10], Step [406/600], Loss: 2.2969, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [3/10], Step [407/600], Loss: 2.3070, batch time: 0.56, accuracy:  20.00%\n",
      "Epoch [3/10], Step [408/600], Loss: 2.3177, batch time: 0.71, accuracy:  7.00%\n",
      "Epoch [3/10], Step [409/600], Loss: 2.3050, batch time: 0.67, accuracy:  14.00%\n",
      "Epoch [3/10], Step [410/600], Loss: 2.2900, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [3/10], Step [411/600], Loss: 2.2879, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [3/10], Step [412/600], Loss: 2.3213, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [3/10], Step [413/600], Loss: 2.3398, batch time: 0.70, accuracy:  10.00%\n",
      "Epoch [3/10], Step [414/600], Loss: 2.3223, batch time: 0.60, accuracy:  7.00%\n",
      "Epoch [3/10], Step [415/600], Loss: 2.3129, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [3/10], Step [416/600], Loss: 2.3012, batch time: 0.59, accuracy:  14.00%\n",
      "Epoch [3/10], Step [417/600], Loss: 2.3049, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [3/10], Step [418/600], Loss: 2.2993, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [3/10], Step [419/600], Loss: 2.3003, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [3/10], Step [420/600], Loss: 2.3127, batch time: 0.64, accuracy:  8.00%\n",
      "Epoch [3/10], Step [421/600], Loss: 2.3201, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [3/10], Step [422/600], Loss: 2.2788, batch time: 0.68, accuracy:  19.00%\n",
      "Epoch [3/10], Step [423/600], Loss: 2.3130, batch time: 0.67, accuracy:  14.00%\n",
      "Epoch [3/10], Step [424/600], Loss: 2.2869, batch time: 0.67, accuracy:  16.00%\n",
      "Epoch [3/10], Step [425/600], Loss: 2.2975, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [3/10], Step [426/600], Loss: 2.3138, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [3/10], Step [427/600], Loss: 2.3029, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [3/10], Step [428/600], Loss: 2.3178, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [3/10], Step [429/600], Loss: 2.3185, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [3/10], Step [430/600], Loss: 2.2854, batch time: 0.68, accuracy:  15.00%\n",
      "Epoch [3/10], Step [431/600], Loss: 2.3040, batch time: 0.69, accuracy:  6.00%\n",
      "Epoch [3/10], Step [432/600], Loss: 2.3122, batch time: 0.67, accuracy:  16.00%\n",
      "Epoch [3/10], Step [433/600], Loss: 2.3198, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [3/10], Step [434/600], Loss: 2.3174, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [3/10], Step [435/600], Loss: 2.3091, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [3/10], Step [436/600], Loss: 2.3040, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [3/10], Step [437/600], Loss: 2.3421, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [3/10], Step [438/600], Loss: 2.2959, batch time: 0.68, accuracy:  17.00%\n",
      "Epoch [3/10], Step [439/600], Loss: 2.3069, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [3/10], Step [440/600], Loss: 2.3037, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [3/10], Step [441/600], Loss: 2.2910, batch time: 0.68, accuracy:  16.00%\n",
      "Epoch [3/10], Step [442/600], Loss: 2.3138, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [3/10], Step [443/600], Loss: 2.3072, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [3/10], Step [444/600], Loss: 2.2939, batch time: 0.68, accuracy:  15.00%\n",
      "Epoch [3/10], Step [445/600], Loss: 2.3307, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [3/10], Step [446/600], Loss: 2.3040, batch time: 0.69, accuracy:  16.00%\n",
      "Epoch [3/10], Step [447/600], Loss: 2.3045, batch time: 0.70, accuracy:  12.00%\n",
      "Epoch [3/10], Step [448/600], Loss: 2.2946, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [3/10], Step [449/600], Loss: 2.3272, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [3/10], Step [450/600], Loss: 2.2911, batch time: 0.60, accuracy:  12.00%\n",
      "Epoch [3/10], Step [451/600], Loss: 2.3035, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [3/10], Step [452/600], Loss: 2.3007, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [3/10], Step [453/600], Loss: 2.3080, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [3/10], Step [454/600], Loss: 2.3115, batch time: 0.83, accuracy:  13.00%\n",
      "Epoch [3/10], Step [455/600], Loss: 2.2974, batch time: 0.67, accuracy:  14.00%\n",
      "Epoch [3/10], Step [456/600], Loss: 2.3148, batch time: 0.69, accuracy:  7.00%\n",
      "Epoch [3/10], Step [457/600], Loss: 2.3184, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [3/10], Step [458/600], Loss: 2.3102, batch time: 0.70, accuracy:  12.00%\n",
      "Epoch [3/10], Step [459/600], Loss: 2.3066, batch time: 0.64, accuracy:  10.00%\n",
      "Epoch [3/10], Step [460/600], Loss: 2.3053, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [3/10], Step [461/600], Loss: 2.3113, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [3/10], Step [462/600], Loss: 2.3079, batch time: 0.64, accuracy:  13.00%\n",
      "Epoch [3/10], Step [463/600], Loss: 2.3021, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [3/10], Step [464/600], Loss: 2.3207, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [3/10], Step [465/600], Loss: 2.2849, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [3/10], Step [466/600], Loss: 2.2970, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [3/10], Step [467/600], Loss: 2.2962, batch time: 0.59, accuracy:  8.00%\n",
      "Epoch [3/10], Step [468/600], Loss: 2.3003, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [3/10], Step [469/600], Loss: 2.3096, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [3/10], Step [470/600], Loss: 2.3213, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [3/10], Step [471/600], Loss: 2.3010, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [3/10], Step [472/600], Loss: 2.3221, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [3/10], Step [473/600], Loss: 2.3100, batch time: 0.70, accuracy:  11.00%\n",
      "Epoch [3/10], Step [474/600], Loss: 2.3025, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [3/10], Step [475/600], Loss: 2.3044, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [3/10], Step [476/600], Loss: 2.2961, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [3/10], Step [477/600], Loss: 2.2936, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [3/10], Step [478/600], Loss: 2.2993, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [3/10], Step [479/600], Loss: 2.3147, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [3/10], Step [480/600], Loss: 2.2915, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [3/10], Step [481/600], Loss: 2.3107, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [3/10], Step [482/600], Loss: 2.2947, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [3/10], Step [483/600], Loss: 2.3141, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [3/10], Step [484/600], Loss: 2.2981, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [3/10], Step [485/600], Loss: 2.3320, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [3/10], Step [486/600], Loss: 2.3026, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [3/10], Step [487/600], Loss: 2.2951, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [3/10], Step [488/600], Loss: 2.3111, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [3/10], Step [489/600], Loss: 2.3073, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [3/10], Step [490/600], Loss: 2.3439, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [3/10], Step [491/600], Loss: 2.3000, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [3/10], Step [492/600], Loss: 2.2985, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [3/10], Step [493/600], Loss: 2.3000, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [3/10], Step [494/600], Loss: 2.3197, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [3/10], Step [495/600], Loss: 2.3172, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [3/10], Step [496/600], Loss: 2.3118, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [3/10], Step [497/600], Loss: 2.2924, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [3/10], Step [498/600], Loss: 2.3122, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [3/10], Step [499/600], Loss: 2.2863, batch time: 0.57, accuracy:  17.00%\n",
      "Epoch [3/10], Step [500/600], Loss: 2.2959, batch time: 0.59, accuracy:  17.00%\n",
      "Epoch [3/10], Step [501/600], Loss: 2.3173, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [3/10], Step [502/600], Loss: 2.3041, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [3/10], Step [503/600], Loss: 2.3036, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [3/10], Step [504/600], Loss: 2.2997, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [3/10], Step [505/600], Loss: 2.2950, batch time: 0.56, accuracy:  15.00%\n",
      "Epoch [3/10], Step [506/600], Loss: 2.3006, batch time: 0.59, accuracy:  17.00%\n",
      "Epoch [3/10], Step [507/600], Loss: 2.2906, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [3/10], Step [508/600], Loss: 2.2927, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [3/10], Step [509/600], Loss: 2.3045, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [3/10], Step [510/600], Loss: 2.3058, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [3/10], Step [511/600], Loss: 2.2954, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [3/10], Step [512/600], Loss: 2.3056, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [3/10], Step [513/600], Loss: 2.3220, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [3/10], Step [514/600], Loss: 2.2963, batch time: 0.64, accuracy:  11.00%\n",
      "Epoch [3/10], Step [515/600], Loss: 2.3240, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [3/10], Step [516/600], Loss: 2.3117, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [3/10], Step [517/600], Loss: 2.3097, batch time: 0.63, accuracy:  6.00%\n",
      "Epoch [3/10], Step [518/600], Loss: 2.3068, batch time: 0.61, accuracy:  8.00%\n",
      "Epoch [3/10], Step [519/600], Loss: 2.3080, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [3/10], Step [520/600], Loss: 2.3058, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [3/10], Step [521/600], Loss: 2.2840, batch time: 0.57, accuracy:  17.00%\n",
      "Epoch [3/10], Step [522/600], Loss: 2.3151, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [3/10], Step [523/600], Loss: 2.3022, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [3/10], Step [524/600], Loss: 2.2938, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [3/10], Step [525/600], Loss: 2.3067, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [3/10], Step [526/600], Loss: 2.3011, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [3/10], Step [527/600], Loss: 2.3107, batch time: 0.65, accuracy:  9.00%\n",
      "Epoch [3/10], Step [528/600], Loss: 2.3221, batch time: 0.59, accuracy:  6.00%\n",
      "Epoch [3/10], Step [529/600], Loss: 2.2939, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [3/10], Step [530/600], Loss: 2.3186, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [3/10], Step [531/600], Loss: 2.3276, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [3/10], Step [532/600], Loss: 2.3125, batch time: 0.70, accuracy:  9.00%\n",
      "Epoch [3/10], Step [533/600], Loss: 2.3303, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [3/10], Step [534/600], Loss: 2.3059, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [3/10], Step [535/600], Loss: 2.3100, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [3/10], Step [536/600], Loss: 2.2950, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [3/10], Step [537/600], Loss: 2.3000, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [3/10], Step [538/600], Loss: 2.3041, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [3/10], Step [539/600], Loss: 2.3003, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [3/10], Step [540/600], Loss: 2.3201, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [3/10], Step [541/600], Loss: 2.2878, batch time: 0.58, accuracy:  15.00%\n",
      "Epoch [3/10], Step [542/600], Loss: 2.3087, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [3/10], Step [543/600], Loss: 2.3087, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [3/10], Step [544/600], Loss: 2.2928, batch time: 0.57, accuracy:  18.00%\n",
      "Epoch [3/10], Step [545/600], Loss: 2.3092, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [3/10], Step [546/600], Loss: 2.2995, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [3/10], Step [547/600], Loss: 2.2975, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [3/10], Step [548/600], Loss: 2.3068, batch time: 0.56, accuracy:  16.00%\n",
      "Epoch [3/10], Step [549/600], Loss: 2.3209, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [3/10], Step [550/600], Loss: 2.3135, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [3/10], Step [551/600], Loss: 2.3098, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [3/10], Step [552/600], Loss: 2.3038, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [3/10], Step [553/600], Loss: 2.3129, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [3/10], Step [554/600], Loss: 2.3026, batch time: 0.57, accuracy:  17.00%\n",
      "Epoch [3/10], Step [555/600], Loss: 2.3055, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [3/10], Step [556/600], Loss: 2.3070, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [3/10], Step [557/600], Loss: 2.2972, batch time: 0.58, accuracy:  19.00%\n",
      "Epoch [3/10], Step [558/600], Loss: 2.2870, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [3/10], Step [559/600], Loss: 2.2941, batch time: 0.57, accuracy:  17.00%\n",
      "Epoch [3/10], Step [560/600], Loss: 2.2989, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [3/10], Step [561/600], Loss: 2.3042, batch time: 0.55, accuracy:  10.00%\n",
      "Epoch [3/10], Step [562/600], Loss: 2.3005, batch time: 0.62, accuracy:  14.00%\n",
      "Epoch [3/10], Step [563/600], Loss: 2.3155, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [3/10], Step [564/600], Loss: 2.3128, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [3/10], Step [565/600], Loss: 2.3376, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [3/10], Step [566/600], Loss: 2.3012, batch time: 0.64, accuracy:  14.00%\n",
      "Epoch [3/10], Step [567/600], Loss: 2.3039, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [3/10], Step [568/600], Loss: 2.3221, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [3/10], Step [569/600], Loss: 2.2835, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [3/10], Step [570/600], Loss: 2.3037, batch time: 0.64, accuracy:  13.00%\n",
      "Epoch [3/10], Step [571/600], Loss: 2.3172, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [3/10], Step [572/600], Loss: 2.3028, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [3/10], Step [573/600], Loss: 2.3015, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [3/10], Step [574/600], Loss: 2.3156, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [3/10], Step [575/600], Loss: 2.3111, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [3/10], Step [576/600], Loss: 2.3176, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [3/10], Step [577/600], Loss: 2.3109, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [3/10], Step [578/600], Loss: 2.3059, batch time: 0.56, accuracy:  16.00%\n",
      "Epoch [3/10], Step [579/600], Loss: 2.2886, batch time: 0.58, accuracy:  17.00%\n",
      "Epoch [3/10], Step [580/600], Loss: 2.3067, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [3/10], Step [581/600], Loss: 2.3156, batch time: 0.57, accuracy:  5.00%\n",
      "Epoch [3/10], Step [582/600], Loss: 2.2984, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [3/10], Step [583/600], Loss: 2.3318, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [3/10], Step [584/600], Loss: 2.3203, batch time: 0.56, accuracy:  15.00%\n",
      "Epoch [3/10], Step [585/600], Loss: 2.2984, batch time: 0.60, accuracy:  14.00%\n",
      "Epoch [3/10], Step [586/600], Loss: 2.3039, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [3/10], Step [587/600], Loss: 2.2952, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [3/10], Step [588/600], Loss: 2.2904, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [3/10], Step [589/600], Loss: 2.2911, batch time: 0.57, accuracy:  18.00%\n",
      "Epoch [3/10], Step [590/600], Loss: 2.3086, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [3/10], Step [591/600], Loss: 2.3104, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [3/10], Step [592/600], Loss: 2.3106, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [3/10], Step [593/600], Loss: 2.3125, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [3/10], Step [594/600], Loss: 2.3002, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [3/10], Step [595/600], Loss: 2.2944, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [3/10], Step [596/600], Loss: 2.3053, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [3/10], Step [597/600], Loss: 2.2927, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [3/10], Step [598/600], Loss: 2.3062, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [3/10], Step [599/600], Loss: 2.3141, batch time: 0.59, accuracy:  6.00%\n",
      "Epoch [3/10], Step [600/600], Loss: 2.3006, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [4/10], Step [1/600], Loss: 2.2986, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [4/10], Step [2/600], Loss: 2.2955, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [4/10], Step [3/600], Loss: 2.3191, batch time: 0.71, accuracy:  10.00%\n",
      "Epoch [4/10], Step [4/600], Loss: 2.2843, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [4/10], Step [5/600], Loss: 2.2813, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [4/10], Step [6/600], Loss: 2.2933, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [4/10], Step [7/600], Loss: 2.2958, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [4/10], Step [8/600], Loss: 2.2978, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [4/10], Step [9/600], Loss: 2.3243, batch time: 0.65, accuracy:  9.00%\n",
      "Epoch [4/10], Step [10/600], Loss: 2.2979, batch time: 0.82, accuracy:  8.00%\n",
      "Epoch [4/10], Step [11/600], Loss: 2.2935, batch time: 0.68, accuracy:  15.00%\n",
      "Epoch [4/10], Step [12/600], Loss: 2.3036, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [4/10], Step [13/600], Loss: 2.2985, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [4/10], Step [14/600], Loss: 2.3008, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [4/10], Step [15/600], Loss: 2.3064, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [4/10], Step [16/600], Loss: 2.3005, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [4/10], Step [17/600], Loss: 2.3091, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [4/10], Step [18/600], Loss: 2.2883, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [4/10], Step [19/600], Loss: 2.2956, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [4/10], Step [20/600], Loss: 2.3113, batch time: 0.70, accuracy:  7.00%\n",
      "Epoch [4/10], Step [21/600], Loss: 2.3019, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [4/10], Step [22/600], Loss: 2.2996, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [4/10], Step [23/600], Loss: 2.2904, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [4/10], Step [24/600], Loss: 2.3083, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [4/10], Step [25/600], Loss: 2.3091, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [4/10], Step [26/600], Loss: 2.3052, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [4/10], Step [27/600], Loss: 2.3208, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [4/10], Step [28/600], Loss: 2.2976, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [4/10], Step [29/600], Loss: 2.3136, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [4/10], Step [30/600], Loss: 2.3117, batch time: 0.83, accuracy:  9.00%\n",
      "Epoch [4/10], Step [31/600], Loss: 2.2854, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [4/10], Step [32/600], Loss: 2.2746, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [4/10], Step [33/600], Loss: 2.2952, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [4/10], Step [34/600], Loss: 2.3033, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [4/10], Step [35/600], Loss: 2.2957, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [4/10], Step [36/600], Loss: 2.3138, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [4/10], Step [37/600], Loss: 2.3065, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [4/10], Step [38/600], Loss: 2.2929, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [4/10], Step [39/600], Loss: 2.3027, batch time: 0.57, accuracy:  4.00%\n",
      "Epoch [4/10], Step [40/600], Loss: 2.3217, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [4/10], Step [41/600], Loss: 2.3270, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [4/10], Step [42/600], Loss: 2.3023, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [4/10], Step [43/600], Loss: 2.3138, batch time: 0.57, accuracy:  4.00%\n",
      "Epoch [4/10], Step [44/600], Loss: 2.2893, batch time: 0.59, accuracy:  12.00%\n",
      "Epoch [4/10], Step [45/600], Loss: 2.3050, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [4/10], Step [46/600], Loss: 2.2900, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [4/10], Step [47/600], Loss: 2.2900, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [4/10], Step [48/600], Loss: 2.3005, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [4/10], Step [49/600], Loss: 2.3082, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [4/10], Step [50/600], Loss: 2.3066, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [4/10], Step [51/600], Loss: 2.2814, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [4/10], Step [52/600], Loss: 2.2819, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [4/10], Step [53/600], Loss: 2.2957, batch time: 0.60, accuracy:  8.00%\n",
      "Epoch [4/10], Step [54/600], Loss: 2.3280, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [4/10], Step [55/600], Loss: 2.2922, batch time: 0.59, accuracy:  15.00%\n",
      "Epoch [4/10], Step [56/600], Loss: 2.2916, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [4/10], Step [57/600], Loss: 2.2933, batch time: 0.55, accuracy:  6.00%\n",
      "Epoch [4/10], Step [58/600], Loss: 2.3047, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [4/10], Step [59/600], Loss: 2.2895, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [4/10], Step [60/600], Loss: 2.3106, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [4/10], Step [61/600], Loss: 2.3056, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [4/10], Step [62/600], Loss: 2.3088, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [4/10], Step [63/600], Loss: 2.3091, batch time: 0.55, accuracy:  4.00%\n",
      "Epoch [4/10], Step [64/600], Loss: 2.3064, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [4/10], Step [65/600], Loss: 2.2844, batch time: 0.56, accuracy:  7.00%\n",
      "Epoch [4/10], Step [66/600], Loss: 2.2848, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [4/10], Step [67/600], Loss: 2.3019, batch time: 0.56, accuracy:  5.00%\n",
      "Epoch [4/10], Step [68/600], Loss: 2.3179, batch time: 0.56, accuracy:  5.00%\n",
      "Epoch [4/10], Step [69/600], Loss: 2.2938, batch time: 0.58, accuracy:  16.00%\n",
      "Epoch [4/10], Step [70/600], Loss: 2.2980, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [4/10], Step [71/600], Loss: 2.3155, batch time: 0.57, accuracy:  5.00%\n",
      "Epoch [4/10], Step [72/600], Loss: 2.3034, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [4/10], Step [73/600], Loss: 2.3045, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [4/10], Step [74/600], Loss: 2.3166, batch time: 0.67, accuracy:  5.00%\n",
      "Epoch [4/10], Step [75/600], Loss: 2.3121, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [4/10], Step [76/600], Loss: 2.3103, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [4/10], Step [77/600], Loss: 2.3125, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [4/10], Step [78/600], Loss: 2.3398, batch time: 0.68, accuracy:  4.00%\n",
      "Epoch [4/10], Step [79/600], Loss: 2.2967, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [4/10], Step [80/600], Loss: 2.3148, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [4/10], Step [81/600], Loss: 2.3071, batch time: 0.58, accuracy:  16.00%\n",
      "Epoch [4/10], Step [82/600], Loss: 2.3017, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [4/10], Step [83/600], Loss: 2.3081, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [4/10], Step [84/600], Loss: 2.2867, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [4/10], Step [85/600], Loss: 2.3187, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [4/10], Step [86/600], Loss: 2.2923, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [4/10], Step [87/600], Loss: 2.2946, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [4/10], Step [88/600], Loss: 2.2827, batch time: 0.70, accuracy:  14.00%\n",
      "Epoch [4/10], Step [89/600], Loss: 2.3029, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [4/10], Step [90/600], Loss: 2.2987, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [4/10], Step [91/600], Loss: 2.3080, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [4/10], Step [92/600], Loss: 2.2980, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [4/10], Step [93/600], Loss: 2.3051, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [4/10], Step [94/600], Loss: 2.3080, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [4/10], Step [95/600], Loss: 2.3016, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [4/10], Step [96/600], Loss: 2.3043, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [4/10], Step [97/600], Loss: 2.2986, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [4/10], Step [98/600], Loss: 2.3145, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [4/10], Step [99/600], Loss: 2.2872, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [4/10], Step [100/600], Loss: 2.3175, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [4/10], Step [101/600], Loss: 2.2966, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [4/10], Step [102/600], Loss: 2.3159, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [4/10], Step [103/600], Loss: 2.3299, batch time: 0.66, accuracy:  7.00%\n",
      "Epoch [4/10], Step [104/600], Loss: 2.2969, batch time: 0.71, accuracy:  15.00%\n",
      "Epoch [4/10], Step [105/600], Loss: 2.2874, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [4/10], Step [106/600], Loss: 2.3056, batch time: 0.69, accuracy:  7.00%\n",
      "Epoch [4/10], Step [107/600], Loss: 2.3172, batch time: 0.70, accuracy:  4.00%\n",
      "Epoch [4/10], Step [108/600], Loss: 2.2835, batch time: 0.60, accuracy:  13.00%\n",
      "Epoch [4/10], Step [109/600], Loss: 2.2941, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [4/10], Step [110/600], Loss: 2.2995, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [4/10], Step [111/600], Loss: 2.2869, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [4/10], Step [112/600], Loss: 2.3070, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [4/10], Step [113/600], Loss: 2.3246, batch time: 0.64, accuracy:  6.00%\n",
      "Epoch [4/10], Step [114/600], Loss: 2.3208, batch time: 0.69, accuracy:  7.00%\n",
      "Epoch [4/10], Step [115/600], Loss: 2.3111, batch time: 0.71, accuracy:  10.00%\n",
      "Epoch [4/10], Step [116/600], Loss: 2.2961, batch time: 0.70, accuracy:  8.00%\n",
      "Epoch [4/10], Step [117/600], Loss: 2.3106, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [4/10], Step [118/600], Loss: 2.3023, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [4/10], Step [119/600], Loss: 2.2960, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [4/10], Step [120/600], Loss: 2.2973, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [4/10], Step [121/600], Loss: 2.2954, batch time: 0.65, accuracy:  6.00%\n",
      "Epoch [4/10], Step [122/600], Loss: 2.2983, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [4/10], Step [123/600], Loss: 2.3079, batch time: 0.63, accuracy:  12.00%\n",
      "Epoch [4/10], Step [124/600], Loss: 2.3040, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [4/10], Step [125/600], Loss: 2.2968, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [4/10], Step [126/600], Loss: 2.3232, batch time: 0.60, accuracy:  5.00%\n",
      "Epoch [4/10], Step [127/600], Loss: 2.3007, batch time: 0.64, accuracy:  10.00%\n",
      "Epoch [4/10], Step [128/600], Loss: 2.2996, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [4/10], Step [129/600], Loss: 2.3142, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [4/10], Step [130/600], Loss: 2.2848, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [4/10], Step [131/600], Loss: 2.3040, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [4/10], Step [132/600], Loss: 2.3113, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [4/10], Step [133/600], Loss: 2.2958, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [4/10], Step [134/600], Loss: 2.3086, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [4/10], Step [135/600], Loss: 2.3093, batch time: 0.56, accuracy:  15.00%\n",
      "Epoch [4/10], Step [136/600], Loss: 2.3025, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [4/10], Step [137/600], Loss: 2.3020, batch time: 0.61, accuracy:  11.00%\n",
      "Epoch [4/10], Step [138/600], Loss: 2.2990, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [4/10], Step [139/600], Loss: 2.2938, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [4/10], Step [140/600], Loss: 2.2957, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [4/10], Step [141/600], Loss: 2.3111, batch time: 0.56, accuracy:  5.00%\n",
      "Epoch [4/10], Step [142/600], Loss: 2.2868, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [4/10], Step [143/600], Loss: 2.3107, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [4/10], Step [144/600], Loss: 2.2998, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [4/10], Step [145/600], Loss: 2.2903, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [4/10], Step [146/600], Loss: 2.2991, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [4/10], Step [147/600], Loss: 2.2992, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [4/10], Step [148/600], Loss: 2.2810, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [4/10], Step [149/600], Loss: 2.2950, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [4/10], Step [150/600], Loss: 2.3319, batch time: 0.57, accuracy:  5.00%\n",
      "Epoch [4/10], Step [151/600], Loss: 2.3008, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [4/10], Step [152/600], Loss: 2.3118, batch time: 0.56, accuracy:  2.00%\n",
      "Epoch [4/10], Step [153/600], Loss: 2.2898, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [4/10], Step [154/600], Loss: 2.2975, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [4/10], Step [155/600], Loss: 2.3130, batch time: 0.56, accuracy:  5.00%\n",
      "Epoch [4/10], Step [156/600], Loss: 2.2975, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [4/10], Step [157/600], Loss: 2.3067, batch time: 0.66, accuracy:  11.00%\n",
      "Epoch [4/10], Step [158/600], Loss: 2.3043, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [4/10], Step [159/600], Loss: 2.2945, batch time: 0.67, accuracy:  14.00%\n",
      "Epoch [4/10], Step [160/600], Loss: 2.2982, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [4/10], Step [161/600], Loss: 2.3012, batch time: 0.69, accuracy:  6.00%\n",
      "Epoch [4/10], Step [162/600], Loss: 2.3063, batch time: 0.70, accuracy:  11.00%\n",
      "Epoch [4/10], Step [163/600], Loss: 2.2989, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [4/10], Step [164/600], Loss: 2.3035, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [4/10], Step [165/600], Loss: 2.3069, batch time: 0.60, accuracy:  9.00%\n",
      "Epoch [4/10], Step [166/600], Loss: 2.3102, batch time: 0.72, accuracy:  9.00%\n",
      "Epoch [4/10], Step [167/600], Loss: 2.2993, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [4/10], Step [168/600], Loss: 2.2732, batch time: 0.69, accuracy:  15.00%\n",
      "Epoch [4/10], Step [169/600], Loss: 2.3164, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [4/10], Step [170/600], Loss: 2.3026, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [4/10], Step [171/600], Loss: 2.2794, batch time: 0.68, accuracy:  15.00%\n",
      "Epoch [4/10], Step [172/600], Loss: 2.2991, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [4/10], Step [173/600], Loss: 2.2934, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [4/10], Step [174/600], Loss: 2.2658, batch time: 0.63, accuracy:  10.00%\n",
      "Epoch [4/10], Step [175/600], Loss: 2.3228, batch time: 0.57, accuracy:  5.00%\n",
      "Epoch [4/10], Step [176/600], Loss: 2.2976, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [4/10], Step [177/600], Loss: 2.3124, batch time: 0.59, accuracy:  9.00%\n",
      "Epoch [4/10], Step [178/600], Loss: 2.3271, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [4/10], Step [179/600], Loss: 2.3046, batch time: 0.61, accuracy:  7.00%\n",
      "Epoch [4/10], Step [180/600], Loss: 2.2985, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [4/10], Step [181/600], Loss: 2.2917, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [4/10], Step [182/600], Loss: 2.3126, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [4/10], Step [183/600], Loss: 2.2969, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [4/10], Step [184/600], Loss: 2.3139, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [4/10], Step [185/600], Loss: 2.2894, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [4/10], Step [186/600], Loss: 2.3048, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [4/10], Step [187/600], Loss: 2.3066, batch time: 0.59, accuracy:  7.00%\n",
      "Epoch [4/10], Step [188/600], Loss: 2.3236, batch time: 0.59, accuracy:  7.00%\n",
      "Epoch [4/10], Step [189/600], Loss: 2.2964, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [4/10], Step [190/600], Loss: 2.3120, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [4/10], Step [191/600], Loss: 2.3006, batch time: 0.62, accuracy:  12.00%\n",
      "Epoch [4/10], Step [192/600], Loss: 2.2880, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [4/10], Step [193/600], Loss: 2.3125, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [4/10], Step [194/600], Loss: 2.3039, batch time: 0.59, accuracy:  12.00%\n",
      "Epoch [4/10], Step [195/600], Loss: 2.3041, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [4/10], Step [196/600], Loss: 2.3031, batch time: 0.64, accuracy:  8.00%\n",
      "Epoch [4/10], Step [197/600], Loss: 2.3018, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [4/10], Step [198/600], Loss: 2.3060, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [4/10], Step [199/600], Loss: 2.3116, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [4/10], Step [200/600], Loss: 2.2769, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [4/10], Step [201/600], Loss: 2.3170, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [4/10], Step [202/600], Loss: 2.3127, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [4/10], Step [203/600], Loss: 2.3206, batch time: 0.60, accuracy:  6.00%\n",
      "Epoch [4/10], Step [204/600], Loss: 2.3078, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [4/10], Step [205/600], Loss: 2.3076, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [4/10], Step [206/600], Loss: 2.3008, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [4/10], Step [207/600], Loss: 2.2791, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [4/10], Step [208/600], Loss: 2.2880, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [4/10], Step [209/600], Loss: 2.2868, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [4/10], Step [210/600], Loss: 2.3014, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [4/10], Step [211/600], Loss: 2.3064, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [4/10], Step [212/600], Loss: 2.3076, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [4/10], Step [213/600], Loss: 2.2865, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [4/10], Step [214/600], Loss: 2.3063, batch time: 0.57, accuracy:  4.00%\n",
      "Epoch [4/10], Step [215/600], Loss: 2.3119, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [4/10], Step [216/600], Loss: 2.3040, batch time: 0.57, accuracy:  4.00%\n",
      "Epoch [4/10], Step [217/600], Loss: 2.2921, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [4/10], Step [218/600], Loss: 2.3067, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [4/10], Step [219/600], Loss: 2.3087, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [4/10], Step [220/600], Loss: 2.2866, batch time: 0.56, accuracy:  18.00%\n",
      "Epoch [4/10], Step [221/600], Loss: 2.2862, batch time: 0.56, accuracy:  16.00%\n",
      "Epoch [4/10], Step [222/600], Loss: 2.3087, batch time: 0.63, accuracy:  10.00%\n",
      "Epoch [4/10], Step [223/600], Loss: 2.3045, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [4/10], Step [224/600], Loss: 2.2835, batch time: 0.67, accuracy:  3.00%\n",
      "Epoch [4/10], Step [225/600], Loss: 2.2976, batch time: 0.67, accuracy:  15.00%\n",
      "Epoch [4/10], Step [226/600], Loss: 2.2959, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [4/10], Step [227/600], Loss: 2.3213, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [4/10], Step [228/600], Loss: 2.3209, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [4/10], Step [229/600], Loss: 2.2961, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [4/10], Step [230/600], Loss: 2.3057, batch time: 0.67, accuracy:  8.00%\n",
      "Epoch [4/10], Step [231/600], Loss: 2.3285, batch time: 0.67, accuracy:  6.00%\n",
      "Epoch [4/10], Step [232/600], Loss: 2.2778, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [4/10], Step [233/600], Loss: 2.3170, batch time: 0.67, accuracy:  4.00%\n",
      "Epoch [4/10], Step [234/600], Loss: 2.3067, batch time: 0.67, accuracy:  16.00%\n",
      "Epoch [4/10], Step [235/600], Loss: 2.2783, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [4/10], Step [236/600], Loss: 2.3000, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [4/10], Step [237/600], Loss: 2.3083, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [4/10], Step [238/600], Loss: 2.2813, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [4/10], Step [239/600], Loss: 2.3114, batch time: 0.67, accuracy:  8.00%\n",
      "Epoch [4/10], Step [240/600], Loss: 2.3101, batch time: 0.69, accuracy:  3.00%\n",
      "Epoch [4/10], Step [241/600], Loss: 2.3031, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [4/10], Step [242/600], Loss: 2.3053, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [4/10], Step [243/600], Loss: 2.3031, batch time: 0.64, accuracy:  9.00%\n",
      "Epoch [4/10], Step [244/600], Loss: 2.2854, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [4/10], Step [245/600], Loss: 2.3092, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [4/10], Step [246/600], Loss: 2.2916, batch time: 0.56, accuracy:  7.00%\n",
      "Epoch [4/10], Step [247/600], Loss: 2.3136, batch time: 0.56, accuracy:  6.00%\n",
      "Epoch [4/10], Step [248/600], Loss: 2.2973, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [4/10], Step [249/600], Loss: 2.3008, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [4/10], Step [250/600], Loss: 2.2958, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [4/10], Step [251/600], Loss: 2.2946, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [4/10], Step [252/600], Loss: 2.3145, batch time: 0.55, accuracy:  10.00%\n",
      "Epoch [4/10], Step [253/600], Loss: 2.3126, batch time: 0.56, accuracy:  7.00%\n",
      "Epoch [4/10], Step [254/600], Loss: 2.2903, batch time: 0.64, accuracy:  12.00%\n",
      "Epoch [4/10], Step [255/600], Loss: 2.3060, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [4/10], Step [256/600], Loss: 2.2885, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [4/10], Step [257/600], Loss: 2.3052, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [4/10], Step [258/600], Loss: 2.2690, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [4/10], Step [259/600], Loss: 2.2991, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [4/10], Step [260/600], Loss: 2.3003, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [4/10], Step [261/600], Loss: 2.2910, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [4/10], Step [262/600], Loss: 2.3115, batch time: 0.56, accuracy:  5.00%\n",
      "Epoch [4/10], Step [263/600], Loss: 2.2978, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [4/10], Step [264/600], Loss: 2.2746, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [4/10], Step [265/600], Loss: 2.3035, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [4/10], Step [266/600], Loss: 2.3164, batch time: 0.57, accuracy:  4.00%\n",
      "Epoch [4/10], Step [267/600], Loss: 2.3124, batch time: 0.56, accuracy:  7.00%\n",
      "Epoch [4/10], Step [268/600], Loss: 2.2980, batch time: 0.61, accuracy:  10.00%\n",
      "Epoch [4/10], Step [269/600], Loss: 2.3263, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [4/10], Step [270/600], Loss: 2.2969, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [4/10], Step [271/600], Loss: 2.3169, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [4/10], Step [272/600], Loss: 2.3029, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [4/10], Step [273/600], Loss: 2.2884, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [4/10], Step [274/600], Loss: 2.2989, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [4/10], Step [275/600], Loss: 2.3045, batch time: 0.61, accuracy:  11.00%\n",
      "Epoch [4/10], Step [276/600], Loss: 2.2984, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [4/10], Step [277/600], Loss: 2.3101, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [4/10], Step [278/600], Loss: 2.2833, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [4/10], Step [279/600], Loss: 2.2989, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [4/10], Step [280/600], Loss: 2.3026, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [4/10], Step [281/600], Loss: 2.2618, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [4/10], Step [282/600], Loss: 2.2997, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [4/10], Step [283/600], Loss: 2.3043, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [4/10], Step [284/600], Loss: 2.2868, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [4/10], Step [285/600], Loss: 2.2809, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [4/10], Step [286/600], Loss: 2.3116, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [4/10], Step [287/600], Loss: 2.3054, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [4/10], Step [288/600], Loss: 2.3049, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [4/10], Step [289/600], Loss: 2.3152, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [4/10], Step [290/600], Loss: 2.3072, batch time: 0.63, accuracy:  10.00%\n",
      "Epoch [4/10], Step [291/600], Loss: 2.3150, batch time: 0.57, accuracy:  4.00%\n",
      "Epoch [4/10], Step [292/600], Loss: 2.2841, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [4/10], Step [293/600], Loss: 2.3125, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [4/10], Step [294/600], Loss: 2.3262, batch time: 0.59, accuracy:  8.00%\n",
      "Epoch [4/10], Step [295/600], Loss: 2.2836, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [4/10], Step [296/600], Loss: 2.2915, batch time: 0.64, accuracy:  8.00%\n",
      "Epoch [4/10], Step [297/600], Loss: 2.2916, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [4/10], Step [298/600], Loss: 2.2922, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [4/10], Step [299/600], Loss: 2.2962, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [4/10], Step [300/600], Loss: 2.3051, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [4/10], Step [301/600], Loss: 2.3169, batch time: 0.67, accuracy:  5.00%\n",
      "Epoch [4/10], Step [302/600], Loss: 2.2866, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [4/10], Step [303/600], Loss: 2.3236, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [4/10], Step [304/600], Loss: 2.3024, batch time: 0.67, accuracy:  8.00%\n",
      "Epoch [4/10], Step [305/600], Loss: 2.3088, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [4/10], Step [306/600], Loss: 2.2795, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [4/10], Step [307/600], Loss: 2.3168, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [4/10], Step [308/600], Loss: 2.3094, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [4/10], Step [309/600], Loss: 2.3156, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [4/10], Step [310/600], Loss: 2.3021, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [4/10], Step [311/600], Loss: 2.2999, batch time: 0.67, accuracy:  6.00%\n",
      "Epoch [4/10], Step [312/600], Loss: 2.2968, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [4/10], Step [313/600], Loss: 2.3153, batch time: 0.67, accuracy:  6.00%\n",
      "Epoch [4/10], Step [314/600], Loss: 2.3057, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [4/10], Step [315/600], Loss: 2.3161, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [4/10], Step [316/600], Loss: 2.2880, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [4/10], Step [317/600], Loss: 2.3036, batch time: 0.67, accuracy:  3.00%\n",
      "Epoch [4/10], Step [318/600], Loss: 2.2963, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [4/10], Step [319/600], Loss: 2.2937, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [4/10], Step [320/600], Loss: 2.2974, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [4/10], Step [321/600], Loss: 2.3069, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [4/10], Step [322/600], Loss: 2.2886, batch time: 0.81, accuracy:  11.00%\n",
      "Epoch [4/10], Step [323/600], Loss: 2.2978, batch time: 0.67, accuracy:  4.00%\n",
      "Epoch [4/10], Step [324/600], Loss: 2.2792, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [4/10], Step [325/600], Loss: 2.2898, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [4/10], Step [326/600], Loss: 2.3029, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [4/10], Step [327/600], Loss: 2.2895, batch time: 0.67, accuracy:  14.00%\n",
      "Epoch [4/10], Step [328/600], Loss: 2.2854, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [4/10], Step [329/600], Loss: 2.3248, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [4/10], Step [330/600], Loss: 2.2716, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [4/10], Step [331/600], Loss: 2.3144, batch time: 0.67, accuracy:  6.00%\n",
      "Epoch [4/10], Step [332/600], Loss: 2.2955, batch time: 0.67, accuracy:  8.00%\n",
      "Epoch [4/10], Step [333/600], Loss: 2.3002, batch time: 0.64, accuracy:  9.00%\n",
      "Epoch [4/10], Step [334/600], Loss: 2.2990, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [4/10], Step [335/600], Loss: 2.2983, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [4/10], Step [336/600], Loss: 2.2921, batch time: 0.56, accuracy:  15.00%\n",
      "Epoch [4/10], Step [337/600], Loss: 2.2966, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [4/10], Step [338/600], Loss: 2.2989, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [4/10], Step [339/600], Loss: 2.3025, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [4/10], Step [340/600], Loss: 2.3100, batch time: 0.57, accuracy:  5.00%\n",
      "Epoch [4/10], Step [341/600], Loss: 2.2889, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [4/10], Step [342/600], Loss: 2.3041, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [4/10], Step [343/600], Loss: 2.3212, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [4/10], Step [344/600], Loss: 2.3018, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [4/10], Step [345/600], Loss: 2.2871, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [4/10], Step [346/600], Loss: 2.3114, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [4/10], Step [347/600], Loss: 2.3211, batch time: 0.61, accuracy:  9.00%\n",
      "Epoch [4/10], Step [348/600], Loss: 2.3205, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [4/10], Step [349/600], Loss: 2.2909, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [4/10], Step [350/600], Loss: 2.3096, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [4/10], Step [351/600], Loss: 2.2949, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [4/10], Step [352/600], Loss: 2.3033, batch time: 0.63, accuracy:  9.00%\n",
      "Epoch [4/10], Step [353/600], Loss: 2.2853, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [4/10], Step [354/600], Loss: 2.3088, batch time: 0.59, accuracy:  7.00%\n",
      "Epoch [4/10], Step [355/600], Loss: 2.3149, batch time: 0.57, accuracy:  5.00%\n",
      "Epoch [4/10], Step [356/600], Loss: 2.3154, batch time: 0.62, accuracy:  4.00%\n",
      "Epoch [4/10], Step [357/600], Loss: 2.2911, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [4/10], Step [358/600], Loss: 2.2960, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [4/10], Step [359/600], Loss: 2.2894, batch time: 0.63, accuracy:  17.00%\n",
      "Epoch [4/10], Step [360/600], Loss: 2.3085, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [4/10], Step [361/600], Loss: 2.3095, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [4/10], Step [362/600], Loss: 2.3056, batch time: 0.56, accuracy:  7.00%\n",
      "Epoch [4/10], Step [363/600], Loss: 2.3234, batch time: 0.56, accuracy:  4.00%\n",
      "Epoch [4/10], Step [364/600], Loss: 2.3063, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [4/10], Step [365/600], Loss: 2.3022, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [4/10], Step [366/600], Loss: 2.3146, batch time: 0.56, accuracy:  5.00%\n",
      "Epoch [4/10], Step [367/600], Loss: 2.2930, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [4/10], Step [368/600], Loss: 2.3067, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [4/10], Step [369/600], Loss: 2.3134, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [4/10], Step [370/600], Loss: 2.2977, batch time: 0.57, accuracy:  5.00%\n",
      "Epoch [4/10], Step [371/600], Loss: 2.3006, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [4/10], Step [372/600], Loss: 2.2911, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [4/10], Step [373/600], Loss: 2.2967, batch time: 0.55, accuracy:  16.00%\n",
      "Epoch [4/10], Step [374/600], Loss: 2.3063, batch time: 0.56, accuracy:  5.00%\n",
      "Epoch [4/10], Step [375/600], Loss: 2.3029, batch time: 0.56, accuracy:  15.00%\n",
      "Epoch [4/10], Step [376/600], Loss: 2.2923, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [4/10], Step [377/600], Loss: 2.2962, batch time: 0.56, accuracy:  6.00%\n",
      "Epoch [4/10], Step [378/600], Loss: 2.3135, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [4/10], Step [379/600], Loss: 2.2984, batch time: 0.68, accuracy:  5.00%\n",
      "Epoch [4/10], Step [380/600], Loss: 2.3021, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [4/10], Step [381/600], Loss: 2.2977, batch time: 0.68, accuracy:  5.00%\n",
      "Epoch [4/10], Step [382/600], Loss: 2.2746, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [4/10], Step [383/600], Loss: 2.3058, batch time: 0.67, accuracy:  5.00%\n",
      "Epoch [4/10], Step [384/600], Loss: 2.2965, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [4/10], Step [385/600], Loss: 2.3073, batch time: 0.66, accuracy:  9.00%\n",
      "Epoch [4/10], Step [386/600], Loss: 2.2698, batch time: 0.68, accuracy:  22.00%\n",
      "Epoch [4/10], Step [387/600], Loss: 2.2922, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [4/10], Step [388/600], Loss: 2.3019, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [4/10], Step [389/600], Loss: 2.3140, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [4/10], Step [390/600], Loss: 2.3065, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [4/10], Step [391/600], Loss: 2.2880, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [4/10], Step [392/600], Loss: 2.3011, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [4/10], Step [393/600], Loss: 2.2958, batch time: 0.67, accuracy:  14.00%\n",
      "Epoch [4/10], Step [394/600], Loss: 2.2902, batch time: 0.61, accuracy:  7.00%\n",
      "Epoch [4/10], Step [395/600], Loss: 2.3102, batch time: 0.58, accuracy:  5.00%\n",
      "Epoch [4/10], Step [396/600], Loss: 2.3097, batch time: 0.59, accuracy:  6.00%\n",
      "Epoch [4/10], Step [397/600], Loss: 2.3060, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [4/10], Step [398/600], Loss: 2.3140, batch time: 0.57, accuracy:  4.00%\n",
      "Epoch [4/10], Step [399/600], Loss: 2.2870, batch time: 0.64, accuracy:  13.00%\n",
      "Epoch [4/10], Step [400/600], Loss: 2.3098, batch time: 0.71, accuracy:  7.00%\n",
      "Epoch [4/10], Step [401/600], Loss: 2.2738, batch time: 0.64, accuracy:  15.00%\n",
      "Epoch [4/10], Step [402/600], Loss: 2.2960, batch time: 0.60, accuracy:  10.00%\n",
      "Epoch [4/10], Step [403/600], Loss: 2.2941, batch time: 0.58, accuracy:  17.00%\n",
      "Epoch [4/10], Step [404/600], Loss: 2.2955, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [4/10], Step [405/600], Loss: 2.2969, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [4/10], Step [406/600], Loss: 2.2951, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [4/10], Step [407/600], Loss: 2.2947, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [4/10], Step [408/600], Loss: 2.3179, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [4/10], Step [409/600], Loss: 2.3064, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [4/10], Step [410/600], Loss: 2.2897, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [4/10], Step [411/600], Loss: 2.2819, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [4/10], Step [412/600], Loss: 2.3111, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [4/10], Step [413/600], Loss: 2.2980, batch time: 0.60, accuracy:  8.00%\n",
      "Epoch [4/10], Step [414/600], Loss: 2.3060, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [4/10], Step [415/600], Loss: 2.3030, batch time: 0.66, accuracy:  8.00%\n",
      "Epoch [4/10], Step [416/600], Loss: 2.3111, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [4/10], Step [417/600], Loss: 2.3042, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [4/10], Step [418/600], Loss: 2.3023, batch time: 0.69, accuracy:  7.00%\n",
      "Epoch [4/10], Step [419/600], Loss: 2.2823, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [4/10], Step [420/600], Loss: 2.3052, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [4/10], Step [421/600], Loss: 2.3150, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [4/10], Step [422/600], Loss: 2.2901, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [4/10], Step [423/600], Loss: 2.3225, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [4/10], Step [424/600], Loss: 2.2982, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [4/10], Step [425/600], Loss: 2.3077, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [4/10], Step [426/600], Loss: 2.2905, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [4/10], Step [427/600], Loss: 2.3112, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [4/10], Step [428/600], Loss: 2.2900, batch time: 0.70, accuracy:  10.00%\n",
      "Epoch [4/10], Step [429/600], Loss: 2.3017, batch time: 0.69, accuracy:  6.00%\n",
      "Epoch [4/10], Step [430/600], Loss: 2.2891, batch time: 0.69, accuracy:  15.00%\n",
      "Epoch [4/10], Step [431/600], Loss: 2.2859, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [4/10], Step [432/600], Loss: 2.3068, batch time: 0.70, accuracy:  9.00%\n",
      "Epoch [4/10], Step [433/600], Loss: 2.2942, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [4/10], Step [434/600], Loss: 2.2918, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [4/10], Step [435/600], Loss: 2.3090, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [4/10], Step [436/600], Loss: 2.3033, batch time: 0.60, accuracy:  10.00%\n",
      "Epoch [4/10], Step [437/600], Loss: 2.2970, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [4/10], Step [438/600], Loss: 2.3019, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [4/10], Step [439/600], Loss: 2.3096, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [4/10], Step [440/600], Loss: 2.3138, batch time: 0.56, accuracy:  7.00%\n",
      "Epoch [4/10], Step [441/600], Loss: 2.2890, batch time: 0.56, accuracy:  17.00%\n",
      "Epoch [4/10], Step [442/600], Loss: 2.2824, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [4/10], Step [443/600], Loss: 2.2964, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [4/10], Step [444/600], Loss: 2.2909, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [4/10], Step [445/600], Loss: 2.3085, batch time: 0.58, accuracy:  5.00%\n",
      "Epoch [4/10], Step [446/600], Loss: 2.2991, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [4/10], Step [447/600], Loss: 2.2907, batch time: 0.67, accuracy:  15.00%\n",
      "Epoch [4/10], Step [448/600], Loss: 2.3077, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [4/10], Step [449/600], Loss: 2.3011, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [4/10], Step [450/600], Loss: 2.3127, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [4/10], Step [451/600], Loss: 2.3067, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [4/10], Step [452/600], Loss: 2.2992, batch time: 0.67, accuracy:  8.00%\n",
      "Epoch [4/10], Step [453/600], Loss: 2.3021, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [4/10], Step [454/600], Loss: 2.2956, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [4/10], Step [455/600], Loss: 2.2881, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [4/10], Step [456/600], Loss: 2.3033, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [4/10], Step [457/600], Loss: 2.3060, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [4/10], Step [458/600], Loss: 2.3173, batch time: 0.67, accuracy:  6.00%\n",
      "Epoch [4/10], Step [459/600], Loss: 2.3183, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [4/10], Step [460/600], Loss: 2.3039, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [4/10], Step [461/600], Loss: 2.3117, batch time: 0.67, accuracy:  8.00%\n",
      "Epoch [4/10], Step [462/600], Loss: 2.3058, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [4/10], Step [463/600], Loss: 2.3084, batch time: 0.67, accuracy:  8.00%\n",
      "Epoch [4/10], Step [464/600], Loss: 2.2974, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [4/10], Step [465/600], Loss: 2.3114, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [4/10], Step [466/600], Loss: 2.2933, batch time: 0.67, accuracy:  8.00%\n",
      "Epoch [4/10], Step [467/600], Loss: 2.3021, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [4/10], Step [468/600], Loss: 2.3096, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [4/10], Step [469/600], Loss: 2.2665, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [4/10], Step [470/600], Loss: 2.2858, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [4/10], Step [471/600], Loss: 2.3123, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [4/10], Step [472/600], Loss: 2.2904, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [4/10], Step [473/600], Loss: 2.2867, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [4/10], Step [474/600], Loss: 2.3013, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [4/10], Step [475/600], Loss: 2.3134, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [4/10], Step [476/600], Loss: 2.2961, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [4/10], Step [477/600], Loss: 2.3200, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [4/10], Step [478/600], Loss: 2.3136, batch time: 0.82, accuracy:  10.00%\n",
      "Epoch [4/10], Step [479/600], Loss: 2.3266, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [4/10], Step [480/600], Loss: 2.2943, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [4/10], Step [481/600], Loss: 2.3205, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [4/10], Step [482/600], Loss: 2.2915, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [4/10], Step [483/600], Loss: 2.3024, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [4/10], Step [484/600], Loss: 2.3107, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [4/10], Step [485/600], Loss: 2.3088, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [4/10], Step [486/600], Loss: 2.3125, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [4/10], Step [487/600], Loss: 2.2986, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [4/10], Step [488/600], Loss: 2.3093, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [4/10], Step [489/600], Loss: 2.3021, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [4/10], Step [490/600], Loss: 2.3072, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [4/10], Step [491/600], Loss: 2.3141, batch time: 0.69, accuracy:  6.00%\n",
      "Epoch [4/10], Step [492/600], Loss: 2.2857, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [4/10], Step [493/600], Loss: 2.3074, batch time: 0.67, accuracy:  8.00%\n",
      "Epoch [4/10], Step [494/600], Loss: 2.2984, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [4/10], Step [495/600], Loss: 2.2826, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [4/10], Step [496/600], Loss: 2.2887, batch time: 0.67, accuracy:  8.00%\n",
      "Epoch [4/10], Step [497/600], Loss: 2.3004, batch time: 0.65, accuracy:  9.00%\n",
      "Epoch [4/10], Step [498/600], Loss: 2.3104, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [4/10], Step [499/600], Loss: 2.3153, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [4/10], Step [500/600], Loss: 2.3020, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [4/10], Step [501/600], Loss: 2.2994, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [4/10], Step [502/600], Loss: 2.2960, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [4/10], Step [503/600], Loss: 2.2909, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [4/10], Step [504/600], Loss: 2.3020, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [4/10], Step [505/600], Loss: 2.3143, batch time: 0.58, accuracy:  4.00%\n",
      "Epoch [4/10], Step [506/600], Loss: 2.2991, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [4/10], Step [507/600], Loss: 2.2945, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [4/10], Step [508/600], Loss: 2.3238, batch time: 0.56, accuracy:  6.00%\n",
      "Epoch [4/10], Step [509/600], Loss: 2.2968, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [4/10], Step [510/600], Loss: 2.3015, batch time: 0.60, accuracy:  10.00%\n",
      "Epoch [4/10], Step [511/600], Loss: 2.2928, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [4/10], Step [512/600], Loss: 2.2903, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [4/10], Step [513/600], Loss: 2.3107, batch time: 0.56, accuracy:  6.00%\n",
      "Epoch [4/10], Step [514/600], Loss: 2.2935, batch time: 0.55, accuracy:  12.00%\n",
      "Epoch [4/10], Step [515/600], Loss: 2.3145, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [4/10], Step [516/600], Loss: 2.2914, batch time: 0.56, accuracy:  6.00%\n",
      "Epoch [4/10], Step [517/600], Loss: 2.2815, batch time: 0.62, accuracy:  15.00%\n",
      "Epoch [4/10], Step [518/600], Loss: 2.2990, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [4/10], Step [519/600], Loss: 2.3007, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [4/10], Step [520/600], Loss: 2.2983, batch time: 0.61, accuracy:  13.00%\n",
      "Epoch [4/10], Step [521/600], Loss: 2.2882, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [4/10], Step [522/600], Loss: 2.2924, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [4/10], Step [523/600], Loss: 2.2891, batch time: 0.58, accuracy:  3.00%\n",
      "Epoch [4/10], Step [524/600], Loss: 2.3098, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [4/10], Step [525/600], Loss: 2.3120, batch time: 0.56, accuracy:  15.00%\n",
      "Epoch [4/10], Step [526/600], Loss: 2.3003, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [4/10], Step [527/600], Loss: 2.2958, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [4/10], Step [528/600], Loss: 2.3095, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [4/10], Step [529/600], Loss: 2.3099, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [4/10], Step [530/600], Loss: 2.3030, batch time: 0.64, accuracy:  9.00%\n",
      "Epoch [4/10], Step [531/600], Loss: 2.2854, batch time: 0.66, accuracy:  11.00%\n",
      "Epoch [4/10], Step [532/600], Loss: 2.3079, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [4/10], Step [533/600], Loss: 2.2935, batch time: 0.56, accuracy:  15.00%\n",
      "Epoch [4/10], Step [534/600], Loss: 2.2964, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [4/10], Step [535/600], Loss: 2.3195, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [4/10], Step [536/600], Loss: 2.3033, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [4/10], Step [537/600], Loss: 2.2874, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [4/10], Step [538/600], Loss: 2.2972, batch time: 0.65, accuracy:  11.00%\n",
      "Epoch [4/10], Step [539/600], Loss: 2.2955, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [4/10], Step [540/600], Loss: 2.2857, batch time: 0.70, accuracy:  12.00%\n",
      "Epoch [4/10], Step [541/600], Loss: 2.2799, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [4/10], Step [542/600], Loss: 2.2903, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [4/10], Step [543/600], Loss: 2.3011, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [4/10], Step [544/600], Loss: 2.2826, batch time: 0.67, accuracy:  17.00%\n",
      "Epoch [4/10], Step [545/600], Loss: 2.3042, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [4/10], Step [546/600], Loss: 2.3005, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [4/10], Step [547/600], Loss: 2.2861, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [4/10], Step [548/600], Loss: 2.3052, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [4/10], Step [549/600], Loss: 2.2776, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [4/10], Step [550/600], Loss: 2.2960, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [4/10], Step [551/600], Loss: 2.2928, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [4/10], Step [552/600], Loss: 2.2895, batch time: 0.64, accuracy:  11.00%\n",
      "Epoch [4/10], Step [553/600], Loss: 2.3089, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [4/10], Step [554/600], Loss: 2.3045, batch time: 0.61, accuracy:  12.00%\n",
      "Epoch [4/10], Step [555/600], Loss: 2.3058, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [4/10], Step [556/600], Loss: 2.3276, batch time: 0.71, accuracy:  5.00%\n",
      "Epoch [4/10], Step [557/600], Loss: 2.3184, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [4/10], Step [558/600], Loss: 2.2948, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [4/10], Step [559/600], Loss: 2.3066, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [4/10], Step [560/600], Loss: 2.3143, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [4/10], Step [561/600], Loss: 2.3139, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [4/10], Step [562/600], Loss: 2.2980, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [4/10], Step [563/600], Loss: 2.3005, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [4/10], Step [564/600], Loss: 2.3051, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [4/10], Step [565/600], Loss: 2.3035, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [4/10], Step [566/600], Loss: 2.2993, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [4/10], Step [567/600], Loss: 2.2851, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [4/10], Step [568/600], Loss: 2.2874, batch time: 0.60, accuracy:  10.00%\n",
      "Epoch [4/10], Step [569/600], Loss: 2.3314, batch time: 0.61, accuracy:  6.00%\n",
      "Epoch [4/10], Step [570/600], Loss: 2.2969, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [4/10], Step [571/600], Loss: 2.3041, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [4/10], Step [572/600], Loss: 2.3020, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [4/10], Step [573/600], Loss: 2.2942, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [4/10], Step [574/600], Loss: 2.3124, batch time: 0.59, accuracy:  7.00%\n",
      "Epoch [4/10], Step [575/600], Loss: 2.3086, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [4/10], Step [576/600], Loss: 2.2979, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [4/10], Step [577/600], Loss: 2.2852, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [4/10], Step [578/600], Loss: 2.2829, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [4/10], Step [579/600], Loss: 2.3006, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [4/10], Step [580/600], Loss: 2.3038, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [4/10], Step [581/600], Loss: 2.3200, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [4/10], Step [582/600], Loss: 2.2953, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [4/10], Step [583/600], Loss: 2.3204, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [4/10], Step [584/600], Loss: 2.2964, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [4/10], Step [585/600], Loss: 2.2934, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [4/10], Step [586/600], Loss: 2.3028, batch time: 0.60, accuracy:  15.00%\n",
      "Epoch [4/10], Step [587/600], Loss: 2.2883, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [4/10], Step [588/600], Loss: 2.3052, batch time: 0.60, accuracy:  9.00%\n",
      "Epoch [4/10], Step [589/600], Loss: 2.3100, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [4/10], Step [590/600], Loss: 2.3028, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [4/10], Step [591/600], Loss: 2.3007, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [4/10], Step [592/600], Loss: 2.2959, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [4/10], Step [593/600], Loss: 2.3049, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [4/10], Step [594/600], Loss: 2.3068, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [4/10], Step [595/600], Loss: 2.2944, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [4/10], Step [596/600], Loss: 2.2959, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [4/10], Step [597/600], Loss: 2.3012, batch time: 0.62, accuracy:  12.00%\n",
      "Epoch [4/10], Step [598/600], Loss: 2.2970, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [4/10], Step [599/600], Loss: 2.3034, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [4/10], Step [600/600], Loss: 2.3027, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [5/10], Step [1/600], Loss: 2.3019, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [5/10], Step [2/600], Loss: 2.3093, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [5/10], Step [3/600], Loss: 2.2811, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [5/10], Step [4/600], Loss: 2.2946, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [5/10], Step [5/600], Loss: 2.3056, batch time: 0.56, accuracy:  4.00%\n",
      "Epoch [5/10], Step [6/600], Loss: 2.3069, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [5/10], Step [7/600], Loss: 2.2982, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [5/10], Step [8/600], Loss: 2.3014, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [5/10], Step [9/600], Loss: 2.3052, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [5/10], Step [10/600], Loss: 2.2884, batch time: 0.70, accuracy:  11.00%\n",
      "Epoch [5/10], Step [11/600], Loss: 2.3005, batch time: 0.60, accuracy:  9.00%\n",
      "Epoch [5/10], Step [12/600], Loss: 2.2999, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [5/10], Step [13/600], Loss: 2.3096, batch time: 0.56, accuracy:  4.00%\n",
      "Epoch [5/10], Step [14/600], Loss: 2.3042, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [5/10], Step [15/600], Loss: 2.2914, batch time: 0.58, accuracy:  15.00%\n",
      "Epoch [5/10], Step [16/600], Loss: 2.3076, batch time: 0.60, accuracy:  7.00%\n",
      "Epoch [5/10], Step [17/600], Loss: 2.2945, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [5/10], Step [18/600], Loss: 2.3124, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [5/10], Step [19/600], Loss: 2.2906, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [5/10], Step [20/600], Loss: 2.3198, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [5/10], Step [21/600], Loss: 2.2907, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [5/10], Step [22/600], Loss: 2.2853, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [5/10], Step [23/600], Loss: 2.2824, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [5/10], Step [24/600], Loss: 2.2967, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [5/10], Step [25/600], Loss: 2.2901, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [5/10], Step [26/600], Loss: 2.3033, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [5/10], Step [27/600], Loss: 2.3092, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [5/10], Step [28/600], Loss: 2.2904, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [5/10], Step [29/600], Loss: 2.2964, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [30/600], Loss: 2.3036, batch time: 0.56, accuracy:  7.00%\n",
      "Epoch [5/10], Step [31/600], Loss: 2.2897, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [5/10], Step [32/600], Loss: 2.2928, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [5/10], Step [33/600], Loss: 2.2876, batch time: 0.68, accuracy:  15.00%\n",
      "Epoch [5/10], Step [34/600], Loss: 2.3293, batch time: 0.80, accuracy:  3.00%\n",
      "Epoch [5/10], Step [35/600], Loss: 2.3078, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [5/10], Step [36/600], Loss: 2.3008, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [5/10], Step [37/600], Loss: 2.2809, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [5/10], Step [38/600], Loss: 2.2939, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [5/10], Step [39/600], Loss: 2.2902, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [5/10], Step [40/600], Loss: 2.2891, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [5/10], Step [41/600], Loss: 2.2773, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [5/10], Step [42/600], Loss: 2.2944, batch time: 0.56, accuracy:  6.00%\n",
      "Epoch [5/10], Step [43/600], Loss: 2.3021, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [5/10], Step [44/600], Loss: 2.2930, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [5/10], Step [45/600], Loss: 2.2980, batch time: 0.56, accuracy:  6.00%\n",
      "Epoch [5/10], Step [46/600], Loss: 2.2859, batch time: 0.61, accuracy:  11.00%\n",
      "Epoch [5/10], Step [47/600], Loss: 2.3134, batch time: 0.59, accuracy:  7.00%\n",
      "Epoch [5/10], Step [48/600], Loss: 2.3162, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [5/10], Step [49/600], Loss: 2.2776, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [5/10], Step [50/600], Loss: 2.3062, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [5/10], Step [51/600], Loss: 2.3074, batch time: 0.66, accuracy:  9.00%\n",
      "Epoch [5/10], Step [52/600], Loss: 2.2939, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [5/10], Step [53/600], Loss: 2.2835, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [5/10], Step [54/600], Loss: 2.2959, batch time: 0.70, accuracy:  9.00%\n",
      "Epoch [5/10], Step [55/600], Loss: 2.3119, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [5/10], Step [56/600], Loss: 2.2960, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [5/10], Step [57/600], Loss: 2.3172, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [5/10], Step [58/600], Loss: 2.3013, batch time: 0.59, accuracy:  8.00%\n",
      "Epoch [5/10], Step [59/600], Loss: 2.3094, batch time: 0.61, accuracy:  8.00%\n",
      "Epoch [5/10], Step [60/600], Loss: 2.3057, batch time: 0.65, accuracy:  10.00%\n",
      "Epoch [5/10], Step [61/600], Loss: 2.2789, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [5/10], Step [62/600], Loss: 2.2893, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [5/10], Step [63/600], Loss: 2.3084, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [5/10], Step [64/600], Loss: 2.3150, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [5/10], Step [65/600], Loss: 2.2897, batch time: 0.58, accuracy:  15.00%\n",
      "Epoch [5/10], Step [66/600], Loss: 2.3113, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [5/10], Step [67/600], Loss: 2.2988, batch time: 0.59, accuracy:  12.00%\n",
      "Epoch [5/10], Step [68/600], Loss: 2.3177, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [5/10], Step [69/600], Loss: 2.2933, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [70/600], Loss: 2.2786, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [5/10], Step [71/600], Loss: 2.3194, batch time: 0.60, accuracy:  8.00%\n",
      "Epoch [5/10], Step [72/600], Loss: 2.2920, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [5/10], Step [73/600], Loss: 2.2976, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [5/10], Step [74/600], Loss: 2.2937, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [5/10], Step [75/600], Loss: 2.3180, batch time: 0.68, accuracy:  5.00%\n",
      "Epoch [5/10], Step [76/600], Loss: 2.3141, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [5/10], Step [77/600], Loss: 2.3111, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [5/10], Step [78/600], Loss: 2.3235, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [5/10], Step [79/600], Loss: 2.2839, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [5/10], Step [80/600], Loss: 2.2859, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [5/10], Step [81/600], Loss: 2.3155, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [5/10], Step [82/600], Loss: 2.2906, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [5/10], Step [83/600], Loss: 2.2819, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [5/10], Step [84/600], Loss: 2.3072, batch time: 0.56, accuracy:  7.00%\n",
      "Epoch [5/10], Step [85/600], Loss: 2.3091, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [5/10], Step [86/600], Loss: 2.2932, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [5/10], Step [87/600], Loss: 2.2896, batch time: 0.61, accuracy:  8.00%\n",
      "Epoch [5/10], Step [88/600], Loss: 2.2892, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [5/10], Step [89/600], Loss: 2.2871, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [5/10], Step [90/600], Loss: 2.2985, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [5/10], Step [91/600], Loss: 2.2919, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [5/10], Step [92/600], Loss: 2.3016, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [93/600], Loss: 2.3194, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [5/10], Step [94/600], Loss: 2.2904, batch time: 0.66, accuracy:  14.00%\n",
      "Epoch [5/10], Step [95/600], Loss: 2.3089, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [96/600], Loss: 2.3019, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [97/600], Loss: 2.3070, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [5/10], Step [98/600], Loss: 2.2989, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [5/10], Step [99/600], Loss: 2.3158, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [5/10], Step [100/600], Loss: 2.3110, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [5/10], Step [101/600], Loss: 2.2964, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [5/10], Step [102/600], Loss: 2.2936, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [5/10], Step [103/600], Loss: 2.3035, batch time: 0.56, accuracy:  7.00%\n",
      "Epoch [5/10], Step [104/600], Loss: 2.2982, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [5/10], Step [105/600], Loss: 2.2999, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [5/10], Step [106/600], Loss: 2.3014, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [5/10], Step [107/600], Loss: 2.2975, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [5/10], Step [108/600], Loss: 2.3035, batch time: 0.60, accuracy:  6.00%\n",
      "Epoch [5/10], Step [109/600], Loss: 2.3007, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [5/10], Step [110/600], Loss: 2.3035, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [111/600], Loss: 2.2963, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [5/10], Step [112/600], Loss: 2.2844, batch time: 0.72, accuracy:  14.00%\n",
      "Epoch [5/10], Step [113/600], Loss: 2.2986, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [5/10], Step [114/600], Loss: 2.3082, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [5/10], Step [115/600], Loss: 2.2971, batch time: 0.56, accuracy:  7.00%\n",
      "Epoch [5/10], Step [116/600], Loss: 2.2979, batch time: 0.57, accuracy:  5.00%\n",
      "Epoch [5/10], Step [117/600], Loss: 2.3084, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [5/10], Step [118/600], Loss: 2.2991, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [119/600], Loss: 2.2973, batch time: 0.63, accuracy:  5.00%\n",
      "Epoch [5/10], Step [120/600], Loss: 2.2931, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [5/10], Step [121/600], Loss: 2.2810, batch time: 0.56, accuracy:  15.00%\n",
      "Epoch [5/10], Step [122/600], Loss: 2.2930, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [123/600], Loss: 2.3116, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [5/10], Step [124/600], Loss: 2.3001, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [5/10], Step [125/600], Loss: 2.3138, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [5/10], Step [126/600], Loss: 2.2982, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [5/10], Step [127/600], Loss: 2.3081, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [5/10], Step [128/600], Loss: 2.3096, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [5/10], Step [129/600], Loss: 2.3151, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [5/10], Step [130/600], Loss: 2.3048, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [5/10], Step [131/600], Loss: 2.2962, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [5/10], Step [132/600], Loss: 2.2840, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [5/10], Step [133/600], Loss: 2.3057, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [134/600], Loss: 2.2899, batch time: 0.66, accuracy:  7.00%\n",
      "Epoch [5/10], Step [135/600], Loss: 2.2871, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [5/10], Step [136/600], Loss: 2.2946, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [137/600], Loss: 2.2749, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [5/10], Step [138/600], Loss: 2.2880, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [5/10], Step [139/600], Loss: 2.3103, batch time: 0.67, accuracy:  8.00%\n",
      "Epoch [5/10], Step [140/600], Loss: 2.2876, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [5/10], Step [141/600], Loss: 2.3079, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [5/10], Step [142/600], Loss: 2.3181, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [5/10], Step [143/600], Loss: 2.3113, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [5/10], Step [144/600], Loss: 2.2941, batch time: 0.60, accuracy:  8.00%\n",
      "Epoch [5/10], Step [145/600], Loss: 2.3023, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [146/600], Loss: 2.2912, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [5/10], Step [147/600], Loss: 2.3054, batch time: 0.56, accuracy:  5.00%\n",
      "Epoch [5/10], Step [148/600], Loss: 2.3044, batch time: 0.64, accuracy:  12.00%\n",
      "Epoch [5/10], Step [149/600], Loss: 2.2827, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [5/10], Step [150/600], Loss: 2.2941, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [5/10], Step [151/600], Loss: 2.2861, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [5/10], Step [152/600], Loss: 2.3052, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [5/10], Step [153/600], Loss: 2.3056, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [5/10], Step [154/600], Loss: 2.3157, batch time: 0.67, accuracy:  2.00%\n",
      "Epoch [5/10], Step [155/600], Loss: 2.2940, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [5/10], Step [156/600], Loss: 2.3057, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [5/10], Step [157/600], Loss: 2.2897, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [5/10], Step [158/600], Loss: 2.3160, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [5/10], Step [159/600], Loss: 2.3020, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [5/10], Step [160/600], Loss: 2.3021, batch time: 0.67, accuracy:  15.00%\n",
      "Epoch [5/10], Step [161/600], Loss: 2.2981, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [5/10], Step [162/600], Loss: 2.2915, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [5/10], Step [163/600], Loss: 2.3003, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [5/10], Step [164/600], Loss: 2.3053, batch time: 0.67, accuracy:  8.00%\n",
      "Epoch [5/10], Step [165/600], Loss: 2.3174, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [5/10], Step [166/600], Loss: 2.2982, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [5/10], Step [167/600], Loss: 2.2866, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [5/10], Step [168/600], Loss: 2.2932, batch time: 0.67, accuracy:  8.00%\n",
      "Epoch [5/10], Step [169/600], Loss: 2.3041, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [5/10], Step [170/600], Loss: 2.2795, batch time: 0.68, accuracy:  17.00%\n",
      "Epoch [5/10], Step [171/600], Loss: 2.2902, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [5/10], Step [172/600], Loss: 2.2943, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [5/10], Step [173/600], Loss: 2.2800, batch time: 0.66, accuracy:  12.00%\n",
      "Epoch [5/10], Step [174/600], Loss: 2.3036, batch time: 0.61, accuracy:  11.00%\n",
      "Epoch [5/10], Step [175/600], Loss: 2.2923, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [5/10], Step [176/600], Loss: 2.3011, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [5/10], Step [177/600], Loss: 2.2958, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [5/10], Step [178/600], Loss: 2.2938, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [5/10], Step [179/600], Loss: 2.3003, batch time: 0.67, accuracy:  14.00%\n",
      "Epoch [5/10], Step [180/600], Loss: 2.2993, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [5/10], Step [181/600], Loss: 2.2881, batch time: 0.66, accuracy:  9.00%\n",
      "Epoch [5/10], Step [182/600], Loss: 2.3106, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [5/10], Step [183/600], Loss: 2.2875, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [5/10], Step [184/600], Loss: 2.2950, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [5/10], Step [185/600], Loss: 2.3050, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [5/10], Step [186/600], Loss: 2.3122, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [5/10], Step [187/600], Loss: 2.3095, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [5/10], Step [188/600], Loss: 2.2988, batch time: 0.66, accuracy:  10.00%\n",
      "Epoch [5/10], Step [189/600], Loss: 2.3082, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [5/10], Step [190/600], Loss: 2.2893, batch time: 0.80, accuracy:  12.00%\n",
      "Epoch [5/10], Step [191/600], Loss: 2.2784, batch time: 0.67, accuracy:  19.00%\n",
      "Epoch [5/10], Step [192/600], Loss: 2.2849, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [5/10], Step [193/600], Loss: 2.2950, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [5/10], Step [194/600], Loss: 2.2811, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [5/10], Step [195/600], Loss: 2.2970, batch time: 0.66, accuracy:  8.00%\n",
      "Epoch [5/10], Step [196/600], Loss: 2.3083, batch time: 0.66, accuracy:  9.00%\n",
      "Epoch [5/10], Step [197/600], Loss: 2.2993, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [5/10], Step [198/600], Loss: 2.3147, batch time: 0.56, accuracy:  5.00%\n",
      "Epoch [5/10], Step [199/600], Loss: 2.3021, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [5/10], Step [200/600], Loss: 2.2896, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [5/10], Step [201/600], Loss: 2.2920, batch time: 0.55, accuracy:  7.00%\n",
      "Epoch [5/10], Step [202/600], Loss: 2.2987, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [5/10], Step [203/600], Loss: 2.3023, batch time: 0.57, accuracy:  4.00%\n",
      "Epoch [5/10], Step [204/600], Loss: 2.3069, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [5/10], Step [205/600], Loss: 2.3081, batch time: 0.66, accuracy:  3.00%\n",
      "Epoch [5/10], Step [206/600], Loss: 2.2872, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [5/10], Step [207/600], Loss: 2.3057, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [5/10], Step [208/600], Loss: 2.3140, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [5/10], Step [209/600], Loss: 2.3164, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [5/10], Step [210/600], Loss: 2.3004, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [5/10], Step [211/600], Loss: 2.2950, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [5/10], Step [212/600], Loss: 2.2909, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [5/10], Step [213/600], Loss: 2.3177, batch time: 0.70, accuracy:  5.00%\n",
      "Epoch [5/10], Step [214/600], Loss: 2.2992, batch time: 0.70, accuracy:  15.00%\n",
      "Epoch [5/10], Step [215/600], Loss: 2.2850, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [5/10], Step [216/600], Loss: 2.2984, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [5/10], Step [217/600], Loss: 2.2879, batch time: 0.69, accuracy:  7.00%\n",
      "Epoch [5/10], Step [218/600], Loss: 2.2971, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [5/10], Step [219/600], Loss: 2.2935, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [5/10], Step [220/600], Loss: 2.2780, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [5/10], Step [221/600], Loss: 2.2896, batch time: 0.70, accuracy:  13.00%\n",
      "Epoch [5/10], Step [222/600], Loss: 2.2926, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [5/10], Step [223/600], Loss: 2.3016, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [5/10], Step [224/600], Loss: 2.2997, batch time: 0.60, accuracy:  12.00%\n",
      "Epoch [5/10], Step [225/600], Loss: 2.2775, batch time: 0.59, accuracy:  20.00%\n",
      "Epoch [5/10], Step [226/600], Loss: 2.3057, batch time: 0.58, accuracy:  5.00%\n",
      "Epoch [5/10], Step [227/600], Loss: 2.2966, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [5/10], Step [228/600], Loss: 2.3138, batch time: 0.58, accuracy:  5.00%\n",
      "Epoch [5/10], Step [229/600], Loss: 2.2956, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [5/10], Step [230/600], Loss: 2.3071, batch time: 0.59, accuracy:  7.00%\n",
      "Epoch [5/10], Step [231/600], Loss: 2.3090, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [5/10], Step [232/600], Loss: 2.3183, batch time: 0.57, accuracy:  5.00%\n",
      "Epoch [5/10], Step [233/600], Loss: 2.3017, batch time: 0.57, accuracy:  4.00%\n",
      "Epoch [5/10], Step [234/600], Loss: 2.2748, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [5/10], Step [235/600], Loss: 2.3025, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [5/10], Step [236/600], Loss: 2.2992, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [5/10], Step [237/600], Loss: 2.2868, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [5/10], Step [238/600], Loss: 2.2769, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [5/10], Step [239/600], Loss: 2.2854, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [5/10], Step [240/600], Loss: 2.3026, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [5/10], Step [241/600], Loss: 2.2927, batch time: 0.60, accuracy:  10.00%\n",
      "Epoch [5/10], Step [242/600], Loss: 2.2849, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [5/10], Step [243/600], Loss: 2.2918, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [5/10], Step [244/600], Loss: 2.2978, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [5/10], Step [245/600], Loss: 2.2890, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [5/10], Step [246/600], Loss: 2.2930, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [5/10], Step [247/600], Loss: 2.3169, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [5/10], Step [248/600], Loss: 2.2928, batch time: 0.58, accuracy:  4.00%\n",
      "Epoch [5/10], Step [249/600], Loss: 2.3056, batch time: 0.60, accuracy:  4.00%\n",
      "Epoch [5/10], Step [250/600], Loss: 2.2939, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [251/600], Loss: 2.3029, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [5/10], Step [252/600], Loss: 2.3080, batch time: 0.63, accuracy:  6.00%\n",
      "Epoch [5/10], Step [253/600], Loss: 2.3009, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [5/10], Step [254/600], Loss: 2.2965, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [5/10], Step [255/600], Loss: 2.2903, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [5/10], Step [256/600], Loss: 2.3095, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [5/10], Step [257/600], Loss: 2.2875, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [5/10], Step [258/600], Loss: 2.3237, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [5/10], Step [259/600], Loss: 2.2997, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [5/10], Step [260/600], Loss: 2.2830, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [5/10], Step [261/600], Loss: 2.3107, batch time: 0.57, accuracy:  5.00%\n",
      "Epoch [5/10], Step [262/600], Loss: 2.2967, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [5/10], Step [263/600], Loss: 2.2926, batch time: 0.61, accuracy:  12.00%\n",
      "Epoch [5/10], Step [264/600], Loss: 2.3152, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [5/10], Step [265/600], Loss: 2.3053, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [266/600], Loss: 2.3153, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [5/10], Step [267/600], Loss: 2.3107, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [5/10], Step [268/600], Loss: 2.2845, batch time: 0.69, accuracy:  16.00%\n",
      "Epoch [5/10], Step [269/600], Loss: 2.2884, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [5/10], Step [270/600], Loss: 2.2997, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [271/600], Loss: 2.2940, batch time: 0.59, accuracy:  8.00%\n",
      "Epoch [5/10], Step [272/600], Loss: 2.2779, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [5/10], Step [273/600], Loss: 2.2979, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [5/10], Step [274/600], Loss: 2.3089, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [5/10], Step [275/600], Loss: 2.3027, batch time: 0.67, accuracy:  6.00%\n",
      "Epoch [5/10], Step [276/600], Loss: 2.3147, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [5/10], Step [277/600], Loss: 2.3006, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [5/10], Step [278/600], Loss: 2.3013, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [5/10], Step [279/600], Loss: 2.2886, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [5/10], Step [280/600], Loss: 2.2889, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [5/10], Step [281/600], Loss: 2.3139, batch time: 0.64, accuracy:  9.00%\n",
      "Epoch [5/10], Step [282/600], Loss: 2.2879, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [5/10], Step [283/600], Loss: 2.2939, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [5/10], Step [284/600], Loss: 2.3181, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [5/10], Step [285/600], Loss: 2.3029, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [5/10], Step [286/600], Loss: 2.2793, batch time: 0.57, accuracy:  17.00%\n",
      "Epoch [5/10], Step [287/600], Loss: 2.2937, batch time: 0.59, accuracy:  5.00%\n",
      "Epoch [5/10], Step [288/600], Loss: 2.2852, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [289/600], Loss: 2.3253, batch time: 0.57, accuracy:  4.00%\n",
      "Epoch [5/10], Step [290/600], Loss: 2.2968, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [5/10], Step [291/600], Loss: 2.2839, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [5/10], Step [292/600], Loss: 2.2753, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [5/10], Step [293/600], Loss: 2.3040, batch time: 0.64, accuracy:  12.00%\n",
      "Epoch [5/10], Step [294/600], Loss: 2.3113, batch time: 0.63, accuracy:  6.00%\n",
      "Epoch [5/10], Step [295/600], Loss: 2.3072, batch time: 0.56, accuracy:  5.00%\n",
      "Epoch [5/10], Step [296/600], Loss: 2.2876, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [5/10], Step [297/600], Loss: 2.2955, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [5/10], Step [298/600], Loss: 2.2994, batch time: 0.58, accuracy:  15.00%\n",
      "Epoch [5/10], Step [299/600], Loss: 2.3036, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [5/10], Step [300/600], Loss: 2.2803, batch time: 0.57, accuracy:  19.00%\n",
      "Epoch [5/10], Step [301/600], Loss: 2.2881, batch time: 0.56, accuracy:  15.00%\n",
      "Epoch [5/10], Step [302/600], Loss: 2.2992, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [5/10], Step [303/600], Loss: 2.2826, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [5/10], Step [304/600], Loss: 2.2932, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [5/10], Step [305/600], Loss: 2.3004, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [5/10], Step [306/600], Loss: 2.3034, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [5/10], Step [307/600], Loss: 2.2733, batch time: 0.59, accuracy:  14.00%\n",
      "Epoch [5/10], Step [308/600], Loss: 2.3074, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [5/10], Step [309/600], Loss: 2.2864, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [5/10], Step [310/600], Loss: 2.2964, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [5/10], Step [311/600], Loss: 2.2839, batch time: 0.62, accuracy:  12.00%\n",
      "Epoch [5/10], Step [312/600], Loss: 2.2774, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [5/10], Step [313/600], Loss: 2.2850, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [5/10], Step [314/600], Loss: 2.3131, batch time: 0.56, accuracy:  6.00%\n",
      "Epoch [5/10], Step [315/600], Loss: 2.2894, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [5/10], Step [316/600], Loss: 2.2964, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [5/10], Step [317/600], Loss: 2.3014, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [5/10], Step [318/600], Loss: 2.3073, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [5/10], Step [319/600], Loss: 2.2948, batch time: 0.61, accuracy:  8.00%\n",
      "Epoch [5/10], Step [320/600], Loss: 2.3018, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [5/10], Step [321/600], Loss: 2.2601, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [5/10], Step [322/600], Loss: 2.2892, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [5/10], Step [323/600], Loss: 2.3234, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [5/10], Step [324/600], Loss: 2.2971, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [5/10], Step [325/600], Loss: 2.3250, batch time: 0.57, accuracy:  4.00%\n",
      "Epoch [5/10], Step [326/600], Loss: 2.2828, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [327/600], Loss: 2.3020, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [5/10], Step [328/600], Loss: 2.2828, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [5/10], Step [329/600], Loss: 2.3192, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [330/600], Loss: 2.2943, batch time: 0.63, accuracy:  10.00%\n",
      "Epoch [5/10], Step [331/600], Loss: 2.3021, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [5/10], Step [332/600], Loss: 2.3218, batch time: 0.67, accuracy:  6.00%\n",
      "Epoch [5/10], Step [333/600], Loss: 2.2910, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [5/10], Step [334/600], Loss: 2.3045, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [5/10], Step [335/600], Loss: 2.2862, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [5/10], Step [336/600], Loss: 2.2967, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [5/10], Step [337/600], Loss: 2.2850, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [5/10], Step [338/600], Loss: 2.3083, batch time: 0.56, accuracy:  6.00%\n",
      "Epoch [5/10], Step [339/600], Loss: 2.2882, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [5/10], Step [340/600], Loss: 2.3051, batch time: 0.66, accuracy:  6.00%\n",
      "Epoch [5/10], Step [341/600], Loss: 2.2856, batch time: 0.68, accuracy:  15.00%\n",
      "Epoch [5/10], Step [342/600], Loss: 2.3184, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [5/10], Step [343/600], Loss: 2.3207, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [5/10], Step [344/600], Loss: 2.3159, batch time: 0.59, accuracy:  4.00%\n",
      "Epoch [5/10], Step [345/600], Loss: 2.3126, batch time: 0.59, accuracy:  9.00%\n",
      "Epoch [5/10], Step [346/600], Loss: 2.3140, batch time: 0.73, accuracy:  4.00%\n",
      "Epoch [5/10], Step [347/600], Loss: 2.3095, batch time: 0.60, accuracy:  6.00%\n",
      "Epoch [5/10], Step [348/600], Loss: 2.3262, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [5/10], Step [349/600], Loss: 2.2982, batch time: 0.61, accuracy:  12.00%\n",
      "Epoch [5/10], Step [350/600], Loss: 2.2935, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [5/10], Step [351/600], Loss: 2.3071, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [5/10], Step [352/600], Loss: 2.2992, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [5/10], Step [353/600], Loss: 2.2889, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [354/600], Loss: 2.3000, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [5/10], Step [355/600], Loss: 2.3080, batch time: 0.61, accuracy:  11.00%\n",
      "Epoch [5/10], Step [356/600], Loss: 2.2699, batch time: 0.57, accuracy:  18.00%\n",
      "Epoch [5/10], Step [357/600], Loss: 2.2845, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [5/10], Step [358/600], Loss: 2.2826, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [5/10], Step [359/600], Loss: 2.3192, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [5/10], Step [360/600], Loss: 2.3143, batch time: 0.60, accuracy:  6.00%\n",
      "Epoch [5/10], Step [361/600], Loss: 2.2962, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [5/10], Step [362/600], Loss: 2.3035, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [5/10], Step [363/600], Loss: 2.2911, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [5/10], Step [364/600], Loss: 2.3154, batch time: 0.56, accuracy:  5.00%\n",
      "Epoch [5/10], Step [365/600], Loss: 2.3127, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [5/10], Step [366/600], Loss: 2.3163, batch time: 0.64, accuracy:  7.00%\n",
      "Epoch [5/10], Step [367/600], Loss: 2.2980, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [5/10], Step [368/600], Loss: 2.3194, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [5/10], Step [369/600], Loss: 2.3023, batch time: 0.58, accuracy:  5.00%\n",
      "Epoch [5/10], Step [370/600], Loss: 2.2836, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [5/10], Step [371/600], Loss: 2.3014, batch time: 0.55, accuracy:  8.00%\n",
      "Epoch [5/10], Step [372/600], Loss: 2.2949, batch time: 0.66, accuracy:  6.00%\n",
      "Epoch [5/10], Step [373/600], Loss: 2.3149, batch time: 0.67, accuracy:  5.00%\n",
      "Epoch [5/10], Step [374/600], Loss: 2.2885, batch time: 0.62, accuracy:  12.00%\n",
      "Epoch [5/10], Step [375/600], Loss: 2.2880, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [5/10], Step [376/600], Loss: 2.2916, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [5/10], Step [377/600], Loss: 2.3138, batch time: 0.57, accuracy:  5.00%\n",
      "Epoch [5/10], Step [378/600], Loss: 2.3089, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [5/10], Step [379/600], Loss: 2.3068, batch time: 0.59, accuracy:  6.00%\n",
      "Epoch [5/10], Step [380/600], Loss: 2.3135, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [5/10], Step [381/600], Loss: 2.2928, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [5/10], Step [382/600], Loss: 2.2959, batch time: 0.56, accuracy:  4.00%\n",
      "Epoch [5/10], Step [383/600], Loss: 2.3049, batch time: 0.60, accuracy:  6.00%\n",
      "Epoch [5/10], Step [384/600], Loss: 2.2888, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [5/10], Step [385/600], Loss: 2.2935, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [5/10], Step [386/600], Loss: 2.3028, batch time: 0.59, accuracy:  5.00%\n",
      "Epoch [5/10], Step [387/600], Loss: 2.2963, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [5/10], Step [388/600], Loss: 2.2879, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [5/10], Step [389/600], Loss: 2.2970, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [5/10], Step [390/600], Loss: 2.3104, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [5/10], Step [391/600], Loss: 2.2952, batch time: 0.55, accuracy:  8.00%\n",
      "Epoch [5/10], Step [392/600], Loss: 2.2789, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [5/10], Step [393/600], Loss: 2.2784, batch time: 0.56, accuracy:  19.00%\n",
      "Epoch [5/10], Step [394/600], Loss: 2.2867, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [5/10], Step [395/600], Loss: 2.2893, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [5/10], Step [396/600], Loss: 2.3098, batch time: 0.56, accuracy:  7.00%\n",
      "Epoch [5/10], Step [397/600], Loss: 2.2924, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [5/10], Step [398/600], Loss: 2.2994, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [5/10], Step [399/600], Loss: 2.2974, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [5/10], Step [400/600], Loss: 2.2872, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [5/10], Step [401/600], Loss: 2.3060, batch time: 0.68, accuracy:  4.00%\n",
      "Epoch [5/10], Step [402/600], Loss: 2.2983, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [5/10], Step [403/600], Loss: 2.2970, batch time: 0.70, accuracy:  7.00%\n",
      "Epoch [5/10], Step [404/600], Loss: 2.2961, batch time: 0.67, accuracy:  15.00%\n",
      "Epoch [5/10], Step [405/600], Loss: 2.2956, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [5/10], Step [406/600], Loss: 2.2871, batch time: 0.55, accuracy:  9.00%\n",
      "Epoch [5/10], Step [407/600], Loss: 2.3064, batch time: 0.56, accuracy:  4.00%\n",
      "Epoch [5/10], Step [408/600], Loss: 2.3087, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [5/10], Step [409/600], Loss: 2.3033, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [5/10], Step [410/600], Loss: 2.2902, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [5/10], Step [411/600], Loss: 2.2834, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [5/10], Step [412/600], Loss: 2.3159, batch time: 0.56, accuracy:  6.00%\n",
      "Epoch [5/10], Step [413/600], Loss: 2.2979, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [5/10], Step [414/600], Loss: 2.2995, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [5/10], Step [415/600], Loss: 2.2795, batch time: 0.60, accuracy:  11.00%\n",
      "Epoch [5/10], Step [416/600], Loss: 2.3068, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [5/10], Step [417/600], Loss: 2.3102, batch time: 0.56, accuracy:  5.00%\n",
      "Epoch [5/10], Step [418/600], Loss: 2.2964, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [5/10], Step [419/600], Loss: 2.3206, batch time: 0.57, accuracy:  5.00%\n",
      "Epoch [5/10], Step [420/600], Loss: 2.3002, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [5/10], Step [421/600], Loss: 2.2944, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [5/10], Step [422/600], Loss: 2.3019, batch time: 0.56, accuracy:  4.00%\n",
      "Epoch [5/10], Step [423/600], Loss: 2.2933, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [5/10], Step [424/600], Loss: 2.2758, batch time: 0.69, accuracy:  14.00%\n",
      "Epoch [5/10], Step [425/600], Loss: 2.2999, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [5/10], Step [426/600], Loss: 2.2917, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [5/10], Step [427/600], Loss: 2.2686, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [5/10], Step [428/600], Loss: 2.2929, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [5/10], Step [429/600], Loss: 2.3155, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [5/10], Step [430/600], Loss: 2.2858, batch time: 0.59, accuracy:  16.00%\n",
      "Epoch [5/10], Step [431/600], Loss: 2.3028, batch time: 0.56, accuracy:  7.00%\n",
      "Epoch [5/10], Step [432/600], Loss: 2.2971, batch time: 0.65, accuracy:  7.00%\n",
      "Epoch [5/10], Step [433/600], Loss: 2.2792, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [5/10], Step [434/600], Loss: 2.2809, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [5/10], Step [435/600], Loss: 2.3049, batch time: 0.68, accuracy:  5.00%\n",
      "Epoch [5/10], Step [436/600], Loss: 2.2971, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [5/10], Step [437/600], Loss: 2.3089, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [5/10], Step [438/600], Loss: 2.2982, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [5/10], Step [439/600], Loss: 2.2793, batch time: 0.67, accuracy:  14.00%\n",
      "Epoch [5/10], Step [440/600], Loss: 2.3092, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [5/10], Step [441/600], Loss: 2.2859, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [5/10], Step [442/600], Loss: 2.2986, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [5/10], Step [443/600], Loss: 2.3123, batch time: 0.69, accuracy:  7.00%\n",
      "Epoch [5/10], Step [444/600], Loss: 2.3044, batch time: 0.70, accuracy:  10.00%\n",
      "Epoch [5/10], Step [445/600], Loss: 2.2872, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [5/10], Step [446/600], Loss: 2.2997, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [5/10], Step [447/600], Loss: 2.2840, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [5/10], Step [448/600], Loss: 2.3023, batch time: 0.70, accuracy:  9.00%\n",
      "Epoch [5/10], Step [449/600], Loss: 2.2976, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [5/10], Step [450/600], Loss: 2.3010, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [5/10], Step [451/600], Loss: 2.3284, batch time: 0.68, accuracy:  5.00%\n",
      "Epoch [5/10], Step [452/600], Loss: 2.2851, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [5/10], Step [453/600], Loss: 2.2966, batch time: 0.64, accuracy:  13.00%\n",
      "Epoch [5/10], Step [454/600], Loss: 2.3028, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [5/10], Step [455/600], Loss: 2.2978, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [5/10], Step [456/600], Loss: 2.3108, batch time: 0.59, accuracy:  6.00%\n",
      "Epoch [5/10], Step [457/600], Loss: 2.2764, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [5/10], Step [458/600], Loss: 2.3084, batch time: 0.59, accuracy:  7.00%\n",
      "Epoch [5/10], Step [459/600], Loss: 2.3008, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [5/10], Step [460/600], Loss: 2.2882, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [5/10], Step [461/600], Loss: 2.2655, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [5/10], Step [462/600], Loss: 2.2968, batch time: 0.65, accuracy:  9.00%\n",
      "Epoch [5/10], Step [463/600], Loss: 2.3048, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [5/10], Step [464/600], Loss: 2.3059, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [5/10], Step [465/600], Loss: 2.2894, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [5/10], Step [466/600], Loss: 2.2907, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [5/10], Step [467/600], Loss: 2.2857, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [5/10], Step [468/600], Loss: 2.2900, batch time: 0.60, accuracy:  13.00%\n",
      "Epoch [5/10], Step [469/600], Loss: 2.3057, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [5/10], Step [470/600], Loss: 2.3070, batch time: 0.59, accuracy:  3.00%\n",
      "Epoch [5/10], Step [471/600], Loss: 2.3041, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [5/10], Step [472/600], Loss: 2.2888, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [473/600], Loss: 2.2988, batch time: 0.58, accuracy:  5.00%\n",
      "Epoch [5/10], Step [474/600], Loss: 2.2950, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [5/10], Step [475/600], Loss: 2.2945, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [5/10], Step [476/600], Loss: 2.3013, batch time: 0.59, accuracy:  6.00%\n",
      "Epoch [5/10], Step [477/600], Loss: 2.3039, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [5/10], Step [478/600], Loss: 2.2953, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [5/10], Step [479/600], Loss: 2.2789, batch time: 0.61, accuracy:  10.00%\n",
      "Epoch [5/10], Step [480/600], Loss: 2.3071, batch time: 0.58, accuracy:  5.00%\n",
      "Epoch [5/10], Step [481/600], Loss: 2.2941, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [5/10], Step [482/600], Loss: 2.2682, batch time: 0.60, accuracy:  11.00%\n",
      "Epoch [5/10], Step [483/600], Loss: 2.2947, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [5/10], Step [484/600], Loss: 2.2803, batch time: 0.63, accuracy:  8.00%\n",
      "Epoch [5/10], Step [485/600], Loss: 2.3067, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [5/10], Step [486/600], Loss: 2.2836, batch time: 0.58, accuracy:  15.00%\n",
      "Epoch [5/10], Step [487/600], Loss: 2.2848, batch time: 0.59, accuracy:  6.00%\n",
      "Epoch [5/10], Step [488/600], Loss: 2.3034, batch time: 0.75, accuracy:  9.00%\n",
      "Epoch [5/10], Step [489/600], Loss: 2.2880, batch time: 1.49, accuracy:  8.00%\n",
      "Epoch [5/10], Step [490/600], Loss: 2.2923, batch time: 1.24, accuracy:  10.00%\n",
      "Epoch [5/10], Step [491/600], Loss: 2.2889, batch time: 0.76, accuracy:  9.00%\n",
      "Epoch [5/10], Step [492/600], Loss: 2.2976, batch time: 0.70, accuracy:  9.00%\n",
      "Epoch [5/10], Step [493/600], Loss: 2.2911, batch time: 0.73, accuracy:  11.00%\n",
      "Epoch [5/10], Step [494/600], Loss: 2.3034, batch time: 0.60, accuracy:  7.00%\n",
      "Epoch [5/10], Step [495/600], Loss: 2.2904, batch time: 0.71, accuracy:  10.00%\n",
      "Epoch [5/10], Step [496/600], Loss: 2.2982, batch time: 0.59, accuracy:  12.00%\n",
      "Epoch [5/10], Step [497/600], Loss: 2.3023, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [5/10], Step [498/600], Loss: 2.3184, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [5/10], Step [499/600], Loss: 2.3234, batch time: 0.56, accuracy:  4.00%\n",
      "Epoch [5/10], Step [500/600], Loss: 2.3009, batch time: 0.55, accuracy:  15.00%\n",
      "Epoch [5/10], Step [501/600], Loss: 2.3047, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [5/10], Step [502/600], Loss: 2.2934, batch time: 0.71, accuracy:  11.00%\n",
      "Epoch [5/10], Step [503/600], Loss: 2.3085, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [5/10], Step [504/600], Loss: 2.2958, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [5/10], Step [505/600], Loss: 2.2892, batch time: 0.56, accuracy:  16.00%\n",
      "Epoch [5/10], Step [506/600], Loss: 2.2990, batch time: 0.55, accuracy:  9.00%\n",
      "Epoch [5/10], Step [507/600], Loss: 2.2878, batch time: 0.60, accuracy:  14.00%\n",
      "Epoch [5/10], Step [508/600], Loss: 2.3122, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [5/10], Step [509/600], Loss: 2.2990, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [5/10], Step [510/600], Loss: 2.2881, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [5/10], Step [511/600], Loss: 2.2919, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [5/10], Step [512/600], Loss: 2.2834, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [5/10], Step [513/600], Loss: 2.2823, batch time: 0.68, accuracy:  16.00%\n",
      "Epoch [5/10], Step [514/600], Loss: 2.2828, batch time: 0.67, accuracy:  14.00%\n",
      "Epoch [5/10], Step [515/600], Loss: 2.2788, batch time: 0.67, accuracy:  14.00%\n",
      "Epoch [5/10], Step [516/600], Loss: 2.3001, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [5/10], Step [517/600], Loss: 2.2952, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [5/10], Step [518/600], Loss: 2.2939, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [5/10], Step [519/600], Loss: 2.3172, batch time: 0.67, accuracy:  8.00%\n",
      "Epoch [5/10], Step [520/600], Loss: 2.2988, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [5/10], Step [521/600], Loss: 2.3003, batch time: 0.67, accuracy:  6.00%\n",
      "Epoch [5/10], Step [522/600], Loss: 2.3196, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [5/10], Step [523/600], Loss: 2.3040, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [5/10], Step [524/600], Loss: 2.2764, batch time: 0.57, accuracy:  19.00%\n",
      "Epoch [5/10], Step [525/600], Loss: 2.3058, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [5/10], Step [526/600], Loss: 2.2929, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [5/10], Step [527/600], Loss: 2.3157, batch time: 0.56, accuracy:  6.00%\n",
      "Epoch [5/10], Step [528/600], Loss: 2.3093, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [529/600], Loss: 2.2953, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [5/10], Step [530/600], Loss: 2.2742, batch time: 0.67, accuracy:  20.00%\n",
      "Epoch [5/10], Step [531/600], Loss: 2.3019, batch time: 0.68, accuracy:  15.00%\n",
      "Epoch [5/10], Step [532/600], Loss: 2.2976, batch time: 0.64, accuracy:  13.00%\n",
      "Epoch [5/10], Step [533/600], Loss: 2.2800, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [5/10], Step [534/600], Loss: 2.3188, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [5/10], Step [535/600], Loss: 2.3044, batch time: 0.63, accuracy:  7.00%\n",
      "Epoch [5/10], Step [536/600], Loss: 2.2969, batch time: 0.59, accuracy:  15.00%\n",
      "Epoch [5/10], Step [537/600], Loss: 2.2908, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [5/10], Step [538/600], Loss: 2.2967, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [5/10], Step [539/600], Loss: 2.2976, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [5/10], Step [540/600], Loss: 2.3020, batch time: 0.59, accuracy:  14.00%\n",
      "Epoch [5/10], Step [541/600], Loss: 2.2579, batch time: 0.57, accuracy:  20.00%\n",
      "Epoch [5/10], Step [542/600], Loss: 2.3038, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [5/10], Step [543/600], Loss: 2.2919, batch time: 0.64, accuracy:  9.00%\n",
      "Epoch [5/10], Step [544/600], Loss: 2.3079, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [5/10], Step [545/600], Loss: 2.2923, batch time: 0.64, accuracy:  11.00%\n",
      "Epoch [5/10], Step [546/600], Loss: 2.3017, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [5/10], Step [547/600], Loss: 2.2971, batch time: 0.70, accuracy:  16.00%\n",
      "Epoch [5/10], Step [548/600], Loss: 2.2775, batch time: 0.58, accuracy:  16.00%\n",
      "Epoch [5/10], Step [549/600], Loss: 2.2999, batch time: 0.65, accuracy:  13.00%\n",
      "Epoch [5/10], Step [550/600], Loss: 2.2915, batch time: 0.59, accuracy:  12.00%\n",
      "Epoch [5/10], Step [551/600], Loss: 2.2798, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [5/10], Step [552/600], Loss: 2.2834, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [5/10], Step [553/600], Loss: 2.3116, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [5/10], Step [554/600], Loss: 2.3009, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [5/10], Step [555/600], Loss: 2.2999, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [5/10], Step [556/600], Loss: 2.2883, batch time: 0.70, accuracy:  14.00%\n",
      "Epoch [5/10], Step [557/600], Loss: 2.2910, batch time: 0.64, accuracy:  13.00%\n",
      "Epoch [5/10], Step [558/600], Loss: 2.2817, batch time: 0.56, accuracy:  15.00%\n",
      "Epoch [5/10], Step [559/600], Loss: 2.3019, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [5/10], Step [560/600], Loss: 2.2820, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [5/10], Step [561/600], Loss: 2.3084, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [5/10], Step [562/600], Loss: 2.3029, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [5/10], Step [563/600], Loss: 2.2918, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [5/10], Step [564/600], Loss: 2.3161, batch time: 0.59, accuracy:  9.00%\n",
      "Epoch [5/10], Step [565/600], Loss: 2.2837, batch time: 0.58, accuracy:  15.00%\n",
      "Epoch [5/10], Step [566/600], Loss: 2.2934, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [5/10], Step [567/600], Loss: 2.3052, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [5/10], Step [568/600], Loss: 2.2963, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [5/10], Step [569/600], Loss: 2.3218, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [5/10], Step [570/600], Loss: 2.2812, batch time: 0.60, accuracy:  14.00%\n",
      "Epoch [5/10], Step [571/600], Loss: 2.2828, batch time: 0.61, accuracy:  13.00%\n",
      "Epoch [5/10], Step [572/600], Loss: 2.2810, batch time: 0.56, accuracy:  20.00%\n",
      "Epoch [5/10], Step [573/600], Loss: 2.3018, batch time: 0.60, accuracy:  14.00%\n",
      "Epoch [5/10], Step [574/600], Loss: 2.2883, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [5/10], Step [575/600], Loss: 2.2847, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [5/10], Step [576/600], Loss: 2.2794, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [5/10], Step [577/600], Loss: 2.2951, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [5/10], Step [578/600], Loss: 2.2975, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [5/10], Step [579/600], Loss: 2.3005, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [5/10], Step [580/600], Loss: 2.2866, batch time: 0.80, accuracy:  12.00%\n",
      "Epoch [5/10], Step [581/600], Loss: 2.2931, batch time: 0.66, accuracy:  12.00%\n",
      "Epoch [5/10], Step [582/600], Loss: 2.2850, batch time: 0.66, accuracy:  14.00%\n",
      "Epoch [5/10], Step [583/600], Loss: 2.2879, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [5/10], Step [584/600], Loss: 2.3019, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [5/10], Step [585/600], Loss: 2.3182, batch time: 0.59, accuracy:  9.00%\n",
      "Epoch [5/10], Step [586/600], Loss: 2.3032, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [5/10], Step [587/600], Loss: 2.2772, batch time: 0.64, accuracy:  10.00%\n",
      "Epoch [5/10], Step [588/600], Loss: 2.3062, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [5/10], Step [589/600], Loss: 2.2868, batch time: 0.58, accuracy:  17.00%\n",
      "Epoch [5/10], Step [590/600], Loss: 2.2978, batch time: 0.59, accuracy:  12.00%\n",
      "Epoch [5/10], Step [591/600], Loss: 2.2990, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [5/10], Step [592/600], Loss: 2.2951, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [5/10], Step [593/600], Loss: 2.2934, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [5/10], Step [594/600], Loss: 2.3052, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [5/10], Step [595/600], Loss: 2.2850, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [5/10], Step [596/600], Loss: 2.3081, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [5/10], Step [597/600], Loss: 2.2998, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [5/10], Step [598/600], Loss: 2.3094, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [5/10], Step [599/600], Loss: 2.2701, batch time: 0.58, accuracy:  21.00%\n",
      "Epoch [5/10], Step [600/600], Loss: 2.2858, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [6/10], Step [1/600], Loss: 2.2885, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [6/10], Step [2/600], Loss: 2.3061, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [6/10], Step [3/600], Loss: 2.2874, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [6/10], Step [4/600], Loss: 2.2966, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [6/10], Step [5/600], Loss: 2.2877, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [6/10], Step [6/600], Loss: 2.2770, batch time: 0.59, accuracy:  17.00%\n",
      "Epoch [6/10], Step [7/600], Loss: 2.2711, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [6/10], Step [8/600], Loss: 2.2997, batch time: 0.59, accuracy:  8.00%\n",
      "Epoch [6/10], Step [9/600], Loss: 2.3029, batch time: 0.59, accuracy:  14.00%\n",
      "Epoch [6/10], Step [10/600], Loss: 2.3049, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [6/10], Step [11/600], Loss: 2.2883, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [6/10], Step [12/600], Loss: 2.3216, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [6/10], Step [13/600], Loss: 2.3030, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [6/10], Step [14/600], Loss: 2.2922, batch time: 0.59, accuracy:  9.00%\n",
      "Epoch [6/10], Step [15/600], Loss: 2.2635, batch time: 0.58, accuracy:  16.00%\n",
      "Epoch [6/10], Step [16/600], Loss: 2.3018, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [6/10], Step [17/600], Loss: 2.2956, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [6/10], Step [18/600], Loss: 2.3055, batch time: 0.57, accuracy:  5.00%\n",
      "Epoch [6/10], Step [19/600], Loss: 2.2963, batch time: 0.59, accuracy:  12.00%\n",
      "Epoch [6/10], Step [20/600], Loss: 2.3115, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [6/10], Step [21/600], Loss: 2.3111, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [6/10], Step [22/600], Loss: 2.2965, batch time: 0.62, accuracy:  13.00%\n",
      "Epoch [6/10], Step [23/600], Loss: 2.2702, batch time: 0.57, accuracy:  18.00%\n",
      "Epoch [6/10], Step [24/600], Loss: 2.2979, batch time: 0.65, accuracy:  11.00%\n",
      "Epoch [6/10], Step [25/600], Loss: 2.3004, batch time: 0.60, accuracy:  10.00%\n",
      "Epoch [6/10], Step [26/600], Loss: 2.2828, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [6/10], Step [27/600], Loss: 2.3045, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [6/10], Step [28/600], Loss: 2.2934, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [6/10], Step [29/600], Loss: 2.2876, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [6/10], Step [30/600], Loss: 2.2671, batch time: 0.56, accuracy:  18.00%\n",
      "Epoch [6/10], Step [31/600], Loss: 2.3011, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [6/10], Step [32/600], Loss: 2.2805, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [6/10], Step [33/600], Loss: 2.3244, batch time: 0.64, accuracy:  10.00%\n",
      "Epoch [6/10], Step [34/600], Loss: 2.2998, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [6/10], Step [35/600], Loss: 2.3012, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [6/10], Step [36/600], Loss: 2.3021, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [6/10], Step [37/600], Loss: 2.2672, batch time: 0.69, accuracy:  16.00%\n",
      "Epoch [6/10], Step [38/600], Loss: 2.2925, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [6/10], Step [39/600], Loss: 2.2850, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [6/10], Step [40/600], Loss: 2.2909, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [6/10], Step [41/600], Loss: 2.2878, batch time: 0.60, accuracy:  15.00%\n",
      "Epoch [6/10], Step [42/600], Loss: 2.2867, batch time: 0.63, accuracy:  18.00%\n",
      "Epoch [6/10], Step [43/600], Loss: 2.2840, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [6/10], Step [44/600], Loss: 2.3148, batch time: 0.60, accuracy:  6.00%\n",
      "Epoch [6/10], Step [45/600], Loss: 2.2961, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [6/10], Step [46/600], Loss: 2.2755, batch time: 0.65, accuracy:  11.00%\n",
      "Epoch [6/10], Step [47/600], Loss: 2.2982, batch time: 0.69, accuracy:  17.00%\n",
      "Epoch [6/10], Step [48/600], Loss: 2.2986, batch time: 0.70, accuracy:  14.00%\n",
      "Epoch [6/10], Step [49/600], Loss: 2.2879, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [6/10], Step [50/600], Loss: 2.2956, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [6/10], Step [51/600], Loss: 2.2969, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [6/10], Step [52/600], Loss: 2.3024, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [6/10], Step [53/600], Loss: 2.3036, batch time: 0.70, accuracy:  8.00%\n",
      "Epoch [6/10], Step [54/600], Loss: 2.3006, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [6/10], Step [55/600], Loss: 2.2985, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [6/10], Step [56/600], Loss: 2.3003, batch time: 0.70, accuracy:  14.00%\n",
      "Epoch [6/10], Step [57/600], Loss: 2.3117, batch time: 0.61, accuracy:  5.00%\n",
      "Epoch [6/10], Step [58/600], Loss: 2.2910, batch time: 0.71, accuracy:  4.00%\n",
      "Epoch [6/10], Step [59/600], Loss: 2.2811, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [6/10], Step [60/600], Loss: 2.3064, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [6/10], Step [61/600], Loss: 2.2941, batch time: 0.58, accuracy:  16.00%\n",
      "Epoch [6/10], Step [62/600], Loss: 2.2824, batch time: 0.58, accuracy:  15.00%\n",
      "Epoch [6/10], Step [63/600], Loss: 2.2974, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [6/10], Step [64/600], Loss: 2.2943, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [6/10], Step [65/600], Loss: 2.3004, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [6/10], Step [66/600], Loss: 2.3130, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [6/10], Step [67/600], Loss: 2.2965, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [6/10], Step [68/600], Loss: 2.2958, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [6/10], Step [69/600], Loss: 2.2948, batch time: 0.60, accuracy:  16.00%\n",
      "Epoch [6/10], Step [70/600], Loss: 2.3007, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [6/10], Step [71/600], Loss: 2.3124, batch time: 0.62, accuracy:  13.00%\n",
      "Epoch [6/10], Step [72/600], Loss: 2.3034, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [6/10], Step [73/600], Loss: 2.3029, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [6/10], Step [74/600], Loss: 2.3108, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [6/10], Step [75/600], Loss: 2.3013, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [6/10], Step [76/600], Loss: 2.2950, batch time: 0.66, accuracy:  11.00%\n",
      "Epoch [6/10], Step [77/600], Loss: 2.2831, batch time: 0.55, accuracy:  13.00%\n",
      "Epoch [6/10], Step [78/600], Loss: 2.2987, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [6/10], Step [79/600], Loss: 2.2802, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [6/10], Step [80/600], Loss: 2.2984, batch time: 0.64, accuracy:  12.00%\n",
      "Epoch [6/10], Step [81/600], Loss: 2.3052, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [6/10], Step [82/600], Loss: 2.2879, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [6/10], Step [83/600], Loss: 2.2865, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [6/10], Step [84/600], Loss: 2.3126, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [6/10], Step [85/600], Loss: 2.3065, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [6/10], Step [86/600], Loss: 2.3048, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [6/10], Step [87/600], Loss: 2.2897, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [6/10], Step [88/600], Loss: 2.3223, batch time: 0.71, accuracy:  4.00%\n",
      "Epoch [6/10], Step [89/600], Loss: 2.2875, batch time: 0.68, accuracy:  19.00%\n",
      "Epoch [6/10], Step [90/600], Loss: 2.2922, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [6/10], Step [91/600], Loss: 2.3045, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [6/10], Step [92/600], Loss: 2.2837, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [6/10], Step [93/600], Loss: 2.3002, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [6/10], Step [94/600], Loss: 2.3016, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [6/10], Step [95/600], Loss: 2.3131, batch time: 0.67, accuracy:  5.00%\n",
      "Epoch [6/10], Step [96/600], Loss: 2.3074, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [6/10], Step [97/600], Loss: 2.2806, batch time: 0.68, accuracy:  16.00%\n",
      "Epoch [6/10], Step [98/600], Loss: 2.2992, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [6/10], Step [99/600], Loss: 2.2868, batch time: 0.67, accuracy:  16.00%\n",
      "Epoch [6/10], Step [100/600], Loss: 2.3004, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [6/10], Step [101/600], Loss: 2.2917, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [6/10], Step [102/600], Loss: 2.2805, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [6/10], Step [103/600], Loss: 2.3042, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [6/10], Step [104/600], Loss: 2.3022, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [6/10], Step [105/600], Loss: 2.2817, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [6/10], Step [106/600], Loss: 2.2950, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [6/10], Step [107/600], Loss: 2.3021, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [6/10], Step [108/600], Loss: 2.3094, batch time: 0.59, accuracy:  7.00%\n",
      "Epoch [6/10], Step [109/600], Loss: 2.2742, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [6/10], Step [110/600], Loss: 2.3076, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [6/10], Step [111/600], Loss: 2.3098, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [6/10], Step [112/600], Loss: 2.3005, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [6/10], Step [113/600], Loss: 2.2621, batch time: 0.64, accuracy:  18.00%\n",
      "Epoch [6/10], Step [114/600], Loss: 2.3089, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [6/10], Step [115/600], Loss: 2.2789, batch time: 0.68, accuracy:  16.00%\n",
      "Epoch [6/10], Step [116/600], Loss: 2.2834, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [6/10], Step [117/600], Loss: 2.2968, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [6/10], Step [118/600], Loss: 2.2756, batch time: 0.69, accuracy:  18.00%\n",
      "Epoch [6/10], Step [119/600], Loss: 2.3145, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [6/10], Step [120/600], Loss: 2.2899, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [6/10], Step [121/600], Loss: 2.2971, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [6/10], Step [122/600], Loss: 2.2981, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [6/10], Step [123/600], Loss: 2.2939, batch time: 0.69, accuracy:  14.00%\n",
      "Epoch [6/10], Step [124/600], Loss: 2.2788, batch time: 0.70, accuracy:  14.00%\n",
      "Epoch [6/10], Step [125/600], Loss: 2.2912, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [6/10], Step [126/600], Loss: 2.2974, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [6/10], Step [127/600], Loss: 2.2793, batch time: 0.68, accuracy:  18.00%\n",
      "Epoch [6/10], Step [128/600], Loss: 2.3046, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [6/10], Step [129/600], Loss: 2.2918, batch time: 0.62, accuracy:  12.00%\n",
      "Epoch [6/10], Step [130/600], Loss: 2.2947, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [6/10], Step [131/600], Loss: 2.3002, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [6/10], Step [132/600], Loss: 2.2967, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [6/10], Step [133/600], Loss: 2.3037, batch time: 0.69, accuracy:  14.00%\n",
      "Epoch [6/10], Step [134/600], Loss: 2.2895, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [6/10], Step [135/600], Loss: 2.3012, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [6/10], Step [136/600], Loss: 2.3036, batch time: 0.82, accuracy:  10.00%\n",
      "Epoch [6/10], Step [137/600], Loss: 2.2854, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [6/10], Step [138/600], Loss: 2.2630, batch time: 0.69, accuracy:  16.00%\n",
      "Epoch [6/10], Step [139/600], Loss: 2.2966, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [6/10], Step [140/600], Loss: 2.2767, batch time: 0.69, accuracy:  15.00%\n",
      "Epoch [6/10], Step [141/600], Loss: 2.2844, batch time: 0.73, accuracy:  11.00%\n",
      "Epoch [6/10], Step [142/600], Loss: 2.2860, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [6/10], Step [143/600], Loss: 2.3030, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [6/10], Step [144/600], Loss: 2.2952, batch time: 0.70, accuracy:  12.00%\n",
      "Epoch [6/10], Step [145/600], Loss: 2.2794, batch time: 0.68, accuracy:  18.00%\n",
      "Epoch [6/10], Step [146/600], Loss: 2.2774, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [6/10], Step [147/600], Loss: 2.2980, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [6/10], Step [148/600], Loss: 2.2777, batch time: 0.69, accuracy:  15.00%\n",
      "Epoch [6/10], Step [149/600], Loss: 2.2848, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [6/10], Step [150/600], Loss: 2.3003, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [6/10], Step [151/600], Loss: 2.2823, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [6/10], Step [152/600], Loss: 2.2829, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [6/10], Step [153/600], Loss: 2.2767, batch time: 0.69, accuracy:  17.00%\n",
      "Epoch [6/10], Step [154/600], Loss: 2.2943, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [6/10], Step [155/600], Loss: 2.2990, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [6/10], Step [156/600], Loss: 2.2908, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [6/10], Step [157/600], Loss: 2.2893, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [6/10], Step [158/600], Loss: 2.2951, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [6/10], Step [159/600], Loss: 2.2943, batch time: 0.68, accuracy:  16.00%\n",
      "Epoch [6/10], Step [160/600], Loss: 2.2811, batch time: 0.66, accuracy:  13.00%\n",
      "Epoch [6/10], Step [161/600], Loss: 2.3225, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [6/10], Step [162/600], Loss: 2.3136, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [6/10], Step [163/600], Loss: 2.2650, batch time: 0.68, accuracy:  16.00%\n",
      "Epoch [6/10], Step [164/600], Loss: 2.3141, batch time: 0.68, accuracy:  5.00%\n",
      "Epoch [6/10], Step [165/600], Loss: 2.2961, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [6/10], Step [166/600], Loss: 2.2711, batch time: 0.67, accuracy:  16.00%\n",
      "Epoch [6/10], Step [167/600], Loss: 2.3201, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [6/10], Step [168/600], Loss: 2.2805, batch time: 0.69, accuracy:  6.00%\n",
      "Epoch [6/10], Step [169/600], Loss: 2.2994, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [6/10], Step [170/600], Loss: 2.2917, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [6/10], Step [171/600], Loss: 2.2986, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [6/10], Step [172/600], Loss: 2.3157, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [6/10], Step [173/600], Loss: 2.2978, batch time: 0.71, accuracy:  11.00%\n",
      "Epoch [6/10], Step [174/600], Loss: 2.2865, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [6/10], Step [175/600], Loss: 2.2900, batch time: 0.68, accuracy:  15.00%\n",
      "Epoch [6/10], Step [176/600], Loss: 2.2974, batch time: 0.69, accuracy:  14.00%\n",
      "Epoch [6/10], Step [177/600], Loss: 2.3152, batch time: 0.70, accuracy:  7.00%\n",
      "Epoch [6/10], Step [178/600], Loss: 2.2921, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [6/10], Step [179/600], Loss: 2.2988, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [6/10], Step [180/600], Loss: 2.2648, batch time: 0.69, accuracy:  18.00%\n",
      "Epoch [6/10], Step [181/600], Loss: 2.3076, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [6/10], Step [182/600], Loss: 2.2825, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [6/10], Step [183/600], Loss: 2.3072, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [6/10], Step [184/600], Loss: 2.2804, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [6/10], Step [185/600], Loss: 2.2824, batch time: 0.84, accuracy:  13.00%\n",
      "Epoch [6/10], Step [186/600], Loss: 2.2797, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [6/10], Step [187/600], Loss: 2.2949, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [6/10], Step [188/600], Loss: 2.2953, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [6/10], Step [189/600], Loss: 2.2971, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [6/10], Step [190/600], Loss: 2.2968, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [6/10], Step [191/600], Loss: 2.3022, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [6/10], Step [192/600], Loss: 2.3104, batch time: 0.67, accuracy:  8.00%\n",
      "Epoch [6/10], Step [193/600], Loss: 2.2887, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [6/10], Step [194/600], Loss: 2.2765, batch time: 0.68, accuracy:  17.00%\n",
      "Epoch [6/10], Step [195/600], Loss: 2.2726, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [6/10], Step [196/600], Loss: 2.3132, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [6/10], Step [197/600], Loss: 2.2993, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [6/10], Step [198/600], Loss: 2.2877, batch time: 0.63, accuracy:  12.00%\n",
      "Epoch [6/10], Step [199/600], Loss: 2.3341, batch time: 0.60, accuracy:  7.00%\n",
      "Epoch [6/10], Step [200/600], Loss: 2.2943, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [6/10], Step [201/600], Loss: 2.2860, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [6/10], Step [202/600], Loss: 2.3016, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [6/10], Step [203/600], Loss: 2.2801, batch time: 0.62, accuracy:  14.00%\n",
      "Epoch [6/10], Step [204/600], Loss: 2.2990, batch time: 0.64, accuracy:  9.00%\n",
      "Epoch [6/10], Step [205/600], Loss: 2.2857, batch time: 0.56, accuracy:  15.00%\n",
      "Epoch [6/10], Step [206/600], Loss: 2.3142, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [6/10], Step [207/600], Loss: 2.2897, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [6/10], Step [208/600], Loss: 2.2894, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [6/10], Step [209/600], Loss: 2.2786, batch time: 0.56, accuracy:  15.00%\n",
      "Epoch [6/10], Step [210/600], Loss: 2.2830, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [6/10], Step [211/600], Loss: 2.2843, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [6/10], Step [212/600], Loss: 2.2805, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [6/10], Step [213/600], Loss: 2.2885, batch time: 0.63, accuracy:  12.00%\n",
      "Epoch [6/10], Step [214/600], Loss: 2.2823, batch time: 0.72, accuracy:  11.00%\n",
      "Epoch [6/10], Step [215/600], Loss: 2.2826, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [6/10], Step [216/600], Loss: 2.3088, batch time: 0.57, accuracy:  5.00%\n",
      "Epoch [6/10], Step [217/600], Loss: 2.2994, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [6/10], Step [218/600], Loss: 2.2755, batch time: 0.60, accuracy:  11.00%\n",
      "Epoch [6/10], Step [219/600], Loss: 2.3035, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [6/10], Step [220/600], Loss: 2.2904, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [6/10], Step [221/600], Loss: 2.2997, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [6/10], Step [222/600], Loss: 2.2996, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [6/10], Step [223/600], Loss: 2.3041, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [6/10], Step [224/600], Loss: 2.3044, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [6/10], Step [225/600], Loss: 2.2878, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [6/10], Step [226/600], Loss: 2.3033, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [6/10], Step [227/600], Loss: 2.2840, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [6/10], Step [228/600], Loss: 2.2984, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [6/10], Step [229/600], Loss: 2.2770, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [6/10], Step [230/600], Loss: 2.3160, batch time: 0.64, accuracy:  9.00%\n",
      "Epoch [6/10], Step [231/600], Loss: 2.2836, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [6/10], Step [232/600], Loss: 2.2843, batch time: 0.63, accuracy:  13.00%\n",
      "Epoch [6/10], Step [233/600], Loss: 2.2975, batch time: 0.60, accuracy:  10.00%\n",
      "Epoch [6/10], Step [234/600], Loss: 2.3008, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [6/10], Step [235/600], Loss: 2.2973, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [6/10], Step [236/600], Loss: 2.2917, batch time: 0.59, accuracy:  9.00%\n",
      "Epoch [6/10], Step [237/600], Loss: 2.3002, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [6/10], Step [238/600], Loss: 2.2990, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [6/10], Step [239/600], Loss: 2.2896, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [6/10], Step [240/600], Loss: 2.2855, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [6/10], Step [241/600], Loss: 2.2771, batch time: 0.58, accuracy:  17.00%\n",
      "Epoch [6/10], Step [242/600], Loss: 2.3003, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [6/10], Step [243/600], Loss: 2.2963, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [6/10], Step [244/600], Loss: 2.3001, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [6/10], Step [245/600], Loss: 2.2973, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [6/10], Step [246/600], Loss: 2.3036, batch time: 0.70, accuracy:  10.00%\n",
      "Epoch [6/10], Step [247/600], Loss: 2.2906, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [6/10], Step [248/600], Loss: 2.2660, batch time: 0.68, accuracy:  15.00%\n",
      "Epoch [6/10], Step [249/600], Loss: 2.3012, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [6/10], Step [250/600], Loss: 2.2950, batch time: 0.69, accuracy:  15.00%\n",
      "Epoch [6/10], Step [251/600], Loss: 2.2995, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [6/10], Step [252/600], Loss: 2.2781, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [6/10], Step [253/600], Loss: 2.2792, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [6/10], Step [254/600], Loss: 2.3016, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [6/10], Step [255/600], Loss: 2.2725, batch time: 0.69, accuracy:  18.00%\n",
      "Epoch [6/10], Step [256/600], Loss: 2.2953, batch time: 0.69, accuracy:  15.00%\n",
      "Epoch [6/10], Step [257/600], Loss: 2.3046, batch time: 0.67, accuracy:  6.00%\n",
      "Epoch [6/10], Step [258/600], Loss: 2.2995, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [6/10], Step [259/600], Loss: 2.3160, batch time: 0.67, accuracy:  8.00%\n",
      "Epoch [6/10], Step [260/600], Loss: 2.2702, batch time: 0.69, accuracy:  17.00%\n",
      "Epoch [6/10], Step [261/600], Loss: 2.2961, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [6/10], Step [262/600], Loss: 2.3182, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [6/10], Step [263/600], Loss: 2.3188, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [6/10], Step [264/600], Loss: 2.2698, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [6/10], Step [265/600], Loss: 2.3003, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [6/10], Step [266/600], Loss: 2.2665, batch time: 0.68, accuracy:  16.00%\n",
      "Epoch [6/10], Step [267/600], Loss: 2.3107, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [6/10], Step [268/600], Loss: 2.2875, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [6/10], Step [269/600], Loss: 2.2793, batch time: 0.67, accuracy:  15.00%\n",
      "Epoch [6/10], Step [270/600], Loss: 2.2857, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [6/10], Step [271/600], Loss: 2.2989, batch time: 0.61, accuracy:  10.00%\n",
      "Epoch [6/10], Step [272/600], Loss: 2.3034, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [6/10], Step [273/600], Loss: 2.2994, batch time: 0.63, accuracy:  12.00%\n",
      "Epoch [6/10], Step [274/600], Loss: 2.2725, batch time: 0.65, accuracy:  14.00%\n",
      "Epoch [6/10], Step [275/600], Loss: 2.2947, batch time: 0.61, accuracy:  7.00%\n",
      "Epoch [6/10], Step [276/600], Loss: 2.3018, batch time: 0.59, accuracy:  9.00%\n",
      "Epoch [6/10], Step [277/600], Loss: 2.3136, batch time: 0.63, accuracy:  10.00%\n",
      "Epoch [6/10], Step [278/600], Loss: 2.2984, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [6/10], Step [279/600], Loss: 2.2777, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [6/10], Step [280/600], Loss: 2.2866, batch time: 0.55, accuracy:  15.00%\n",
      "Epoch [6/10], Step [281/600], Loss: 2.2963, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [6/10], Step [282/600], Loss: 2.3026, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [6/10], Step [283/600], Loss: 2.3022, batch time: 0.55, accuracy:  7.00%\n",
      "Epoch [6/10], Step [284/600], Loss: 2.3038, batch time: 0.62, accuracy:  13.00%\n",
      "Epoch [6/10], Step [285/600], Loss: 2.3020, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [6/10], Step [286/600], Loss: 2.3046, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [6/10], Step [287/600], Loss: 2.2917, batch time: 0.58, accuracy:  17.00%\n",
      "Epoch [6/10], Step [288/600], Loss: 2.2919, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [6/10], Step [289/600], Loss: 2.2896, batch time: 0.64, accuracy:  15.00%\n",
      "Epoch [6/10], Step [290/600], Loss: 2.2969, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [6/10], Step [291/600], Loss: 2.2752, batch time: 0.63, accuracy:  13.00%\n",
      "Epoch [6/10], Step [292/600], Loss: 2.2829, batch time: 0.71, accuracy:  13.00%\n",
      "Epoch [6/10], Step [293/600], Loss: 2.3011, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [6/10], Step [294/600], Loss: 2.2923, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [6/10], Step [295/600], Loss: 2.2842, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [6/10], Step [296/600], Loss: 2.2724, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [6/10], Step [297/600], Loss: 2.2804, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [6/10], Step [298/600], Loss: 2.3044, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [6/10], Step [299/600], Loss: 2.3077, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [6/10], Step [300/600], Loss: 2.2935, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [6/10], Step [301/600], Loss: 2.3104, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [6/10], Step [302/600], Loss: 2.2742, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [6/10], Step [303/600], Loss: 2.3121, batch time: 0.56, accuracy:  7.00%\n",
      "Epoch [6/10], Step [304/600], Loss: 2.2927, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [6/10], Step [305/600], Loss: 2.2943, batch time: 0.60, accuracy:  12.00%\n",
      "Epoch [6/10], Step [306/600], Loss: 2.2948, batch time: 0.66, accuracy:  8.00%\n",
      "Epoch [6/10], Step [307/600], Loss: 2.3069, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [6/10], Step [308/600], Loss: 2.3073, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [6/10], Step [309/600], Loss: 2.2942, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [6/10], Step [310/600], Loss: 2.2657, batch time: 0.68, accuracy:  19.00%\n",
      "Epoch [6/10], Step [311/600], Loss: 2.2913, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [6/10], Step [312/600], Loss: 2.3006, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [6/10], Step [313/600], Loss: 2.2643, batch time: 0.68, accuracy:  15.00%\n",
      "Epoch [6/10], Step [314/600], Loss: 2.3012, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [6/10], Step [315/600], Loss: 2.3094, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [6/10], Step [316/600], Loss: 2.2818, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [6/10], Step [317/600], Loss: 2.2906, batch time: 0.70, accuracy:  10.00%\n",
      "Epoch [6/10], Step [318/600], Loss: 2.3001, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [6/10], Step [319/600], Loss: 2.3113, batch time: 0.70, accuracy:  8.00%\n",
      "Epoch [6/10], Step [320/600], Loss: 2.2962, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [6/10], Step [321/600], Loss: 2.2834, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [6/10], Step [322/600], Loss: 2.2828, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [6/10], Step [323/600], Loss: 2.3086, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [6/10], Step [324/600], Loss: 2.3000, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [6/10], Step [325/600], Loss: 2.2772, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [6/10], Step [326/600], Loss: 2.2905, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [6/10], Step [327/600], Loss: 2.3095, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [6/10], Step [328/600], Loss: 2.2824, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [6/10], Step [329/600], Loss: 2.2949, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [6/10], Step [330/600], Loss: 2.2955, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [6/10], Step [331/600], Loss: 2.3125, batch time: 0.59, accuracy:  6.00%\n",
      "Epoch [6/10], Step [332/600], Loss: 2.2900, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [6/10], Step [333/600], Loss: 2.2919, batch time: 0.60, accuracy:  15.00%\n",
      "Epoch [6/10], Step [334/600], Loss: 2.2798, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [6/10], Step [335/600], Loss: 2.2870, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [6/10], Step [336/600], Loss: 2.2941, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [6/10], Step [337/600], Loss: 2.3160, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [6/10], Step [338/600], Loss: 2.3018, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [6/10], Step [339/600], Loss: 2.3151, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [6/10], Step [340/600], Loss: 2.3248, batch time: 0.56, accuracy:  4.00%\n",
      "Epoch [6/10], Step [341/600], Loss: 2.2872, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [6/10], Step [342/600], Loss: 2.2825, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [6/10], Step [343/600], Loss: 2.2820, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [6/10], Step [344/600], Loss: 2.3282, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [6/10], Step [345/600], Loss: 2.2816, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [6/10], Step [346/600], Loss: 2.3232, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [6/10], Step [347/600], Loss: 2.2992, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [6/10], Step [348/600], Loss: 2.3120, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [6/10], Step [349/600], Loss: 2.2959, batch time: 0.61, accuracy:  14.00%\n",
      "Epoch [6/10], Step [350/600], Loss: 2.2808, batch time: 0.56, accuracy:  18.00%\n",
      "Epoch [6/10], Step [351/600], Loss: 2.2999, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [6/10], Step [352/600], Loss: 2.2955, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [6/10], Step [353/600], Loss: 2.2881, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [6/10], Step [354/600], Loss: 2.3030, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [6/10], Step [355/600], Loss: 2.2895, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [6/10], Step [356/600], Loss: 2.2953, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [6/10], Step [357/600], Loss: 2.2976, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [6/10], Step [358/600], Loss: 2.2814, batch time: 0.56, accuracy:  15.00%\n",
      "Epoch [6/10], Step [359/600], Loss: 2.3119, batch time: 0.62, accuracy:  9.00%\n",
      "Epoch [6/10], Step [360/600], Loss: 2.2882, batch time: 0.63, accuracy:  11.00%\n",
      "Epoch [6/10], Step [361/600], Loss: 2.2719, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [6/10], Step [362/600], Loss: 2.3079, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [6/10], Step [363/600], Loss: 2.2824, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [6/10], Step [364/600], Loss: 2.3024, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [6/10], Step [365/600], Loss: 2.2766, batch time: 0.58, accuracy:  16.00%\n",
      "Epoch [6/10], Step [366/600], Loss: 2.2919, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [6/10], Step [367/600], Loss: 2.3149, batch time: 0.56, accuracy:  7.00%\n",
      "Epoch [6/10], Step [368/600], Loss: 2.2945, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [6/10], Step [369/600], Loss: 2.3077, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [6/10], Step [370/600], Loss: 2.3010, batch time: 0.70, accuracy:  11.00%\n",
      "Epoch [6/10], Step [371/600], Loss: 2.3230, batch time: 0.58, accuracy:  5.00%\n",
      "Epoch [6/10], Step [372/600], Loss: 2.2856, batch time: 0.56, accuracy:  15.00%\n",
      "Epoch [6/10], Step [373/600], Loss: 2.2728, batch time: 0.58, accuracy:  21.00%\n",
      "Epoch [6/10], Step [374/600], Loss: 2.3004, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [6/10], Step [375/600], Loss: 2.2785, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [6/10], Step [376/600], Loss: 2.2749, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [6/10], Step [377/600], Loss: 2.2844, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [6/10], Step [378/600], Loss: 2.3146, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [6/10], Step [379/600], Loss: 2.2777, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [6/10], Step [380/600], Loss: 2.2823, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [6/10], Step [381/600], Loss: 2.2935, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [6/10], Step [382/600], Loss: 2.2887, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [6/10], Step [383/600], Loss: 2.2985, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [6/10], Step [384/600], Loss: 2.3011, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [6/10], Step [385/600], Loss: 2.2870, batch time: 0.57, accuracy:  17.00%\n",
      "Epoch [6/10], Step [386/600], Loss: 2.3070, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [6/10], Step [387/600], Loss: 2.2729, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [6/10], Step [388/600], Loss: 2.2977, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [6/10], Step [389/600], Loss: 2.3122, batch time: 0.59, accuracy:  8.00%\n",
      "Epoch [6/10], Step [390/600], Loss: 2.3266, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [6/10], Step [391/600], Loss: 2.2926, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [6/10], Step [392/600], Loss: 2.2883, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [6/10], Step [393/600], Loss: 2.3102, batch time: 0.60, accuracy:  11.00%\n",
      "Epoch [6/10], Step [394/600], Loss: 2.2943, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [6/10], Step [395/600], Loss: 2.2895, batch time: 0.59, accuracy:  14.00%\n",
      "Epoch [6/10], Step [396/600], Loss: 2.3223, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [6/10], Step [397/600], Loss: 2.3095, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [6/10], Step [398/600], Loss: 2.2872, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [6/10], Step [399/600], Loss: 2.2915, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [6/10], Step [400/600], Loss: 2.3186, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [6/10], Step [401/600], Loss: 2.2967, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [6/10], Step [402/600], Loss: 2.2760, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [6/10], Step [403/600], Loss: 2.2876, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [6/10], Step [404/600], Loss: 2.3081, batch time: 0.56, accuracy:  6.00%\n",
      "Epoch [6/10], Step [405/600], Loss: 2.3015, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [6/10], Step [406/600], Loss: 2.2990, batch time: 0.61, accuracy:  14.00%\n",
      "Epoch [6/10], Step [407/600], Loss: 2.2644, batch time: 0.59, accuracy:  16.00%\n",
      "Epoch [6/10], Step [408/600], Loss: 2.3030, batch time: 0.66, accuracy:  11.00%\n",
      "Epoch [6/10], Step [409/600], Loss: 2.2695, batch time: 0.67, accuracy:  14.00%\n",
      "Epoch [6/10], Step [410/600], Loss: 2.2943, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [6/10], Step [411/600], Loss: 2.2869, batch time: 0.69, accuracy:  7.00%\n",
      "Epoch [6/10], Step [412/600], Loss: 2.2880, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [6/10], Step [413/600], Loss: 2.3140, batch time: 0.67, accuracy:  6.00%\n",
      "Epoch [6/10], Step [414/600], Loss: 2.3298, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [6/10], Step [415/600], Loss: 2.3014, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [6/10], Step [416/600], Loss: 2.3069, batch time: 0.58, accuracy:  5.00%\n",
      "Epoch [6/10], Step [417/600], Loss: 2.2749, batch time: 0.59, accuracy:  16.00%\n",
      "Epoch [6/10], Step [418/600], Loss: 2.2906, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [6/10], Step [419/600], Loss: 2.2900, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [6/10], Step [420/600], Loss: 2.2899, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [6/10], Step [421/600], Loss: 2.2827, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [6/10], Step [422/600], Loss: 2.2908, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [6/10], Step [423/600], Loss: 2.2959, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [6/10], Step [424/600], Loss: 2.2843, batch time: 0.70, accuracy:  13.00%\n",
      "Epoch [6/10], Step [425/600], Loss: 2.3035, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [6/10], Step [426/600], Loss: 2.2698, batch time: 0.69, accuracy:  14.00%\n",
      "Epoch [6/10], Step [427/600], Loss: 2.2744, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [6/10], Step [428/600], Loss: 2.2794, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [6/10], Step [429/600], Loss: 2.2821, batch time: 0.70, accuracy:  13.00%\n",
      "Epoch [6/10], Step [430/600], Loss: 2.2917, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [6/10], Step [431/600], Loss: 2.2679, batch time: 0.57, accuracy:  17.00%\n",
      "Epoch [6/10], Step [432/600], Loss: 2.2829, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [6/10], Step [433/600], Loss: 2.2681, batch time: 0.59, accuracy:  15.00%\n",
      "Epoch [6/10], Step [434/600], Loss: 2.2877, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [6/10], Step [435/600], Loss: 2.2725, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [6/10], Step [436/600], Loss: 2.3037, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [6/10], Step [437/600], Loss: 2.2751, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [6/10], Step [438/600], Loss: 2.2937, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [6/10], Step [439/600], Loss: 2.2571, batch time: 0.57, accuracy:  20.00%\n",
      "Epoch [6/10], Step [440/600], Loss: 2.3025, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [6/10], Step [441/600], Loss: 2.2907, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [6/10], Step [442/600], Loss: 2.3076, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [6/10], Step [443/600], Loss: 2.2905, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [6/10], Step [444/600], Loss: 2.2665, batch time: 0.69, accuracy:  14.00%\n",
      "Epoch [6/10], Step [445/600], Loss: 2.3068, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [6/10], Step [446/600], Loss: 2.3110, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [6/10], Step [447/600], Loss: 2.3038, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [6/10], Step [448/600], Loss: 2.3047, batch time: 0.83, accuracy:  7.00%\n",
      "Epoch [6/10], Step [449/600], Loss: 2.3126, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [6/10], Step [450/600], Loss: 2.2836, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [6/10], Step [451/600], Loss: 2.2944, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [6/10], Step [452/600], Loss: 2.2794, batch time: 0.72, accuracy:  11.00%\n",
      "Epoch [6/10], Step [453/600], Loss: 2.2994, batch time: 0.70, accuracy:  7.00%\n",
      "Epoch [6/10], Step [454/600], Loss: 2.2946, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [6/10], Step [455/600], Loss: 2.2822, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [6/10], Step [456/600], Loss: 2.3190, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [6/10], Step [457/600], Loss: 2.2895, batch time: 0.60, accuracy:  17.00%\n",
      "Epoch [6/10], Step [458/600], Loss: 2.2903, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [6/10], Step [459/600], Loss: 2.2861, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [6/10], Step [460/600], Loss: 2.3060, batch time: 0.57, accuracy:  2.00%\n",
      "Epoch [6/10], Step [461/600], Loss: 2.2854, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [6/10], Step [462/600], Loss: 2.3136, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [6/10], Step [463/600], Loss: 2.2765, batch time: 0.69, accuracy:  20.00%\n",
      "Epoch [6/10], Step [464/600], Loss: 2.2617, batch time: 0.70, accuracy:  18.00%\n",
      "Epoch [6/10], Step [465/600], Loss: 2.2875, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [6/10], Step [466/600], Loss: 2.3178, batch time: 0.70, accuracy:  4.00%\n",
      "Epoch [6/10], Step [467/600], Loss: 2.2972, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [6/10], Step [468/600], Loss: 2.2960, batch time: 0.68, accuracy:  15.00%\n",
      "Epoch [6/10], Step [469/600], Loss: 2.2888, batch time: 0.70, accuracy:  15.00%\n",
      "Epoch [6/10], Step [470/600], Loss: 2.3126, batch time: 0.70, accuracy:  5.00%\n",
      "Epoch [6/10], Step [471/600], Loss: 2.2828, batch time: 0.69, accuracy:  15.00%\n",
      "Epoch [6/10], Step [472/600], Loss: 2.3007, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [6/10], Step [473/600], Loss: 2.2855, batch time: 0.61, accuracy:  13.00%\n",
      "Epoch [6/10], Step [474/600], Loss: 2.2819, batch time: 0.60, accuracy:  17.00%\n",
      "Epoch [6/10], Step [475/600], Loss: 2.3086, batch time: 0.59, accuracy:  8.00%\n",
      "Epoch [6/10], Step [476/600], Loss: 2.2990, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [6/10], Step [477/600], Loss: 2.2998, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [6/10], Step [478/600], Loss: 2.2589, batch time: 0.69, accuracy:  19.00%\n",
      "Epoch [6/10], Step [479/600], Loss: 2.2931, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [6/10], Step [480/600], Loss: 2.3031, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [6/10], Step [481/600], Loss: 2.2931, batch time: 0.70, accuracy:  12.00%\n",
      "Epoch [6/10], Step [482/600], Loss: 2.2774, batch time: 0.60, accuracy:  18.00%\n",
      "Epoch [6/10], Step [483/600], Loss: 2.3239, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [6/10], Step [484/600], Loss: 2.2878, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [6/10], Step [485/600], Loss: 2.2979, batch time: 0.59, accuracy:  12.00%\n",
      "Epoch [6/10], Step [486/600], Loss: 2.3150, batch time: 0.58, accuracy:  5.00%\n",
      "Epoch [6/10], Step [487/600], Loss: 2.2890, batch time: 0.59, accuracy:  15.00%\n",
      "Epoch [6/10], Step [488/600], Loss: 2.2947, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [6/10], Step [489/600], Loss: 2.2982, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [6/10], Step [490/600], Loss: 2.2951, batch time: 0.59, accuracy:  12.00%\n",
      "Epoch [6/10], Step [491/600], Loss: 2.2967, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [6/10], Step [492/600], Loss: 2.3043, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [6/10], Step [493/600], Loss: 2.3018, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [6/10], Step [494/600], Loss: 2.2775, batch time: 0.58, accuracy:  16.00%\n",
      "Epoch [6/10], Step [495/600], Loss: 2.2956, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [6/10], Step [496/600], Loss: 2.3201, batch time: 0.59, accuracy:  9.00%\n",
      "Epoch [6/10], Step [497/600], Loss: 2.2737, batch time: 0.58, accuracy:  15.00%\n",
      "Epoch [6/10], Step [498/600], Loss: 2.2933, batch time: 0.62, accuracy:  16.00%\n",
      "Epoch [6/10], Step [499/600], Loss: 2.3005, batch time: 0.63, accuracy:  12.00%\n",
      "Epoch [6/10], Step [500/600], Loss: 2.3037, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [6/10], Step [501/600], Loss: 2.2845, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [6/10], Step [502/600], Loss: 2.3118, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [6/10], Step [503/600], Loss: 2.3141, batch time: 0.59, accuracy:  4.00%\n",
      "Epoch [6/10], Step [504/600], Loss: 2.2906, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [6/10], Step [505/600], Loss: 2.2602, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [6/10], Step [506/600], Loss: 2.2924, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [6/10], Step [507/600], Loss: 2.2785, batch time: 0.60, accuracy:  12.00%\n",
      "Epoch [6/10], Step [508/600], Loss: 2.2845, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [6/10], Step [509/600], Loss: 2.2954, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [6/10], Step [510/600], Loss: 2.2995, batch time: 0.63, accuracy:  10.00%\n",
      "Epoch [6/10], Step [511/600], Loss: 2.2940, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [6/10], Step [512/600], Loss: 2.2912, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [6/10], Step [513/600], Loss: 2.2965, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [6/10], Step [514/600], Loss: 2.3018, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [6/10], Step [515/600], Loss: 2.2970, batch time: 0.58, accuracy:  15.00%\n",
      "Epoch [6/10], Step [516/600], Loss: 2.2831, batch time: 0.58, accuracy:  15.00%\n",
      "Epoch [6/10], Step [517/600], Loss: 2.3201, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [6/10], Step [518/600], Loss: 2.2620, batch time: 0.59, accuracy:  17.00%\n",
      "Epoch [6/10], Step [519/600], Loss: 2.3058, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [6/10], Step [520/600], Loss: 2.2768, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [6/10], Step [521/600], Loss: 2.2890, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [6/10], Step [522/600], Loss: 2.2795, batch time: 0.62, accuracy:  16.00%\n",
      "Epoch [6/10], Step [523/600], Loss: 2.3013, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [6/10], Step [524/600], Loss: 2.3057, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [6/10], Step [525/600], Loss: 2.2775, batch time: 0.57, accuracy:  18.00%\n",
      "Epoch [6/10], Step [526/600], Loss: 2.2849, batch time: 0.72, accuracy:  10.00%\n",
      "Epoch [6/10], Step [527/600], Loss: 2.2881, batch time: 0.65, accuracy:  13.00%\n",
      "Epoch [6/10], Step [528/600], Loss: 2.2792, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [6/10], Step [529/600], Loss: 2.3248, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [6/10], Step [530/600], Loss: 2.3048, batch time: 0.69, accuracy:  6.00%\n",
      "Epoch [6/10], Step [531/600], Loss: 2.2697, batch time: 0.67, accuracy:  17.00%\n",
      "Epoch [6/10], Step [532/600], Loss: 2.3098, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [6/10], Step [533/600], Loss: 2.2934, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [6/10], Step [534/600], Loss: 2.2943, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [6/10], Step [535/600], Loss: 2.3245, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [6/10], Step [536/600], Loss: 2.2917, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [6/10], Step [537/600], Loss: 2.3085, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [6/10], Step [538/600], Loss: 2.3069, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [6/10], Step [539/600], Loss: 2.3053, batch time: 0.56, accuracy:  6.00%\n",
      "Epoch [6/10], Step [540/600], Loss: 2.3183, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [6/10], Step [541/600], Loss: 2.2727, batch time: 0.56, accuracy:  15.00%\n",
      "Epoch [6/10], Step [542/600], Loss: 2.2813, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [6/10], Step [543/600], Loss: 2.3223, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [6/10], Step [544/600], Loss: 2.2921, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [6/10], Step [545/600], Loss: 2.2966, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [6/10], Step [546/600], Loss: 2.2959, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [6/10], Step [547/600], Loss: 2.2858, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [6/10], Step [548/600], Loss: 2.2945, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [6/10], Step [549/600], Loss: 2.2747, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [6/10], Step [550/600], Loss: 2.2910, batch time: 0.65, accuracy:  13.00%\n",
      "Epoch [6/10], Step [551/600], Loss: 2.2853, batch time: 0.67, accuracy:  8.00%\n",
      "Epoch [6/10], Step [552/600], Loss: 2.3005, batch time: 0.69, accuracy:  7.00%\n",
      "Epoch [6/10], Step [553/600], Loss: 2.2854, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [6/10], Step [554/600], Loss: 2.2857, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [6/10], Step [555/600], Loss: 2.2902, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [6/10], Step [556/600], Loss: 2.2982, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [6/10], Step [557/600], Loss: 2.2823, batch time: 0.68, accuracy:  16.00%\n",
      "Epoch [6/10], Step [558/600], Loss: 2.2963, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [6/10], Step [559/600], Loss: 2.2901, batch time: 0.67, accuracy:  14.00%\n",
      "Epoch [6/10], Step [560/600], Loss: 2.3012, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [6/10], Step [561/600], Loss: 2.3008, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [6/10], Step [562/600], Loss: 2.2854, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [6/10], Step [563/600], Loss: 2.2930, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [6/10], Step [564/600], Loss: 2.2879, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [6/10], Step [565/600], Loss: 2.2980, batch time: 0.61, accuracy:  9.00%\n",
      "Epoch [6/10], Step [566/600], Loss: 2.3032, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [6/10], Step [567/600], Loss: 2.2938, batch time: 0.59, accuracy:  12.00%\n",
      "Epoch [6/10], Step [568/600], Loss: 2.2922, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [6/10], Step [569/600], Loss: 2.3252, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [6/10], Step [570/600], Loss: 2.2708, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [6/10], Step [571/600], Loss: 2.2828, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [6/10], Step [572/600], Loss: 2.3105, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [6/10], Step [573/600], Loss: 2.3048, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [6/10], Step [574/600], Loss: 2.2905, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [6/10], Step [575/600], Loss: 2.2891, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [6/10], Step [576/600], Loss: 2.3160, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [6/10], Step [577/600], Loss: 2.2993, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [6/10], Step [578/600], Loss: 2.2803, batch time: 0.64, accuracy:  11.00%\n",
      "Epoch [6/10], Step [579/600], Loss: 2.3021, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [6/10], Step [580/600], Loss: 2.2960, batch time: 0.69, accuracy:  7.00%\n",
      "Epoch [6/10], Step [581/600], Loss: 2.2894, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [6/10], Step [582/600], Loss: 2.3019, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [6/10], Step [583/600], Loss: 2.3076, batch time: 0.70, accuracy:  10.00%\n",
      "Epoch [6/10], Step [584/600], Loss: 2.3208, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [6/10], Step [585/600], Loss: 2.2984, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [6/10], Step [586/600], Loss: 2.3059, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [6/10], Step [587/600], Loss: 2.3220, batch time: 0.68, accuracy:  4.00%\n",
      "Epoch [6/10], Step [588/600], Loss: 2.2879, batch time: 0.70, accuracy:  11.00%\n",
      "Epoch [6/10], Step [589/600], Loss: 2.2910, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [6/10], Step [590/600], Loss: 2.2908, batch time: 0.68, accuracy:  16.00%\n",
      "Epoch [6/10], Step [591/600], Loss: 2.2933, batch time: 0.68, accuracy:  16.00%\n",
      "Epoch [6/10], Step [592/600], Loss: 2.2905, batch time: 0.69, accuracy:  7.00%\n",
      "Epoch [6/10], Step [593/600], Loss: 2.2856, batch time: 0.69, accuracy:  15.00%\n",
      "Epoch [6/10], Step [594/600], Loss: 2.2836, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [6/10], Step [595/600], Loss: 2.2950, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [6/10], Step [596/600], Loss: 2.2992, batch time: 0.67, accuracy:  14.00%\n",
      "Epoch [6/10], Step [597/600], Loss: 2.3036, batch time: 0.59, accuracy:  9.00%\n",
      "Epoch [6/10], Step [598/600], Loss: 2.2779, batch time: 0.58, accuracy:  18.00%\n",
      "Epoch [6/10], Step [599/600], Loss: 2.2916, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [6/10], Step [600/600], Loss: 2.3061, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [7/10], Step [1/600], Loss: 2.2645, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [7/10], Step [2/600], Loss: 2.2773, batch time: 0.57, accuracy:  18.00%\n",
      "Epoch [7/10], Step [3/600], Loss: 2.3134, batch time: 0.59, accuracy:  7.00%\n",
      "Epoch [7/10], Step [4/600], Loss: 2.3055, batch time: 0.83, accuracy:  8.00%\n",
      "Epoch [7/10], Step [5/600], Loss: 2.3071, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [7/10], Step [6/600], Loss: 2.3032, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [7/600], Loss: 2.2931, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [8/600], Loss: 2.2938, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [7/10], Step [9/600], Loss: 2.2995, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [7/10], Step [10/600], Loss: 2.3038, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [7/10], Step [11/600], Loss: 2.3148, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [7/10], Step [12/600], Loss: 2.3161, batch time: 0.58, accuracy:  5.00%\n",
      "Epoch [7/10], Step [13/600], Loss: 2.2831, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [7/10], Step [14/600], Loss: 2.2914, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [7/10], Step [15/600], Loss: 2.2982, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [16/600], Loss: 2.2924, batch time: 0.59, accuracy:  9.00%\n",
      "Epoch [7/10], Step [17/600], Loss: 2.2827, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [7/10], Step [18/600], Loss: 2.3012, batch time: 0.63, accuracy:  10.00%\n",
      "Epoch [7/10], Step [19/600], Loss: 2.2989, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [7/10], Step [20/600], Loss: 2.2728, batch time: 0.69, accuracy:  15.00%\n",
      "Epoch [7/10], Step [21/600], Loss: 2.2881, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [7/10], Step [22/600], Loss: 2.3076, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [7/10], Step [23/600], Loss: 2.2882, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [7/10], Step [24/600], Loss: 2.2679, batch time: 0.58, accuracy:  18.00%\n",
      "Epoch [7/10], Step [25/600], Loss: 2.2980, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [26/600], Loss: 2.3117, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [7/10], Step [27/600], Loss: 2.2883, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [28/600], Loss: 2.2992, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [7/10], Step [29/600], Loss: 2.3091, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [7/10], Step [30/600], Loss: 2.2995, batch time: 0.65, accuracy:  12.00%\n",
      "Epoch [7/10], Step [31/600], Loss: 2.2831, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [7/10], Step [32/600], Loss: 2.3006, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [7/10], Step [33/600], Loss: 2.3014, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [7/10], Step [34/600], Loss: 2.2941, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [7/10], Step [35/600], Loss: 2.3161, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [7/10], Step [36/600], Loss: 2.2795, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [37/600], Loss: 2.2917, batch time: 0.63, accuracy:  12.00%\n",
      "Epoch [7/10], Step [38/600], Loss: 2.2692, batch time: 0.57, accuracy:  17.00%\n",
      "Epoch [7/10], Step [39/600], Loss: 2.3025, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [7/10], Step [40/600], Loss: 2.3135, batch time: 0.60, accuracy:  9.00%\n",
      "Epoch [7/10], Step [41/600], Loss: 2.2901, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [7/10], Step [42/600], Loss: 2.3046, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [43/600], Loss: 2.2935, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [7/10], Step [44/600], Loss: 2.3108, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [7/10], Step [45/600], Loss: 2.2811, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [7/10], Step [46/600], Loss: 2.3119, batch time: 0.59, accuracy:  5.00%\n",
      "Epoch [7/10], Step [47/600], Loss: 2.2993, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [7/10], Step [48/600], Loss: 2.2879, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [7/10], Step [49/600], Loss: 2.2969, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [7/10], Step [50/600], Loss: 2.2981, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [51/600], Loss: 2.2948, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [7/10], Step [52/600], Loss: 2.2880, batch time: 0.62, accuracy:  13.00%\n",
      "Epoch [7/10], Step [53/600], Loss: 2.3110, batch time: 0.57, accuracy:  5.00%\n",
      "Epoch [7/10], Step [54/600], Loss: 2.3017, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [7/10], Step [55/600], Loss: 2.2861, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [56/600], Loss: 2.3031, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [7/10], Step [57/600], Loss: 2.2678, batch time: 0.65, accuracy:  17.00%\n",
      "Epoch [7/10], Step [58/600], Loss: 2.2821, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [7/10], Step [59/600], Loss: 2.2849, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [7/10], Step [60/600], Loss: 2.3127, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [7/10], Step [61/600], Loss: 2.2876, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [7/10], Step [62/600], Loss: 2.2760, batch time: 0.69, accuracy:  14.00%\n",
      "Epoch [7/10], Step [63/600], Loss: 2.2844, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [7/10], Step [64/600], Loss: 2.2914, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [7/10], Step [65/600], Loss: 2.2773, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [7/10], Step [66/600], Loss: 2.3084, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [7/10], Step [67/600], Loss: 2.3125, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [7/10], Step [68/600], Loss: 2.2844, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [7/10], Step [69/600], Loss: 2.2790, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [7/10], Step [70/600], Loss: 2.3044, batch time: 0.57, accuracy:  4.00%\n",
      "Epoch [7/10], Step [71/600], Loss: 2.2603, batch time: 0.58, accuracy:  17.00%\n",
      "Epoch [7/10], Step [72/600], Loss: 2.2887, batch time: 0.61, accuracy:  13.00%\n",
      "Epoch [7/10], Step [73/600], Loss: 2.2941, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [7/10], Step [74/600], Loss: 2.2907, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [7/10], Step [75/600], Loss: 2.2672, batch time: 0.68, accuracy:  19.00%\n",
      "Epoch [7/10], Step [76/600], Loss: 2.2950, batch time: 0.90, accuracy:  12.00%\n",
      "Epoch [7/10], Step [77/600], Loss: 2.2933, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [7/10], Step [78/600], Loss: 2.3137, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [7/10], Step [79/600], Loss: 2.2823, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [80/600], Loss: 2.2872, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [7/10], Step [81/600], Loss: 2.3009, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [7/10], Step [82/600], Loss: 2.2860, batch time: 0.81, accuracy:  13.00%\n",
      "Epoch [7/10], Step [83/600], Loss: 2.3072, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [7/10], Step [84/600], Loss: 2.2892, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [7/10], Step [85/600], Loss: 2.2865, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [7/10], Step [86/600], Loss: 2.2882, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [7/10], Step [87/600], Loss: 2.2956, batch time: 0.60, accuracy:  11.00%\n",
      "Epoch [7/10], Step [88/600], Loss: 2.2790, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [7/10], Step [89/600], Loss: 2.3120, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [90/600], Loss: 2.2881, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [7/10], Step [91/600], Loss: 2.2674, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [7/10], Step [92/600], Loss: 2.2777, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [7/10], Step [93/600], Loss: 2.3096, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [7/10], Step [94/600], Loss: 2.2888, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [7/10], Step [95/600], Loss: 2.2918, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [7/10], Step [96/600], Loss: 2.2735, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [97/600], Loss: 2.2735, batch time: 0.62, accuracy:  17.00%\n",
      "Epoch [7/10], Step [98/600], Loss: 2.2778, batch time: 0.58, accuracy:  17.00%\n",
      "Epoch [7/10], Step [99/600], Loss: 2.2901, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [100/600], Loss: 2.3015, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [7/10], Step [101/600], Loss: 2.2791, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [7/10], Step [102/600], Loss: 2.2876, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [7/10], Step [103/600], Loss: 2.2840, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [7/10], Step [104/600], Loss: 2.3040, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [7/10], Step [105/600], Loss: 2.2962, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [7/10], Step [106/600], Loss: 2.3126, batch time: 0.61, accuracy:  7.00%\n",
      "Epoch [7/10], Step [107/600], Loss: 2.3042, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [7/10], Step [108/600], Loss: 2.3063, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [7/10], Step [109/600], Loss: 2.2767, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [7/10], Step [110/600], Loss: 2.2803, batch time: 0.58, accuracy:  18.00%\n",
      "Epoch [7/10], Step [111/600], Loss: 2.2693, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [7/10], Step [112/600], Loss: 2.2756, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [7/10], Step [113/600], Loss: 2.2976, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [114/600], Loss: 2.2882, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [7/10], Step [115/600], Loss: 2.2838, batch time: 0.56, accuracy:  14.00%\n",
      "Epoch [7/10], Step [116/600], Loss: 2.2515, batch time: 0.57, accuracy:  20.00%\n",
      "Epoch [7/10], Step [117/600], Loss: 2.2831, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [7/10], Step [118/600], Loss: 2.2871, batch time: 0.65, accuracy:  10.00%\n",
      "Epoch [7/10], Step [119/600], Loss: 2.2815, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [7/10], Step [120/600], Loss: 2.2896, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [7/10], Step [121/600], Loss: 2.2807, batch time: 0.69, accuracy:  16.00%\n",
      "Epoch [7/10], Step [122/600], Loss: 2.2905, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [7/10], Step [123/600], Loss: 2.2894, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [7/10], Step [124/600], Loss: 2.2933, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [7/10], Step [125/600], Loss: 2.2822, batch time: 0.69, accuracy:  15.00%\n",
      "Epoch [7/10], Step [126/600], Loss: 2.3125, batch time: 0.70, accuracy:  11.00%\n",
      "Epoch [7/10], Step [127/600], Loss: 2.2928, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [7/10], Step [128/600], Loss: 2.2967, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [7/10], Step [129/600], Loss: 2.2807, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [7/10], Step [130/600], Loss: 2.2935, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [7/10], Step [131/600], Loss: 2.3178, batch time: 0.69, accuracy:  7.00%\n",
      "Epoch [7/10], Step [132/600], Loss: 2.2737, batch time: 0.68, accuracy:  18.00%\n",
      "Epoch [7/10], Step [133/600], Loss: 2.2504, batch time: 0.67, accuracy:  20.00%\n",
      "Epoch [7/10], Step [134/600], Loss: 2.2920, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [7/10], Step [135/600], Loss: 2.3005, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [7/10], Step [136/600], Loss: 2.3074, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [7/10], Step [137/600], Loss: 2.3090, batch time: 0.67, accuracy:  6.00%\n",
      "Epoch [7/10], Step [138/600], Loss: 2.2772, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [7/10], Step [139/600], Loss: 2.2580, batch time: 0.68, accuracy:  15.00%\n",
      "Epoch [7/10], Step [140/600], Loss: 2.2758, batch time: 0.69, accuracy:  16.00%\n",
      "Epoch [7/10], Step [141/600], Loss: 2.2672, batch time: 0.69, accuracy:  19.00%\n",
      "Epoch [7/10], Step [142/600], Loss: 2.2871, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [7/10], Step [143/600], Loss: 2.2969, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [7/10], Step [144/600], Loss: 2.2893, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [7/10], Step [145/600], Loss: 2.2947, batch time: 0.69, accuracy:  7.00%\n",
      "Epoch [7/10], Step [146/600], Loss: 2.2878, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [7/10], Step [147/600], Loss: 2.2793, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [7/10], Step [148/600], Loss: 2.3075, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [7/10], Step [149/600], Loss: 2.2885, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [7/10], Step [150/600], Loss: 2.2893, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [7/10], Step [151/600], Loss: 2.2960, batch time: 0.69, accuracy:  6.00%\n",
      "Epoch [7/10], Step [152/600], Loss: 2.3057, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [7/10], Step [153/600], Loss: 2.3106, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [7/10], Step [154/600], Loss: 2.2835, batch time: 0.70, accuracy:  12.00%\n",
      "Epoch [7/10], Step [155/600], Loss: 2.3046, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [7/10], Step [156/600], Loss: 2.3124, batch time: 0.69, accuracy:  6.00%\n",
      "Epoch [7/10], Step [157/600], Loss: 2.2833, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [7/10], Step [158/600], Loss: 2.2823, batch time: 0.66, accuracy:  12.00%\n",
      "Epoch [7/10], Step [159/600], Loss: 2.2672, batch time: 0.56, accuracy:  18.00%\n",
      "Epoch [7/10], Step [160/600], Loss: 2.3134, batch time: 0.71, accuracy:  9.00%\n",
      "Epoch [7/10], Step [161/600], Loss: 2.2999, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [7/10], Step [162/600], Loss: 2.2972, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [7/10], Step [163/600], Loss: 2.3101, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [7/10], Step [164/600], Loss: 2.2845, batch time: 0.62, accuracy:  14.00%\n",
      "Epoch [7/10], Step [165/600], Loss: 2.2749, batch time: 0.59, accuracy:  6.00%\n",
      "Epoch [7/10], Step [166/600], Loss: 2.2947, batch time: 0.60, accuracy:  11.00%\n",
      "Epoch [7/10], Step [167/600], Loss: 2.3037, batch time: 0.65, accuracy:  11.00%\n",
      "Epoch [7/10], Step [168/600], Loss: 2.2767, batch time: 0.58, accuracy:  16.00%\n",
      "Epoch [7/10], Step [169/600], Loss: 2.2603, batch time: 0.58, accuracy:  19.00%\n",
      "Epoch [7/10], Step [170/600], Loss: 2.3002, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [7/10], Step [171/600], Loss: 2.2812, batch time: 0.58, accuracy:  15.00%\n",
      "Epoch [7/10], Step [172/600], Loss: 2.2908, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [7/10], Step [173/600], Loss: 2.2826, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [7/10], Step [174/600], Loss: 2.2849, batch time: 0.56, accuracy:  17.00%\n",
      "Epoch [7/10], Step [175/600], Loss: 2.2837, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [7/10], Step [176/600], Loss: 2.2917, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [177/600], Loss: 2.2898, batch time: 0.64, accuracy:  10.00%\n",
      "Epoch [7/10], Step [178/600], Loss: 2.3067, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [7/10], Step [179/600], Loss: 2.2885, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [7/10], Step [180/600], Loss: 2.2826, batch time: 0.65, accuracy:  11.00%\n",
      "Epoch [7/10], Step [181/600], Loss: 2.3154, batch time: 0.59, accuracy:  6.00%\n",
      "Epoch [7/10], Step [182/600], Loss: 2.3042, batch time: 0.56, accuracy:  6.00%\n",
      "Epoch [7/10], Step [183/600], Loss: 2.2894, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [7/10], Step [184/600], Loss: 2.2895, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [7/10], Step [185/600], Loss: 2.3085, batch time: 0.56, accuracy:  7.00%\n",
      "Epoch [7/10], Step [186/600], Loss: 2.2825, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [7/10], Step [187/600], Loss: 2.3181, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [7/10], Step [188/600], Loss: 2.2911, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [7/10], Step [189/600], Loss: 2.3039, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [7/10], Step [190/600], Loss: 2.3014, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [7/10], Step [191/600], Loss: 2.2626, batch time: 0.70, accuracy:  20.00%\n",
      "Epoch [7/10], Step [192/600], Loss: 2.2874, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [7/10], Step [193/600], Loss: 2.2861, batch time: 0.61, accuracy:  13.00%\n",
      "Epoch [7/10], Step [194/600], Loss: 2.3025, batch time: 0.65, accuracy:  11.00%\n",
      "Epoch [7/10], Step [195/600], Loss: 2.3113, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [7/10], Step [196/600], Loss: 2.2959, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [7/10], Step [197/600], Loss: 2.3243, batch time: 0.59, accuracy:  6.00%\n",
      "Epoch [7/10], Step [198/600], Loss: 2.3199, batch time: 0.56, accuracy:  5.00%\n",
      "Epoch [7/10], Step [199/600], Loss: 2.3013, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [7/10], Step [200/600], Loss: 2.2833, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [201/600], Loss: 2.3014, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [7/10], Step [202/600], Loss: 2.2926, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [7/10], Step [203/600], Loss: 2.2908, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [7/10], Step [204/600], Loss: 2.2845, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [7/10], Step [205/600], Loss: 2.2696, batch time: 0.62, accuracy:  13.00%\n",
      "Epoch [7/10], Step [206/600], Loss: 2.3179, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [7/10], Step [207/600], Loss: 2.2955, batch time: 0.56, accuracy:  15.00%\n",
      "Epoch [7/10], Step [208/600], Loss: 2.3065, batch time: 0.61, accuracy:  8.00%\n",
      "Epoch [7/10], Step [209/600], Loss: 2.2945, batch time: 0.59, accuracy:  9.00%\n",
      "Epoch [7/10], Step [210/600], Loss: 2.2769, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [7/10], Step [211/600], Loss: 2.2736, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [7/10], Step [212/600], Loss: 2.2870, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [7/10], Step [213/600], Loss: 2.3080, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [7/10], Step [214/600], Loss: 2.2925, batch time: 0.63, accuracy:  12.00%\n",
      "Epoch [7/10], Step [215/600], Loss: 2.2777, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [7/10], Step [216/600], Loss: 2.2761, batch time: 0.58, accuracy:  15.00%\n",
      "Epoch [7/10], Step [217/600], Loss: 2.3097, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [7/10], Step [218/600], Loss: 2.2909, batch time: 0.58, accuracy:  16.00%\n",
      "Epoch [7/10], Step [219/600], Loss: 2.2930, batch time: 0.59, accuracy:  9.00%\n",
      "Epoch [7/10], Step [220/600], Loss: 2.2794, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [7/10], Step [221/600], Loss: 2.2832, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [7/10], Step [222/600], Loss: 2.3207, batch time: 0.58, accuracy:  4.00%\n",
      "Epoch [7/10], Step [223/600], Loss: 2.2848, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [7/10], Step [224/600], Loss: 2.2906, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [7/10], Step [225/600], Loss: 2.3122, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [7/10], Step [226/600], Loss: 2.2888, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [7/10], Step [227/600], Loss: 2.3044, batch time: 0.63, accuracy:  11.00%\n",
      "Epoch [7/10], Step [228/600], Loss: 2.2882, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [7/10], Step [229/600], Loss: 2.2938, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [230/600], Loss: 2.2815, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [7/10], Step [231/600], Loss: 2.3073, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [7/10], Step [232/600], Loss: 2.3052, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [7/10], Step [233/600], Loss: 2.2967, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [7/10], Step [234/600], Loss: 2.2876, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [7/10], Step [235/600], Loss: 2.2984, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [7/10], Step [236/600], Loss: 2.2910, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [7/10], Step [237/600], Loss: 2.2932, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [7/10], Step [238/600], Loss: 2.2730, batch time: 0.81, accuracy:  13.00%\n",
      "Epoch [7/10], Step [239/600], Loss: 2.2818, batch time: 0.70, accuracy:  10.00%\n",
      "Epoch [7/10], Step [240/600], Loss: 2.2812, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [7/10], Step [241/600], Loss: 2.3013, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [7/10], Step [242/600], Loss: 2.3102, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [7/10], Step [243/600], Loss: 2.2796, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [244/600], Loss: 2.2973, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [7/10], Step [245/600], Loss: 2.2872, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [246/600], Loss: 2.2871, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [247/600], Loss: 2.2768, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [7/10], Step [248/600], Loss: 2.2833, batch time: 0.58, accuracy:  15.00%\n",
      "Epoch [7/10], Step [249/600], Loss: 2.2786, batch time: 0.58, accuracy:  15.00%\n",
      "Epoch [7/10], Step [250/600], Loss: 2.2801, batch time: 0.59, accuracy:  16.00%\n",
      "Epoch [7/10], Step [251/600], Loss: 2.2738, batch time: 0.59, accuracy:  16.00%\n",
      "Epoch [7/10], Step [252/600], Loss: 2.3050, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [253/600], Loss: 2.2758, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [7/10], Step [254/600], Loss: 2.2952, batch time: 0.64, accuracy:  12.00%\n",
      "Epoch [7/10], Step [255/600], Loss: 2.2889, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [7/10], Step [256/600], Loss: 2.2842, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [7/10], Step [257/600], Loss: 2.3048, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [7/10], Step [258/600], Loss: 2.2779, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [7/10], Step [259/600], Loss: 2.2946, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [260/600], Loss: 2.2762, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [261/600], Loss: 2.3034, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [7/10], Step [262/600], Loss: 2.2731, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [7/10], Step [263/600], Loss: 2.2882, batch time: 0.58, accuracy:  16.00%\n",
      "Epoch [7/10], Step [264/600], Loss: 2.2972, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [7/10], Step [265/600], Loss: 2.2845, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [7/10], Step [266/600], Loss: 2.2879, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [7/10], Step [267/600], Loss: 2.3005, batch time: 0.60, accuracy:  8.00%\n",
      "Epoch [7/10], Step [268/600], Loss: 2.2861, batch time: 0.61, accuracy:  16.00%\n",
      "Epoch [7/10], Step [269/600], Loss: 2.2950, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [7/10], Step [270/600], Loss: 2.2635, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [7/10], Step [271/600], Loss: 2.2775, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [7/10], Step [272/600], Loss: 2.3034, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [7/10], Step [273/600], Loss: 2.2963, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [7/10], Step [274/600], Loss: 2.2941, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [7/10], Step [275/600], Loss: 2.2949, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [276/600], Loss: 2.2850, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [7/10], Step [277/600], Loss: 2.2853, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [7/10], Step [278/600], Loss: 2.2870, batch time: 0.63, accuracy:  13.00%\n",
      "Epoch [7/10], Step [279/600], Loss: 2.2782, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [7/10], Step [280/600], Loss: 2.2809, batch time: 0.69, accuracy:  15.00%\n",
      "Epoch [7/10], Step [281/600], Loss: 2.2898, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [7/10], Step [282/600], Loss: 2.2890, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [7/10], Step [283/600], Loss: 2.2647, batch time: 0.70, accuracy:  14.00%\n",
      "Epoch [7/10], Step [284/600], Loss: 2.2681, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [7/10], Step [285/600], Loss: 2.2832, batch time: 0.69, accuracy:  14.00%\n",
      "Epoch [7/10], Step [286/600], Loss: 2.2740, batch time: 0.69, accuracy:  15.00%\n",
      "Epoch [7/10], Step [287/600], Loss: 2.2740, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [7/10], Step [288/600], Loss: 2.3076, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [7/10], Step [289/600], Loss: 2.2938, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [7/10], Step [290/600], Loss: 2.2709, batch time: 0.66, accuracy:  13.00%\n",
      "Epoch [7/10], Step [291/600], Loss: 2.2640, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [7/10], Step [292/600], Loss: 2.3130, batch time: 0.56, accuracy:  7.00%\n",
      "Epoch [7/10], Step [293/600], Loss: 2.3219, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [7/10], Step [294/600], Loss: 2.3258, batch time: 0.56, accuracy:  5.00%\n",
      "Epoch [7/10], Step [295/600], Loss: 2.3097, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [7/10], Step [296/600], Loss: 2.2822, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [7/10], Step [297/600], Loss: 2.2845, batch time: 0.62, accuracy:  8.00%\n",
      "Epoch [7/10], Step [298/600], Loss: 2.2844, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [7/10], Step [299/600], Loss: 2.2799, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [7/10], Step [300/600], Loss: 2.2513, batch time: 0.57, accuracy:  20.00%\n",
      "Epoch [7/10], Step [301/600], Loss: 2.3041, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [7/10], Step [302/600], Loss: 2.2840, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [7/10], Step [303/600], Loss: 2.2974, batch time: 0.63, accuracy:  13.00%\n",
      "Epoch [7/10], Step [304/600], Loss: 2.2908, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [7/10], Step [305/600], Loss: 2.2921, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [7/10], Step [306/600], Loss: 2.2895, batch time: 0.62, accuracy:  13.00%\n",
      "Epoch [7/10], Step [307/600], Loss: 2.2733, batch time: 0.56, accuracy:  16.00%\n",
      "Epoch [7/10], Step [308/600], Loss: 2.2771, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [7/10], Step [309/600], Loss: 2.2991, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [7/10], Step [310/600], Loss: 2.2866, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [7/10], Step [311/600], Loss: 2.2856, batch time: 0.57, accuracy:  17.00%\n",
      "Epoch [7/10], Step [312/600], Loss: 2.3059, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [7/10], Step [313/600], Loss: 2.3016, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [314/600], Loss: 2.3036, batch time: 0.71, accuracy:  10.00%\n",
      "Epoch [7/10], Step [315/600], Loss: 2.2978, batch time: 0.60, accuracy:  6.00%\n",
      "Epoch [7/10], Step [316/600], Loss: 2.2853, batch time: 0.73, accuracy:  10.00%\n",
      "Epoch [7/10], Step [317/600], Loss: 2.2907, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [7/10], Step [318/600], Loss: 2.2994, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [7/10], Step [319/600], Loss: 2.2937, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [7/10], Step [320/600], Loss: 2.2831, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [7/10], Step [321/600], Loss: 2.2860, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [322/600], Loss: 2.2549, batch time: 0.58, accuracy:  21.00%\n",
      "Epoch [7/10], Step [323/600], Loss: 2.2741, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [7/10], Step [324/600], Loss: 2.2819, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [7/10], Step [325/600], Loss: 2.3022, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [326/600], Loss: 2.2902, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [7/10], Step [327/600], Loss: 2.2886, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [7/10], Step [328/600], Loss: 2.2999, batch time: 0.68, accuracy:  15.00%\n",
      "Epoch [7/10], Step [329/600], Loss: 2.2839, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [7/10], Step [330/600], Loss: 2.2938, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [7/10], Step [331/600], Loss: 2.3019, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [7/10], Step [332/600], Loss: 2.2659, batch time: 0.69, accuracy:  16.00%\n",
      "Epoch [7/10], Step [333/600], Loss: 2.3044, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [7/10], Step [334/600], Loss: 2.3004, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [7/10], Step [335/600], Loss: 2.3262, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [7/10], Step [336/600], Loss: 2.2998, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [7/10], Step [337/600], Loss: 2.2839, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [7/10], Step [338/600], Loss: 2.2968, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [7/10], Step [339/600], Loss: 2.2901, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [7/10], Step [340/600], Loss: 2.2594, batch time: 0.67, accuracy:  20.00%\n",
      "Epoch [7/10], Step [341/600], Loss: 2.3029, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [7/10], Step [342/600], Loss: 2.2759, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [7/10], Step [343/600], Loss: 2.3013, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [7/10], Step [344/600], Loss: 2.2888, batch time: 0.63, accuracy:  10.00%\n",
      "Epoch [7/10], Step [345/600], Loss: 2.3025, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [7/10], Step [346/600], Loss: 2.2916, batch time: 0.69, accuracy:  14.00%\n",
      "Epoch [7/10], Step [347/600], Loss: 2.2661, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [7/10], Step [348/600], Loss: 2.3088, batch time: 0.70, accuracy:  9.00%\n",
      "Epoch [7/10], Step [349/600], Loss: 2.2899, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [7/10], Step [350/600], Loss: 2.3216, batch time: 0.68, accuracy:  3.00%\n",
      "Epoch [7/10], Step [351/600], Loss: 2.2872, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [7/10], Step [352/600], Loss: 2.2934, batch time: 0.70, accuracy:  7.00%\n",
      "Epoch [7/10], Step [353/600], Loss: 2.3026, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [7/10], Step [354/600], Loss: 2.2907, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [7/10], Step [355/600], Loss: 2.2872, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [7/10], Step [356/600], Loss: 2.2891, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [7/10], Step [357/600], Loss: 2.2799, batch time: 0.70, accuracy:  13.00%\n",
      "Epoch [7/10], Step [358/600], Loss: 2.2726, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [7/10], Step [359/600], Loss: 2.2971, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [7/10], Step [360/600], Loss: 2.3013, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [7/10], Step [361/600], Loss: 2.2977, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [7/10], Step [362/600], Loss: 2.2829, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [7/10], Step [363/600], Loss: 2.2793, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [7/10], Step [364/600], Loss: 2.3084, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [7/10], Step [365/600], Loss: 2.2667, batch time: 0.69, accuracy:  17.00%\n",
      "Epoch [7/10], Step [366/600], Loss: 2.2617, batch time: 0.70, accuracy:  19.00%\n",
      "Epoch [7/10], Step [367/600], Loss: 2.2837, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [7/10], Step [368/600], Loss: 2.2866, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [7/10], Step [369/600], Loss: 2.2882, batch time: 0.70, accuracy:  10.00%\n",
      "Epoch [7/10], Step [370/600], Loss: 2.2847, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [7/10], Step [371/600], Loss: 2.2960, batch time: 0.71, accuracy:  10.00%\n",
      "Epoch [7/10], Step [372/600], Loss: 2.2709, batch time: 0.60, accuracy:  13.00%\n",
      "Epoch [7/10], Step [373/600], Loss: 2.2985, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [374/600], Loss: 2.2723, batch time: 0.63, accuracy:  15.00%\n",
      "Epoch [7/10], Step [375/600], Loss: 2.2828, batch time: 0.59, accuracy:  14.00%\n",
      "Epoch [7/10], Step [376/600], Loss: 2.2913, batch time: 0.64, accuracy:  12.00%\n",
      "Epoch [7/10], Step [377/600], Loss: 2.2461, batch time: 0.70, accuracy:  13.00%\n",
      "Epoch [7/10], Step [378/600], Loss: 2.2815, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [7/10], Step [379/600], Loss: 2.2935, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [7/10], Step [380/600], Loss: 2.3120, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [7/10], Step [381/600], Loss: 2.3250, batch time: 0.69, accuracy:  6.00%\n",
      "Epoch [7/10], Step [382/600], Loss: 2.3119, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [7/10], Step [383/600], Loss: 2.2969, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [7/10], Step [384/600], Loss: 2.2827, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [7/10], Step [385/600], Loss: 2.2842, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [7/10], Step [386/600], Loss: 2.2813, batch time: 0.70, accuracy:  10.00%\n",
      "Epoch [7/10], Step [387/600], Loss: 2.2896, batch time: 0.70, accuracy:  14.00%\n",
      "Epoch [7/10], Step [388/600], Loss: 2.2757, batch time: 0.70, accuracy:  16.00%\n",
      "Epoch [7/10], Step [389/600], Loss: 2.2738, batch time: 0.70, accuracy:  14.00%\n",
      "Epoch [7/10], Step [390/600], Loss: 2.2606, batch time: 0.70, accuracy:  14.00%\n",
      "Epoch [7/10], Step [391/600], Loss: 2.2857, batch time: 0.71, accuracy:  11.00%\n",
      "Epoch [7/10], Step [392/600], Loss: 2.3214, batch time: 0.68, accuracy:  5.00%\n",
      "Epoch [7/10], Step [393/600], Loss: 2.2722, batch time: 0.63, accuracy:  17.00%\n",
      "Epoch [7/10], Step [394/600], Loss: 2.3117, batch time: 0.78, accuracy:  9.00%\n",
      "Epoch [7/10], Step [395/600], Loss: 2.2997, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [7/10], Step [396/600], Loss: 2.2680, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [7/10], Step [397/600], Loss: 2.2995, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [7/10], Step [398/600], Loss: 2.2780, batch time: 0.58, accuracy:  16.00%\n",
      "Epoch [7/10], Step [399/600], Loss: 2.2886, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [7/10], Step [400/600], Loss: 2.3067, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [7/10], Step [401/600], Loss: 2.2575, batch time: 0.58, accuracy:  20.00%\n",
      "Epoch [7/10], Step [402/600], Loss: 2.3010, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [7/10], Step [403/600], Loss: 2.3220, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [7/10], Step [404/600], Loss: 2.2771, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [7/10], Step [405/600], Loss: 2.2853, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [7/10], Step [406/600], Loss: 2.2723, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [7/10], Step [407/600], Loss: 2.2896, batch time: 0.60, accuracy:  11.00%\n",
      "Epoch [7/10], Step [408/600], Loss: 2.2834, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [7/10], Step [409/600], Loss: 2.2774, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [7/10], Step [410/600], Loss: 2.3163, batch time: 0.67, accuracy:  5.00%\n",
      "Epoch [7/10], Step [411/600], Loss: 2.2619, batch time: 0.67, accuracy:  20.00%\n",
      "Epoch [7/10], Step [412/600], Loss: 2.3208, batch time: 0.69, accuracy:  4.00%\n",
      "Epoch [7/10], Step [413/600], Loss: 2.2781, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [7/10], Step [414/600], Loss: 2.2980, batch time: 0.67, accuracy:  8.00%\n",
      "Epoch [7/10], Step [415/600], Loss: 2.3067, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [7/10], Step [416/600], Loss: 2.3062, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [7/10], Step [417/600], Loss: 2.2695, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [7/10], Step [418/600], Loss: 2.3055, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [7/10], Step [419/600], Loss: 2.2943, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [7/10], Step [420/600], Loss: 2.2903, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [7/10], Step [421/600], Loss: 2.2995, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [7/10], Step [422/600], Loss: 2.2731, batch time: 0.69, accuracy:  16.00%\n",
      "Epoch [7/10], Step [423/600], Loss: 2.2803, batch time: 0.68, accuracy:  15.00%\n",
      "Epoch [7/10], Step [424/600], Loss: 2.3011, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [7/10], Step [425/600], Loss: 2.2927, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [7/10], Step [426/600], Loss: 2.2873, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [7/10], Step [427/600], Loss: 2.2974, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [7/10], Step [428/600], Loss: 2.2876, batch time: 0.68, accuracy:  13.00%\n",
      "Epoch [7/10], Step [429/600], Loss: 2.2975, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [7/10], Step [430/600], Loss: 2.2698, batch time: 0.68, accuracy:  16.00%\n",
      "Epoch [7/10], Step [431/600], Loss: 2.2747, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [7/10], Step [432/600], Loss: 2.2798, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [433/600], Loss: 2.2764, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [7/10], Step [434/600], Loss: 2.2911, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [7/10], Step [435/600], Loss: 2.2843, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [7/10], Step [436/600], Loss: 2.2873, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [7/10], Step [437/600], Loss: 2.2835, batch time: 0.59, accuracy:  7.00%\n",
      "Epoch [7/10], Step [438/600], Loss: 2.2719, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [7/10], Step [439/600], Loss: 2.2575, batch time: 0.59, accuracy:  16.00%\n",
      "Epoch [7/10], Step [440/600], Loss: 2.3100, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [441/600], Loss: 2.2794, batch time: 0.63, accuracy:  12.00%\n",
      "Epoch [7/10], Step [442/600], Loss: 2.3015, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [7/10], Step [443/600], Loss: 2.2908, batch time: 0.70, accuracy:  12.00%\n",
      "Epoch [7/10], Step [444/600], Loss: 2.2998, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [7/10], Step [445/600], Loss: 2.2918, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [7/10], Step [446/600], Loss: 2.2794, batch time: 0.69, accuracy:  15.00%\n",
      "Epoch [7/10], Step [447/600], Loss: 2.2933, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [7/10], Step [448/600], Loss: 2.2900, batch time: 0.69, accuracy:  8.00%\n",
      "Epoch [7/10], Step [449/600], Loss: 2.3103, batch time: 0.69, accuracy:  5.00%\n",
      "Epoch [7/10], Step [450/600], Loss: 2.2641, batch time: 0.60, accuracy:  14.00%\n",
      "Epoch [7/10], Step [451/600], Loss: 2.2987, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [7/10], Step [452/600], Loss: 2.2809, batch time: 0.59, accuracy:  12.00%\n",
      "Epoch [7/10], Step [453/600], Loss: 2.2512, batch time: 0.63, accuracy:  21.00%\n",
      "Epoch [7/10], Step [454/600], Loss: 2.3061, batch time: 0.59, accuracy:  6.00%\n",
      "Epoch [7/10], Step [455/600], Loss: 2.2970, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [456/600], Loss: 2.2722, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [7/10], Step [457/600], Loss: 2.2920, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [7/10], Step [458/600], Loss: 2.2656, batch time: 0.62, accuracy:  14.00%\n",
      "Epoch [7/10], Step [459/600], Loss: 2.2803, batch time: 0.70, accuracy:  13.00%\n",
      "Epoch [7/10], Step [460/600], Loss: 2.2911, batch time: 0.64, accuracy:  12.00%\n",
      "Epoch [7/10], Step [461/600], Loss: 2.2977, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [7/10], Step [462/600], Loss: 2.2866, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [7/10], Step [463/600], Loss: 2.2890, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [7/10], Step [464/600], Loss: 2.2831, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [7/10], Step [465/600], Loss: 2.2790, batch time: 0.63, accuracy:  12.00%\n",
      "Epoch [7/10], Step [466/600], Loss: 2.2907, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [467/600], Loss: 2.2893, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [7/10], Step [468/600], Loss: 2.2599, batch time: 0.63, accuracy:  15.00%\n",
      "Epoch [7/10], Step [469/600], Loss: 2.2999, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [7/10], Step [470/600], Loss: 2.2659, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [7/10], Step [471/600], Loss: 2.2556, batch time: 0.61, accuracy:  16.00%\n",
      "Epoch [7/10], Step [472/600], Loss: 2.3079, batch time: 0.73, accuracy:  7.00%\n",
      "Epoch [7/10], Step [473/600], Loss: 2.2646, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [7/10], Step [474/600], Loss: 2.2751, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [7/10], Step [475/600], Loss: 2.2940, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [7/10], Step [476/600], Loss: 2.3017, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [7/10], Step [477/600], Loss: 2.2965, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [478/600], Loss: 2.2899, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [7/10], Step [479/600], Loss: 2.2872, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [7/10], Step [480/600], Loss: 2.3216, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [7/10], Step [481/600], Loss: 2.2723, batch time: 0.69, accuracy:  11.00%\n",
      "Epoch [7/10], Step [482/600], Loss: 2.3111, batch time: 0.59, accuracy:  7.00%\n",
      "Epoch [7/10], Step [483/600], Loss: 2.3138, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [7/10], Step [484/600], Loss: 2.3006, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [485/600], Loss: 2.2800, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [7/10], Step [486/600], Loss: 2.2969, batch time: 0.56, accuracy:  11.00%\n",
      "Epoch [7/10], Step [487/600], Loss: 2.2670, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [7/10], Step [488/600], Loss: 2.3189, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [7/10], Step [489/600], Loss: 2.2627, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [7/10], Step [490/600], Loss: 2.3097, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [7/10], Step [491/600], Loss: 2.3060, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [492/600], Loss: 2.3020, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [493/600], Loss: 2.2514, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [7/10], Step [494/600], Loss: 2.2913, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [495/600], Loss: 2.2941, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [7/10], Step [496/600], Loss: 2.2997, batch time: 0.62, accuracy:  5.00%\n",
      "Epoch [7/10], Step [497/600], Loss: 2.2770, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [7/10], Step [498/600], Loss: 2.3030, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [7/10], Step [499/600], Loss: 2.2888, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [7/10], Step [500/600], Loss: 2.2985, batch time: 0.56, accuracy:  7.00%\n",
      "Epoch [7/10], Step [501/600], Loss: 2.3286, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [7/10], Step [502/600], Loss: 2.2746, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [7/10], Step [503/600], Loss: 2.2835, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [504/600], Loss: 2.2777, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [7/10], Step [505/600], Loss: 2.3085, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [7/10], Step [506/600], Loss: 2.2831, batch time: 0.66, accuracy:  8.00%\n",
      "Epoch [7/10], Step [507/600], Loss: 2.3029, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [7/10], Step [508/600], Loss: 2.3018, batch time: 0.67, accuracy:  7.00%\n",
      "Epoch [7/10], Step [509/600], Loss: 2.3052, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [7/10], Step [510/600], Loss: 2.2680, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [7/10], Step [511/600], Loss: 2.2940, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [7/10], Step [512/600], Loss: 2.2872, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [7/10], Step [513/600], Loss: 2.2760, batch time: 0.70, accuracy:  16.00%\n",
      "Epoch [7/10], Step [514/600], Loss: 2.2957, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [7/10], Step [515/600], Loss: 2.2819, batch time: 0.62, accuracy:  11.00%\n",
      "Epoch [7/10], Step [516/600], Loss: 2.2858, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [7/10], Step [517/600], Loss: 2.2913, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [7/10], Step [518/600], Loss: 2.2693, batch time: 0.65, accuracy:  11.00%\n",
      "Epoch [7/10], Step [519/600], Loss: 2.2793, batch time: 0.70, accuracy:  15.00%\n",
      "Epoch [7/10], Step [520/600], Loss: 2.2888, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [7/10], Step [521/600], Loss: 2.2926, batch time: 0.66, accuracy:  10.00%\n",
      "Epoch [7/10], Step [522/600], Loss: 2.2775, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [7/10], Step [523/600], Loss: 2.2839, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [524/600], Loss: 2.2876, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [7/10], Step [525/600], Loss: 2.3030, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [7/10], Step [526/600], Loss: 2.2904, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [7/10], Step [527/600], Loss: 2.2819, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [7/10], Step [528/600], Loss: 2.3052, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [7/10], Step [529/600], Loss: 2.3015, batch time: 0.59, accuracy:  8.00%\n",
      "Epoch [7/10], Step [530/600], Loss: 2.2681, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [531/600], Loss: 2.2683, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [7/10], Step [532/600], Loss: 2.2938, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [7/10], Step [533/600], Loss: 2.2938, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [534/600], Loss: 2.2934, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [7/10], Step [535/600], Loss: 2.2985, batch time: 0.60, accuracy:  11.00%\n",
      "Epoch [7/10], Step [536/600], Loss: 2.3091, batch time: 0.62, accuracy:  6.00%\n",
      "Epoch [7/10], Step [537/600], Loss: 2.2745, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [7/10], Step [538/600], Loss: 2.2715, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [539/600], Loss: 2.3013, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [7/10], Step [540/600], Loss: 2.2854, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [7/10], Step [541/600], Loss: 2.2998, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [7/10], Step [542/600], Loss: 2.2916, batch time: 0.62, accuracy:  10.00%\n",
      "Epoch [7/10], Step [543/600], Loss: 2.2914, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [7/10], Step [544/600], Loss: 2.2842, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [7/10], Step [545/600], Loss: 2.2566, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [7/10], Step [546/600], Loss: 2.2639, batch time: 0.59, accuracy:  14.00%\n",
      "Epoch [7/10], Step [547/600], Loss: 2.2909, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [7/10], Step [548/600], Loss: 2.2970, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [7/10], Step [549/600], Loss: 2.2842, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [7/10], Step [550/600], Loss: 2.2754, batch time: 0.71, accuracy:  10.00%\n",
      "Epoch [7/10], Step [551/600], Loss: 2.2889, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [7/10], Step [552/600], Loss: 2.2889, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [7/10], Step [553/600], Loss: 2.2912, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [7/10], Step [554/600], Loss: 2.2956, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [7/10], Step [555/600], Loss: 2.2634, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [556/600], Loss: 2.2823, batch time: 0.63, accuracy:  10.00%\n",
      "Epoch [7/10], Step [557/600], Loss: 2.2809, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [7/10], Step [558/600], Loss: 2.3018, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [7/10], Step [559/600], Loss: 2.2685, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [7/10], Step [560/600], Loss: 2.2656, batch time: 0.61, accuracy:  17.00%\n",
      "Epoch [7/10], Step [561/600], Loss: 2.2737, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [7/10], Step [562/600], Loss: 2.2768, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [7/10], Step [563/600], Loss: 2.2887, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [7/10], Step [564/600], Loss: 2.2918, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [7/10], Step [565/600], Loss: 2.3088, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [7/10], Step [566/600], Loss: 2.2758, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [7/10], Step [567/600], Loss: 2.2963, batch time: 0.65, accuracy:  8.00%\n",
      "Epoch [7/10], Step [568/600], Loss: 2.2766, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [7/10], Step [569/600], Loss: 2.3117, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [7/10], Step [570/600], Loss: 2.2896, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [7/10], Step [571/600], Loss: 2.2794, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [7/10], Step [572/600], Loss: 2.2741, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [7/10], Step [573/600], Loss: 2.2926, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [7/10], Step [574/600], Loss: 2.3091, batch time: 0.57, accuracy:  6.00%\n",
      "Epoch [7/10], Step [575/600], Loss: 2.2837, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [7/10], Step [576/600], Loss: 2.2867, batch time: 0.62, accuracy:  12.00%\n",
      "Epoch [7/10], Step [577/600], Loss: 2.2834, batch time: 0.66, accuracy:  13.00%\n",
      "Epoch [7/10], Step [578/600], Loss: 2.2803, batch time: 0.70, accuracy:  9.00%\n",
      "Epoch [7/10], Step [579/600], Loss: 2.2867, batch time: 0.60, accuracy:  9.00%\n",
      "Epoch [7/10], Step [580/600], Loss: 2.3022, batch time: 0.59, accuracy:  6.00%\n",
      "Epoch [7/10], Step [581/600], Loss: 2.2755, batch time: 0.59, accuracy:  17.00%\n",
      "Epoch [7/10], Step [582/600], Loss: 2.3030, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [7/10], Step [583/600], Loss: 2.2374, batch time: 0.57, accuracy:  19.00%\n",
      "Epoch [7/10], Step [584/600], Loss: 2.2782, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [7/10], Step [585/600], Loss: 2.3065, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [7/10], Step [586/600], Loss: 2.3358, batch time: 0.59, accuracy:  8.00%\n",
      "Epoch [7/10], Step [587/600], Loss: 2.2570, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [7/10], Step [588/600], Loss: 2.2736, batch time: 0.58, accuracy:  15.00%\n",
      "Epoch [7/10], Step [589/600], Loss: 2.3024, batch time: 0.63, accuracy:  5.00%\n",
      "Epoch [7/10], Step [590/600], Loss: 2.3074, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [7/10], Step [591/600], Loss: 2.3181, batch time: 0.58, accuracy:  6.00%\n",
      "Epoch [7/10], Step [592/600], Loss: 2.3064, batch time: 0.59, accuracy:  7.00%\n",
      "Epoch [7/10], Step [593/600], Loss: 2.2971, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [7/10], Step [594/600], Loss: 2.2768, batch time: 0.60, accuracy:  15.00%\n",
      "Epoch [7/10], Step [595/600], Loss: 2.3006, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [7/10], Step [596/600], Loss: 2.2446, batch time: 0.58, accuracy:  17.00%\n",
      "Epoch [7/10], Step [597/600], Loss: 2.2846, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [7/10], Step [598/600], Loss: 2.2878, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [7/10], Step [599/600], Loss: 2.2971, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [7/10], Step [600/600], Loss: 2.2820, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [8/10], Step [1/600], Loss: 2.2942, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [8/10], Step [2/600], Loss: 2.2865, batch time: 0.60, accuracy:  10.00%\n",
      "Epoch [8/10], Step [3/600], Loss: 2.3022, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [8/10], Step [4/600], Loss: 2.2781, batch time: 0.59, accuracy:  9.00%\n",
      "Epoch [8/10], Step [5/600], Loss: 2.3014, batch time: 0.65, accuracy:  9.00%\n",
      "Epoch [8/10], Step [6/600], Loss: 2.2869, batch time: 0.65, accuracy:  12.00%\n",
      "Epoch [8/10], Step [7/600], Loss: 2.2929, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [8/10], Step [8/600], Loss: 2.3067, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [8/10], Step [9/600], Loss: 2.2802, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [8/10], Step [10/600], Loss: 2.2973, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [8/10], Step [11/600], Loss: 2.2746, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [8/10], Step [12/600], Loss: 2.2970, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [8/10], Step [13/600], Loss: 2.2861, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [8/10], Step [14/600], Loss: 2.2916, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [8/10], Step [15/600], Loss: 2.2952, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [8/10], Step [16/600], Loss: 2.2782, batch time: 0.61, accuracy:  11.00%\n",
      "Epoch [8/10], Step [17/600], Loss: 2.2653, batch time: 0.69, accuracy:  12.00%\n",
      "Epoch [8/10], Step [18/600], Loss: 2.2939, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [8/10], Step [19/600], Loss: 2.2876, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [8/10], Step [20/600], Loss: 2.3065, batch time: 0.69, accuracy:  9.00%\n",
      "Epoch [8/10], Step [21/600], Loss: 2.3108, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [8/10], Step [22/600], Loss: 2.2962, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [8/10], Step [23/600], Loss: 2.2894, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [8/10], Step [24/600], Loss: 2.2949, batch time: 0.62, accuracy:  7.00%\n",
      "Epoch [8/10], Step [25/600], Loss: 2.2834, batch time: 0.59, accuracy:  13.00%\n",
      "Epoch [8/10], Step [26/600], Loss: 2.2697, batch time: 0.62, accuracy:  17.00%\n",
      "Epoch [8/10], Step [27/600], Loss: 2.2977, batch time: 0.58, accuracy:  9.00%\n",
      "Epoch [8/10], Step [28/600], Loss: 2.2777, batch time: 0.74, accuracy:  14.00%\n",
      "Epoch [8/10], Step [29/600], Loss: 2.2951, batch time: 0.61, accuracy:  10.00%\n",
      "Epoch [8/10], Step [30/600], Loss: 2.2661, batch time: 0.59, accuracy:  14.00%\n",
      "Epoch [8/10], Step [31/600], Loss: 2.3121, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [8/10], Step [32/600], Loss: 2.2915, batch time: 0.67, accuracy:  11.00%\n",
      "Epoch [8/10], Step [33/600], Loss: 2.2970, batch time: 0.59, accuracy:  9.00%\n",
      "Epoch [8/10], Step [34/600], Loss: 2.2597, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [8/10], Step [35/600], Loss: 2.2634, batch time: 0.56, accuracy:  16.00%\n",
      "Epoch [8/10], Step [36/600], Loss: 2.2744, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [8/10], Step [37/600], Loss: 2.2854, batch time: 0.56, accuracy:  15.00%\n",
      "Epoch [8/10], Step [38/600], Loss: 2.2749, batch time: 0.56, accuracy:  16.00%\n",
      "Epoch [8/10], Step [39/600], Loss: 2.3110, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [8/10], Step [40/600], Loss: 2.2995, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [8/10], Step [41/600], Loss: 2.2843, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [8/10], Step [42/600], Loss: 2.2806, batch time: 0.59, accuracy:  15.00%\n",
      "Epoch [8/10], Step [43/600], Loss: 2.2669, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [8/10], Step [44/600], Loss: 2.3077, batch time: 0.56, accuracy:  9.00%\n",
      "Epoch [8/10], Step [45/600], Loss: 2.2637, batch time: 0.56, accuracy:  18.00%\n",
      "Epoch [8/10], Step [46/600], Loss: 2.2810, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [8/10], Step [47/600], Loss: 2.3090, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [8/10], Step [48/600], Loss: 2.3060, batch time: 0.56, accuracy:  10.00%\n",
      "Epoch [8/10], Step [49/600], Loss: 2.3017, batch time: 0.60, accuracy:  9.00%\n",
      "Epoch [8/10], Step [50/600], Loss: 2.2817, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [8/10], Step [51/600], Loss: 2.2668, batch time: 0.58, accuracy:  17.00%\n",
      "Epoch [8/10], Step [52/600], Loss: 2.2803, batch time: 0.62, accuracy:  15.00%\n",
      "Epoch [8/10], Step [53/600], Loss: 2.2986, batch time: 0.64, accuracy:  13.00%\n",
      "Epoch [8/10], Step [54/600], Loss: 2.2752, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [8/10], Step [55/600], Loss: 2.2462, batch time: 0.68, accuracy:  20.00%\n",
      "Epoch [8/10], Step [56/600], Loss: 2.3036, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [8/10], Step [57/600], Loss: 2.2695, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [8/10], Step [58/600], Loss: 2.2773, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [8/10], Step [59/600], Loss: 2.2979, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [8/10], Step [60/600], Loss: 2.2862, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [8/10], Step [61/600], Loss: 2.2922, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [8/10], Step [62/600], Loss: 2.2834, batch time: 0.58, accuracy:  16.00%\n",
      "Epoch [8/10], Step [63/600], Loss: 2.2760, batch time: 0.59, accuracy:  18.00%\n",
      "Epoch [8/10], Step [64/600], Loss: 2.2904, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [8/10], Step [65/600], Loss: 2.2868, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [8/10], Step [66/600], Loss: 2.2853, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [8/10], Step [67/600], Loss: 2.3026, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [8/10], Step [68/600], Loss: 2.2860, batch time: 0.70, accuracy:  10.00%\n",
      "Epoch [8/10], Step [69/600], Loss: 2.3117, batch time: 0.61, accuracy:  8.00%\n",
      "Epoch [8/10], Step [70/600], Loss: 2.2764, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [8/10], Step [71/600], Loss: 2.2912, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [8/10], Step [72/600], Loss: 2.2420, batch time: 0.57, accuracy:  16.00%\n",
      "Epoch [8/10], Step [73/600], Loss: 2.2741, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [8/10], Step [74/600], Loss: 2.2940, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [8/10], Step [75/600], Loss: 2.3157, batch time: 0.59, accuracy:  7.00%\n",
      "Epoch [8/10], Step [76/600], Loss: 2.2821, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [8/10], Step [77/600], Loss: 2.2991, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [8/10], Step [78/600], Loss: 2.2840, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [8/10], Step [79/600], Loss: 2.2820, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [8/10], Step [80/600], Loss: 2.2701, batch time: 0.68, accuracy:  16.00%\n",
      "Epoch [8/10], Step [81/600], Loss: 2.2772, batch time: 0.65, accuracy:  10.00%\n",
      "Epoch [8/10], Step [82/600], Loss: 2.3051, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [8/10], Step [83/600], Loss: 2.3175, batch time: 0.57, accuracy:  5.00%\n",
      "Epoch [8/10], Step [84/600], Loss: 2.2970, batch time: 0.67, accuracy:  9.00%\n",
      "Epoch [8/10], Step [85/600], Loss: 2.2798, batch time: 0.72, accuracy:  13.00%\n",
      "Epoch [8/10], Step [86/600], Loss: 2.2885, batch time: 0.71, accuracy:  8.00%\n",
      "Epoch [8/10], Step [87/600], Loss: 2.2745, batch time: 0.68, accuracy:  14.00%\n",
      "Epoch [8/10], Step [88/600], Loss: 2.2948, batch time: 0.71, accuracy:  9.00%\n",
      "Epoch [8/10], Step [89/600], Loss: 2.2797, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [8/10], Step [90/600], Loss: 2.2988, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [8/10], Step [91/600], Loss: 2.2956, batch time: 0.59, accuracy:  14.00%\n",
      "Epoch [8/10], Step [92/600], Loss: 2.2612, batch time: 0.63, accuracy:  16.00%\n",
      "Epoch [8/10], Step [93/600], Loss: 2.2817, batch time: 0.59, accuracy:  12.00%\n",
      "Epoch [8/10], Step [94/600], Loss: 2.2745, batch time: 0.58, accuracy:  12.00%\n",
      "Epoch [8/10], Step [95/600], Loss: 2.3055, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [8/10], Step [96/600], Loss: 2.2889, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [8/10], Step [97/600], Loss: 2.3040, batch time: 0.58, accuracy:  5.00%\n",
      "Epoch [8/10], Step [98/600], Loss: 2.2767, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [8/10], Step [99/600], Loss: 2.3032, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [8/10], Step [100/600], Loss: 2.2956, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [8/10], Step [101/600], Loss: 2.2987, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [8/10], Step [102/600], Loss: 2.2988, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [8/10], Step [103/600], Loss: 2.2854, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [8/10], Step [104/600], Loss: 2.2790, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [8/10], Step [105/600], Loss: 2.2807, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [8/10], Step [106/600], Loss: 2.2745, batch time: 0.75, accuracy:  11.00%\n",
      "Epoch [8/10], Step [107/600], Loss: 2.3218, batch time: 0.69, accuracy:  7.00%\n",
      "Epoch [8/10], Step [108/600], Loss: 2.2939, batch time: 0.70, accuracy:  11.00%\n",
      "Epoch [8/10], Step [109/600], Loss: 2.2894, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [8/10], Step [110/600], Loss: 2.2799, batch time: 0.57, accuracy:  15.00%\n",
      "Epoch [8/10], Step [111/600], Loss: 2.2878, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [8/10], Step [112/600], Loss: 2.2953, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [8/10], Step [113/600], Loss: 2.2955, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [8/10], Step [114/600], Loss: 2.2966, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [8/10], Step [115/600], Loss: 2.2793, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [8/10], Step [116/600], Loss: 2.2971, batch time: 0.58, accuracy:  11.00%\n",
      "Epoch [8/10], Step [117/600], Loss: 2.2840, batch time: 0.64, accuracy:  12.00%\n",
      "Epoch [8/10], Step [118/600], Loss: 2.2864, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [8/10], Step [119/600], Loss: 2.2845, batch time: 0.58, accuracy:  14.00%\n",
      "Epoch [8/10], Step [120/600], Loss: 2.2868, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [8/10], Step [121/600], Loss: 2.2889, batch time: 0.57, accuracy:  11.00%\n",
      "Epoch [8/10], Step [122/600], Loss: 2.2922, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [8/10], Step [123/600], Loss: 2.2894, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [8/10], Step [124/600], Loss: 2.3051, batch time: 0.58, accuracy:  8.00%\n",
      "Epoch [8/10], Step [125/600], Loss: 2.2529, batch time: 0.58, accuracy:  17.00%\n",
      "Epoch [8/10], Step [126/600], Loss: 2.3072, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [8/10], Step [127/600], Loss: 2.2846, batch time: 0.57, accuracy:  12.00%\n",
      "Epoch [8/10], Step [128/600], Loss: 2.2628, batch time: 0.56, accuracy:  13.00%\n",
      "Epoch [8/10], Step [129/600], Loss: 2.2861, batch time: 0.56, accuracy:  12.00%\n",
      "Epoch [8/10], Step [130/600], Loss: 2.2618, batch time: 0.57, accuracy:  17.00%\n",
      "Epoch [8/10], Step [131/600], Loss: 2.3071, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [8/10], Step [132/600], Loss: 2.2914, batch time: 0.57, accuracy:  7.00%\n",
      "Epoch [8/10], Step [133/600], Loss: 2.2765, batch time: 0.65, accuracy:  14.00%\n",
      "Epoch [8/10], Step [134/600], Loss: 2.2699, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [8/10], Step [135/600], Loss: 2.2657, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [8/10], Step [136/600], Loss: 2.2895, batch time: 0.69, accuracy:  10.00%\n",
      "Epoch [8/10], Step [137/600], Loss: 2.3111, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [8/10], Step [138/600], Loss: 2.2927, batch time: 0.67, accuracy:  12.00%\n",
      "Epoch [8/10], Step [139/600], Loss: 2.2914, batch time: 0.60, accuracy:  14.00%\n",
      "Epoch [8/10], Step [140/600], Loss: 2.2471, batch time: 0.58, accuracy:  18.00%\n",
      "Epoch [8/10], Step [141/600], Loss: 2.2635, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [8/10], Step [142/600], Loss: 2.2679, batch time: 0.57, accuracy:  13.00%\n",
      "Epoch [8/10], Step [143/600], Loss: 2.2960, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [8/10], Step [144/600], Loss: 2.3151, batch time: 0.57, accuracy:  5.00%\n",
      "Epoch [8/10], Step [145/600], Loss: 2.2692, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [8/10], Step [146/600], Loss: 2.2993, batch time: 0.56, accuracy:  8.00%\n",
      "Epoch [8/10], Step [147/600], Loss: 2.3075, batch time: 0.58, accuracy:  10.00%\n",
      "Epoch [8/10], Step [148/600], Loss: 2.2775, batch time: 0.57, accuracy:  10.00%\n",
      "Epoch [8/10], Step [149/600], Loss: 2.3037, batch time: 0.57, accuracy:  9.00%\n",
      "Epoch [8/10], Step [150/600], Loss: 2.2574, batch time: 0.59, accuracy:  17.00%\n",
      "Epoch [8/10], Step [151/600], Loss: 2.2953, batch time: 0.59, accuracy:  9.00%\n",
      "Epoch [8/10], Step [152/600], Loss: 2.3035, batch time: 0.67, accuracy:  8.00%\n",
      "Epoch [8/10], Step [153/600], Loss: 2.3021, batch time: 0.58, accuracy:  7.00%\n",
      "Epoch [8/10], Step [154/600], Loss: 2.2605, batch time: 0.57, accuracy:  14.00%\n",
      "Epoch [8/10], Step [155/600], Loss: 2.3016, batch time: 0.57, accuracy:  8.00%\n",
      "Epoch [8/10], Step [156/600], Loss: 2.2913, batch time: 0.59, accuracy:  10.00%\n",
      "Epoch [8/10], Step [157/600], Loss: 2.2875, batch time: 0.68, accuracy:  8.00%\n",
      "Epoch [8/10], Step [158/600], Loss: 2.2956, batch time: 0.70, accuracy:  5.00%\n",
      "Epoch [8/10], Step [159/600], Loss: 2.2715, batch time: 0.69, accuracy:  14.00%\n",
      "Epoch [8/10], Step [160/600], Loss: 2.3116, batch time: 0.68, accuracy:  7.00%\n",
      "Epoch [8/10], Step [161/600], Loss: 2.2752, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [8/10], Step [162/600], Loss: 2.3108, batch time: 0.68, accuracy:  9.00%\n",
      "Epoch [8/10], Step [163/600], Loss: 2.2729, batch time: 0.69, accuracy:  13.00%\n",
      "Epoch [8/10], Step [164/600], Loss: 2.2725, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [8/10], Step [165/600], Loss: 2.3058, batch time: 0.68, accuracy:  6.00%\n",
      "Epoch [8/10], Step [166/600], Loss: 2.2736, batch time: 0.67, accuracy:  13.00%\n",
      "Epoch [8/10], Step [167/600], Loss: 2.2865, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [8/10], Step [168/600], Loss: 2.2997, batch time: 0.69, accuracy:  6.00%\n",
      "Epoch [8/10], Step [169/600], Loss: 2.2948, batch time: 0.68, accuracy:  11.00%\n",
      "Epoch [8/10], Step [170/600], Loss: 2.2589, batch time: 0.63, accuracy:  16.00%\n",
      "Epoch [8/10], Step [171/600], Loss: 2.3008, batch time: 0.57, accuracy:  4.00%\n",
      "Epoch [8/10], Step [172/600], Loss: 2.2461, batch time: 0.57, accuracy:  18.00%\n",
      "Epoch [8/10], Step [173/600], Loss: 2.2943, batch time: 0.59, accuracy:  11.00%\n",
      "Epoch [8/10], Step [174/600], Loss: 2.2778, batch time: 0.58, accuracy:  13.00%\n",
      "Epoch [8/10], Step [175/600], Loss: 2.2802, batch time: 0.73, accuracy:  13.00%\n",
      "Epoch [8/10], Step [176/600], Loss: 2.2864, batch time: 0.67, accuracy:  10.00%\n",
      "Epoch [8/10], Step [177/600], Loss: 2.2816, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [8/10], Step [178/600], Loss: 2.2636, batch time: 0.68, accuracy:  12.00%\n",
      "Epoch [8/10], Step [179/600], Loss: 2.2692, batch time: 0.68, accuracy:  15.00%\n",
      "Epoch [8/10], Step [180/600], Loss: 2.2904, batch time: 0.68, accuracy:  10.00%\n",
      "Epoch [8/10], Step [181/600], Loss: 2.2652, batch time: 0.68, accuracy:  16.00%\n",
      "Epoch [8/10], Step [182/600], Loss: 2.3131, batch time: 0.60, accuracy:  8.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_qt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(\"output: \", outputs)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m labels_one_hot \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(labels, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/anaconda3/envs/torchquantum/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchquantum/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/downloads/QuantumTrain/QuantumTrain/general/util.py:218\u001b[0m, in \u001b[0;36mQuantumTrain.<locals>.LewHybridNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    216\u001b[0m device \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 218\u001b[0m probs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQuantumNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m probs_ \u001b[38;5;241m=\u001b[39m probs_[:\u001b[38;5;28mlen\u001b[39m(nw_list_normal)]\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# Generate qubit states using PyTorch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchquantum/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchquantum/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/downloads/QuantumTrain/QuantumTrain/general/util.py:167\u001b[0m, in \u001b[0;36mQuantumTrain.<locals>.LewHybridNN.QLayer.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_blocks):\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mu3_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqdev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcu3_layers[k](qdev)\n\u001b[1;32m    170\u001b[0m state_mag \u001b[38;5;241m=\u001b[39m qdev\u001b[38;5;241m.\u001b[39mget_states_1d()\u001b[38;5;241m.\u001b[39mabs()[\u001b[38;5;241m0\u001b[39m] \n",
      "File \u001b[0;32m~/anaconda3/envs/torchquantum/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchquantum/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchquantum/lib/python3.9/site-packages/torchquantum/graph/graphs.py:73\u001b[0m, in \u001b[0;36mstatic_support.<locals>.forward_register_graph\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstatic_mode \u001b[38;5;129;01mand\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mparent_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mparent_graph\u001b[38;5;241m.\u001b[39madd_op(args[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 73\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstatic_mode \u001b[38;5;129;01mand\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mis_graph_top:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# finish build graph, set flag\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_graph_build_finish()\n",
      "File \u001b[0;32m~/anaconda3/envs/torchquantum/lib/python3.9/site-packages/torchquantum/layer/layers/layers.py:96\u001b[0m, in \u001b[0;36mOp1QAllLayer.forward\u001b[0;34m(self, q_device)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@tq\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_support\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, q_device):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_wires):\n\u001b[0;32m---> 96\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwires\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchquantum/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchquantum/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchquantum/lib/python3.9/site-packages/torchquantum/operator/op_types.py:252\u001b[0m, in \u001b[0;36mOperator.forward\u001b[0;34m(self, q_device, wires, params, inverse)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(q_device, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwires, params\u001b[38;5;241m=\u001b[39mparams, inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minverse)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 252\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m            \u001b[49m\u001b[43mq_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwires\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_wires\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_wires\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m            \u001b[49m\u001b[43minverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_model_tq \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_model_tq\u001b[38;5;241m.\u001b[39mis_add_noise:\n\u001b[1;32m    261\u001b[0m     noise_ops \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_model_tq\u001b[38;5;241m.\u001b[39msample_noise_op(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchquantum/lib/python3.9/site-packages/torchquantum/functional/u3.py:155\u001b[0m, in \u001b[0;36mu3\u001b[0;34m(q_device, wires, params, n_wires, static, parent_graph, inverse, comp_method)\u001b[0m\n\u001b[1;32m    153\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m mat \u001b[38;5;241m=\u001b[39m _u3_mat_dict[name]\n\u001b[0;32m--> 155\u001b[0m \u001b[43mgate_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomp_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwires\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwires\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_wires\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_wires\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43minverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchquantum/lib/python3.9/site-packages/torchquantum/functional/gate_wrapper.py:422\u001b[0m, in \u001b[0;36mgate_wrapper\u001b[0;34m(name, mat, method, q_device, wires, params, n_wires, static, parent_graph, inverse)\u001b[0m\n\u001b[1;32m    420\u001b[0m         matrix \u001b[38;5;241m=\u001b[39m mat(params, n_wires)\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 422\u001b[0m         matrix \u001b[38;5;241m=\u001b[39m \u001b[43mmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    425\u001b[0m     matrix \u001b[38;5;241m=\u001b[39m mat\n",
      "File \u001b[0;32m~/anaconda3/envs/torchquantum/lib/python3.9/site-packages/torchquantum/functional/u3.py:71\u001b[0m, in \u001b[0;36mu3_matrix\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     66\u001b[0m co \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(theta \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     67\u001b[0m si \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msin(theta \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[1;32m     70\u001b[0m     [\n\u001b[0;32m---> 71\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mco\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43msi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     72\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m     73\u001b[0m             [si \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m1\u001b[39mj \u001b[38;5;241m*\u001b[39m phi), co \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m1\u001b[39mj \u001b[38;5;241m*\u001b[39m (phi \u001b[38;5;241m+\u001b[39m lam))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m         ),\n\u001b[1;32m     75\u001b[0m     ],\n\u001b[1;32m     76\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     77\u001b[0m )\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#############################################\n",
    "### Training loop ###########################\n",
    "\n",
    "### (Optional) Start from pretrained model ##\n",
    "# model_qt = torch.load('L16/tq_mm_acc_99_bsf')\n",
    "# model_qt.eval()  # Set the model to evaluation mode\n",
    "#############################################\n",
    "\n",
    "loss_list = [] \n",
    "acc_list = [] \n",
    "acc_best = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model_qt.train()\n",
    "    train_loss = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        since_batch = time.time()\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model_qt(images)\n",
    "        # print(\"output: \", outputs)\n",
    "        labels_one_hot = F.one_hot(labels, num_classes=10).float()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels_one_hot)\n",
    "        # log_loss = torch.log(loss + 1e-6)\n",
    "        \n",
    "        loss_list.append(loss.cpu().detach().numpy())\n",
    "        acc = 100 * correct / total\n",
    "        acc_list.append(acc)\n",
    "        train_loss += loss.cpu().detach().numpy()\n",
    "        \n",
    "        # np.array(loss_list).dump(\"L16/3/loss_list.dat\")\n",
    "        # np.array(acc_list).dump(\"L16/3/acc_list.dat\")\n",
    "        if acc > acc_best:\n",
    "            # torch.save(model_qt, 'L16/3/tq_mm_acc_'+str(int(acc))+'_bsf')\n",
    "            acc_best = acc\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        # if (i+1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}, batch time: {time.time() - since_batch:.2f}, accuracy:  {(acc):.2f}%\")\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    scheduler.step(train_loss)\n",
    "    \n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of MappingNetwork.input_layer.weight: tensor([[-7.9607e+00, -7.3004e+00, -7.4006e+00, -6.6216e+00, -1.1015e+01,\n",
      "         -3.7132e-01, -3.8950e+00, -4.8194e+00,  3.5117e-01, -3.9566e+00,\n",
      "         -2.9183e+00, -2.7443e-01,  3.9356e-02, -3.5626e-01],\n",
      "        [ 5.5427e+00,  5.0829e+00,  5.1527e+00,  4.6103e+00,  7.6690e+00,\n",
      "          2.5853e-01,  2.7119e+00,  3.3555e+00, -2.4450e-01,  2.7548e+00,\n",
      "          2.0319e+00,  1.9108e-01, -2.7402e-02,  2.4805e-01],\n",
      "        [-3.0348e+00, -2.7830e+00, -2.8212e+00, -2.5243e+00, -4.1990e+00,\n",
      "         -1.4155e-01, -1.4848e+00, -1.8372e+00,  1.3387e-01, -1.5083e+00,\n",
      "         -1.1125e+00, -1.0462e-01,  1.5003e-02, -1.3581e-01],\n",
      "        [ 1.5247e-01,  1.3982e-01,  1.4174e-01,  1.2682e-01,  2.1096e-01,\n",
      "          7.1115e-03,  7.4599e-02,  9.2304e-02, -6.7258e-03,  7.5780e-02,\n",
      "          5.5894e-02,  5.2562e-03, -7.5391e-04,  6.8233e-03]], device='cuda:1')\n",
      "Gradient of MappingNetwork.input_layer.bias: tensor([ 8.9407e-08, -5.9605e-08, -1.4901e-08, -2.1886e-08], device='cuda:1')\n",
      "Gradient of MappingNetwork.hidden_layers.0.weight: tensor([[ 0.3615,  0.3823, -0.4653, -1.2855],\n",
      "        [-0.1000, -0.1057,  0.1287,  0.3555],\n",
      "        [ 0.1934,  0.2046, -0.2490, -0.6878],\n",
      "        [ 0.4767,  0.5041, -0.6136, -1.6949],\n",
      "        [-0.7480, -0.7910,  0.9627,  2.6594],\n",
      "        [ 0.1823,  0.1927, -0.2346, -0.6480],\n",
      "        [ 0.5060,  0.5351, -0.6513, -1.7991],\n",
      "        [-0.2087, -0.2207,  0.2687,  0.7421],\n",
      "        [-0.3984, -0.4213,  0.5128,  1.4166],\n",
      "        [ 0.9237,  0.9768, -1.1889, -3.2843],\n",
      "        [-0.0105, -0.0111,  0.0135,  0.0374],\n",
      "        [ 0.2237,  0.2366, -0.2879, -0.7954],\n",
      "        [ 0.3231,  0.3417, -0.4158, -1.1487],\n",
      "        [ 0.3387,  0.3582, -0.4360, -1.2044],\n",
      "        [ 0.9878,  1.0447, -1.2715, -3.5124],\n",
      "        [ 0.1072,  0.1134, -0.1380, -0.3813],\n",
      "        [-0.2241, -0.2370,  0.2885,  0.7970],\n",
      "        [-0.0380, -0.0402,  0.0489,  0.1352],\n",
      "        [-1.1007, -1.1640,  1.4167,  3.9135],\n",
      "        [-0.5425, -0.5738,  0.6983,  1.9291]], device='cuda:1')\n",
      "Gradient of MappingNetwork.hidden_layers.0.bias: tensor([-2.9802e-08, -3.7253e-09, -1.4901e-08, -1.4901e-08, -8.9407e-08,\n",
      "        -1.4901e-08, -2.9802e-08,  4.8429e-08,  2.9802e-08, -1.7881e-07,\n",
      "        -6.7521e-09, -3.3528e-08, -1.4901e-08,  2.9802e-08,  2.9802e-08,\n",
      "        -1.8626e-08, -7.4506e-09, -1.3039e-08, -5.9605e-08, -1.4901e-08],\n",
      "       device='cuda:1')\n",
      "Gradient of MappingNetwork.hidden_layers.1.weight: tensor([[ 0.1156, -1.5559,  4.0297,  2.2302, -1.4594, -0.6014, -0.8662,  2.3803,\n",
      "          0.0675, -0.6686, -5.4274, -1.8728,  4.0427,  2.2892,  1.5308,  0.9241,\n",
      "          1.3543,  2.6736,  1.5359,  3.9676],\n",
      "        [ 0.0451, -0.6078,  1.5742,  0.8712, -0.5701, -0.2349, -0.3384,  0.9299,\n",
      "          0.0264, -0.2612, -2.1202, -0.7316,  1.5793,  0.8943,  0.5980,  0.3610,\n",
      "          0.5290,  1.0444,  0.6000,  1.5499],\n",
      "        [ 0.0611, -0.8230,  2.1314,  1.1796, -0.7719, -0.3181, -0.4582,  1.2590,\n",
      "          0.0357, -0.3537, -2.8707, -0.9906,  2.1383,  1.2108,  0.8097,  0.4888,\n",
      "          0.7163,  1.4142,  0.8124,  2.0986],\n",
      "        [ 0.0822, -1.1066,  2.8660,  1.5862, -1.0380, -0.4277, -0.6161,  1.6929,\n",
      "          0.0480, -0.4756, -3.8601, -1.3320,  2.8753,  1.6281,  1.0888,  0.6572,\n",
      "          0.9632,  1.9016,  1.0924,  2.8219]], device='cuda:1')\n",
      "Gradient of MappingNetwork.hidden_layers.1.bias: tensor([1.1921e-07, 1.1921e-07, 1.4901e-07, 5.9605e-08], device='cuda:1')\n",
      "Gradient of MappingNetwork.output_layer.weight: tensor([[ 2.7631, -4.2278, -1.7978, -0.9447]], device='cuda:1')\n",
      "Gradient of MappingNetwork.output_layer.bias: tensor([-4.7684e-07], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.0.ops_all.0.params: tensor([[ 0.1848, -0.0558,  0.0000]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.0.ops_all.1.params: tensor([[-0.2728, -0.1130,  0.0000]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.0.ops_all.2.params: tensor([[ 0.7049, -0.0013,  0.0000]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.0.ops_all.3.params: tensor([[ 0.0068, -0.0946,  0.0000]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.0.ops_all.4.params: tensor([[ 0.3533, -0.1875,  0.0000]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.0.ops_all.5.params: tensor([[-0.0175, -0.0900,  0.0000]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.0.ops_all.6.params: tensor([[0.2224, 0.0006, 0.0000]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.0.ops_all.7.params: tensor([[0.0735, 0.0992, 0.0000]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.0.ops_all.8.params: tensor([[-0.0182, -0.0518,  0.0000]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.0.ops_all.9.params: tensor([[0.2985, 0.3791, 0.0000]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.0.ops_all.10.params: tensor([[-0.5186,  0.0706,  0.0000]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.0.ops_all.11.params: tensor([[0.1257, 0.1143, 0.0000]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.0.ops_all.12.params: tensor([[ 0.4708, -0.2667,  0.0000]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.1.ops_all.0.params: tensor([[ 0.1109,  0.0195, -0.2310]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.1.ops_all.1.params: tensor([[-0.2612,  0.3285,  0.2624]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.1.ops_all.2.params: tensor([[0.0941, 0.4297, 0.3523]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.1.ops_all.3.params: tensor([[ 0.2770, -0.0403, -0.0488]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.1.ops_all.4.params: tensor([[ 0.7013, -0.0509, -0.0564]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.1.ops_all.5.params: tensor([[ 0.1813,  0.3071, -0.2689]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.1.ops_all.6.params: tensor([[ 0.3366,  0.2573, -0.1860]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.1.ops_all.7.params: tensor([[-0.1069,  0.0057,  0.0979]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.1.ops_all.8.params: tensor([[ 0.2049, -0.1410,  0.0165]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.1.ops_all.9.params: tensor([[-0.4799, -0.1034, -0.0499]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.1.ops_all.10.params: tensor([[ 0.0366,  0.1900, -0.0153]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.1.ops_all.11.params: tensor([[-0.1854,  0.1892,  0.0097]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.1.ops_all.12.params: tensor([[-0.2260,  0.3143, -0.2924]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.2.ops_all.0.params: tensor([[ 0.1597,  0.0564, -0.0613]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.2.ops_all.1.params: tensor([[0.0086, 0.3291, 0.3199]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.2.ops_all.2.params: tensor([[-0.0229,  0.2380,  0.1330]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.2.ops_all.3.params: tensor([[ 0.3929, -0.2755,  0.2749]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.2.ops_all.4.params: tensor([[-0.2817,  0.0062,  0.0797]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.2.ops_all.5.params: tensor([[0.1527, 0.6009, 0.3250]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.2.ops_all.6.params: tensor([[-0.0166,  0.2835,  0.3157]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.2.ops_all.7.params: tensor([[ 0.1356, -0.1359,  0.1955]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.2.ops_all.8.params: tensor([[-0.1022, -0.1045,  0.0043]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.2.ops_all.9.params: tensor([[-0.4000, -0.0724, -0.0622]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.2.ops_all.10.params: tensor([[ 0.1879, -0.0533,  0.2086]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.2.ops_all.11.params: tensor([[-0.3735,  0.1497,  0.2991]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.2.ops_all.12.params: tensor([[-0.1918, -0.1146, -0.0545]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.3.ops_all.0.params: tensor([[0.0341, 0.2608, 0.0714]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.3.ops_all.1.params: tensor([[ 0.0511, -0.0426, -0.1730]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.3.ops_all.2.params: tensor([[-0.1672, -0.2491,  0.2400]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.3.ops_all.3.params: tensor([[-0.1979,  0.3006, -0.2714]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.3.ops_all.4.params: tensor([[-0.3351,  0.1456,  0.2814]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.3.ops_all.5.params: tensor([[-0.1792, -0.0169,  0.1549]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.3.ops_all.6.params: tensor([[-0.3855,  0.0044,  0.2489]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.3.ops_all.7.params: tensor([[-0.2295,  0.1348, -0.1769]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.3.ops_all.8.params: tensor([[-0.4963,  0.1834,  0.0792]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.3.ops_all.9.params: tensor([[-0.2655,  0.1548, -0.0729]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.3.ops_all.10.params: tensor([[ 0.1154,  0.3085, -0.4184]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.3.ops_all.11.params: tensor([[-0.4026, -0.0431,  0.0700]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.3.ops_all.12.params: tensor([[-0.1902,  0.2288,  0.2021]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.4.ops_all.0.params: tensor([[ 0.3549, -0.1400,  0.2707]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.4.ops_all.1.params: tensor([[-0.3598, -0.3837,  0.5097]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.4.ops_all.2.params: tensor([[-0.1808, -0.0510, -0.0815]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.4.ops_all.3.params: tensor([[-0.0249,  0.0126,  0.2896]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.4.ops_all.4.params: tensor([[ 0.3356, -0.0225, -0.0478]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.4.ops_all.5.params: tensor([[0.2338, 0.4755, 0.4356]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.4.ops_all.6.params: tensor([[-0.0498,  0.2265, -0.1484]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.4.ops_all.7.params: tensor([[-0.0971, -0.1895,  0.1673]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.4.ops_all.8.params: tensor([[-0.0003, -0.2571,  0.2248]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.4.ops_all.9.params: tensor([[ 0.2903,  0.2359, -0.1115]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.4.ops_all.10.params: tensor([[-0.0307, -0.0807,  0.4980]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.4.ops_all.11.params: tensor([[-0.1104,  0.3331, -0.0670]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.4.ops_all.12.params: tensor([[-0.3296,  0.0175, -0.0285]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.5.ops_all.0.params: tensor([[ 0.0315, -0.1779, -0.0328]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.5.ops_all.1.params: tensor([[-0.2373,  0.2411, -0.0634]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.5.ops_all.2.params: tensor([[-0.3530, -0.3753,  0.3736]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.5.ops_all.3.params: tensor([[ 0.2280, -0.0909, -0.0220]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.5.ops_all.4.params: tensor([[-0.2962,  0.1090, -0.0578]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.5.ops_all.5.params: tensor([[0.1054, 0.2530, 0.3247]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.5.ops_all.6.params: tensor([[-0.0335,  0.0551, -0.0078]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.5.ops_all.7.params: tensor([[-0.0931,  0.2264,  0.0646]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.5.ops_all.8.params: tensor([[-0.2809, -0.2989, -0.2960]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.5.ops_all.9.params: tensor([[-0.2863, -0.0710,  0.1288]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.5.ops_all.10.params: tensor([[ 0.0869,  0.1081, -0.2479]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.5.ops_all.11.params: tensor([[-0.0054,  0.2824,  0.1054]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.5.ops_all.12.params: tensor([[ 0.0920, -0.1298, -0.1124]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.6.ops_all.0.params: tensor([[ 0.1884,  0.4519, -0.2877]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.6.ops_all.1.params: tensor([[0.3780, 0.0099, 0.0781]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.6.ops_all.2.params: tensor([[-0.2567, -0.4891, -0.3342]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.6.ops_all.3.params: tensor([[-0.0320, -0.2965, -0.1065]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.6.ops_all.4.params: tensor([[-0.2914,  0.0500,  0.0406]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.6.ops_all.5.params: tensor([[0.0842, 0.1661, 0.2872]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.6.ops_all.6.params: tensor([[ 0.1987, -0.0831,  0.0263]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.6.ops_all.7.params: tensor([[ 0.1596, -0.0033,  0.2289]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.6.ops_all.8.params: tensor([[ 0.2093, -0.2275,  0.2048]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.6.ops_all.9.params: tensor([[-0.1807, -0.0904, -0.0260]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.6.ops_all.10.params: tensor([[-0.2982,  0.6867,  0.2609]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.6.ops_all.11.params: tensor([[-0.2012, -0.1707,  0.3453]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.6.ops_all.12.params: tensor([[ 0.0064, -0.3179, -0.3692]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.7.ops_all.0.params: tensor([[ 0.3719, -0.0143,  0.3858]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.7.ops_all.1.params: tensor([[-0.2827, -0.0143, -0.2879]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.7.ops_all.2.params: tensor([[0.0292, 0.4888, 0.1916]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.7.ops_all.3.params: tensor([[ 0.0780,  0.1464, -0.1020]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.7.ops_all.4.params: tensor([[-0.3117, -0.0231,  0.0167]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.7.ops_all.5.params: tensor([[-0.0805, -0.4988,  0.1569]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.7.ops_all.6.params: tensor([[ 0.0297, -0.0860, -0.1638]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.7.ops_all.7.params: tensor([[-0.3530,  0.1588, -0.4272]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.7.ops_all.8.params: tensor([[-0.0178,  0.4574, -0.4238]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.7.ops_all.9.params: tensor([[ 0.2472, -0.5135, -0.0708]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.7.ops_all.10.params: tensor([[0.7575, 0.2115, 0.4616]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.7.ops_all.11.params: tensor([[-0.1997,  0.4977, -0.1883]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.7.ops_all.12.params: tensor([[ 0.3712,  0.2041, -0.3115]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.8.ops_all.0.params: tensor([[-0.0122,  0.2666,  0.0393]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.8.ops_all.1.params: tensor([[-0.2921,  0.0092, -0.0096]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.8.ops_all.2.params: tensor([[-0.2930, -0.1583,  0.1379]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.8.ops_all.3.params: tensor([[-0.4436, -0.1275,  0.1669]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.8.ops_all.4.params: tensor([[ 0.4779, -0.1886,  0.0088]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.8.ops_all.5.params: tensor([[-0.0424, -0.1211,  0.0606]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.8.ops_all.6.params: tensor([[ 0.0068,  0.0878, -0.0838]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.8.ops_all.7.params: tensor([[0.3039, 0.2609, 0.1226]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.8.ops_all.8.params: tensor([[-0.1108, -0.0135, -0.0314]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.8.ops_all.9.params: tensor([[ 0.2124,  0.0201, -0.3819]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.8.ops_all.10.params: tensor([[-0.3323, -0.1615,  0.0768]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.8.ops_all.11.params: tensor([[0.0838, 0.5686, 0.5851]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.8.ops_all.12.params: tensor([[-0.1134,  0.1128,  0.1784]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.9.ops_all.0.params: tensor([[-0.0904, -0.2164,  0.4005]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.9.ops_all.1.params: tensor([[-0.2536, -0.1897,  0.0017]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.9.ops_all.2.params: tensor([[-0.0230,  0.1014, -0.4451]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.9.ops_all.3.params: tensor([[-0.3566, -0.0562,  0.0125]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.9.ops_all.4.params: tensor([[-0.0074, -0.1259,  0.0062]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.9.ops_all.5.params: tensor([[0.0909, 0.0451, 0.0431]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.9.ops_all.6.params: tensor([[ 0.5861, -0.4532,  0.2236]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.9.ops_all.7.params: tensor([[-0.1025, -0.3628,  0.4079]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.9.ops_all.8.params: tensor([[ 0.1163, -0.3546,  0.2638]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.9.ops_all.9.params: tensor([[0.0624, 0.1651, 0.0131]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.9.ops_all.10.params: tensor([[ 0.2935,  0.2033, -0.3274]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.9.ops_all.11.params: tensor([[-0.4879,  0.1501,  0.5606]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.9.ops_all.12.params: tensor([[0.1699, 0.1926, 0.0979]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.10.ops_all.0.params: tensor([[-0.1374,  0.4048, -0.2170]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.10.ops_all.1.params: tensor([[0.2444, 0.0171, 0.0137]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.10.ops_all.2.params: tensor([[ 0.1684, -0.1580, -0.0037]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.10.ops_all.3.params: tensor([[ 0.4425, -0.1064, -0.3075]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.10.ops_all.4.params: tensor([[-0.2005, -0.2816, -0.1208]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.10.ops_all.5.params: tensor([[-0.0425,  0.0113, -0.0199]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.10.ops_all.6.params: tensor([[ 0.2942,  0.3791, -0.4507]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.10.ops_all.7.params: tensor([[-0.2063, -0.0291, -0.1442]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.10.ops_all.8.params: tensor([[ 0.0126,  0.1803, -0.1364]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.10.ops_all.9.params: tensor([[-0.1517, -0.1983,  0.0576]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.10.ops_all.10.params: tensor([[-0.2448,  0.1569,  0.2200]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.10.ops_all.11.params: tensor([[-0.1037,  0.1309,  0.1347]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.10.ops_all.12.params: tensor([[-0.0621,  0.0340,  0.2739]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.11.ops_all.0.params: tensor([[ 0.0883, -0.0461,  0.1970]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.11.ops_all.1.params: tensor([[ 0.2604, -0.4042, -0.4533]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.11.ops_all.2.params: tensor([[-0.1747,  0.1477, -0.0732]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.11.ops_all.3.params: tensor([[-0.2292,  0.3415,  0.1201]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.11.ops_all.4.params: tensor([[ 0.3686,  0.0458, -0.0686]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.11.ops_all.5.params: tensor([[-0.0460,  0.1172,  0.1103]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.11.ops_all.6.params: tensor([[-0.3396,  0.3921,  0.3679]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.11.ops_all.7.params: tensor([[-0.0368,  0.0962, -0.1910]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.11.ops_all.8.params: tensor([[-0.1776, -0.3470,  0.0687]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.11.ops_all.9.params: tensor([[ 0.3340, -0.1178, -0.2208]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.11.ops_all.10.params: tensor([[0.1312, 0.1763, 0.1695]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.11.ops_all.11.params: tensor([[-0.0145, -0.1732,  0.1382]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.11.ops_all.12.params: tensor([[-0.0623,  0.3145, -0.3128]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.12.ops_all.0.params: tensor([[0.0117, 0.5906, 0.0593]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.12.ops_all.1.params: tensor([[-0.1903, -0.1603, -0.1604]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.12.ops_all.2.params: tensor([[-0.1948, -0.1702,  0.1606]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.12.ops_all.3.params: tensor([[ 0.2028, -0.2216, -0.0975]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.12.ops_all.4.params: tensor([[ 0.0924, -0.3488,  0.2979]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.12.ops_all.5.params: tensor([[-0.3733,  0.1363,  0.0168]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.12.ops_all.6.params: tensor([[ 0.2745, -0.6242,  0.3561]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.12.ops_all.7.params: tensor([[ 0.0393, -0.0049,  0.2388]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.12.ops_all.8.params: tensor([[ 0.6165, -0.1971, -0.1463]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.12.ops_all.9.params: tensor([[-0.1963,  0.1672, -0.1505]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.12.ops_all.10.params: tensor([[0.0896, 0.0676, 0.1657]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.12.ops_all.11.params: tensor([[-0.0926, -0.4766, -0.1472]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.12.ops_all.12.params: tensor([[ 0.0209,  0.0642, -0.0430]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.13.ops_all.0.params: tensor([[-0.2024,  0.6091,  0.5121]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.13.ops_all.1.params: tensor([[-0.2059,  0.0142, -0.0808]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.13.ops_all.2.params: tensor([[0.2138, 0.3633, 0.1563]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.13.ops_all.3.params: tensor([[ 0.0558, -0.0500, -0.0356]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.13.ops_all.4.params: tensor([[-0.0984, -0.3829, -0.2308]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.13.ops_all.5.params: tensor([[ 0.0266, -0.4324, -0.1169]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.13.ops_all.6.params: tensor([[ 0.1450,  0.3768, -0.6652]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.13.ops_all.7.params: tensor([[-0.0322,  0.1569, -0.4124]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.13.ops_all.8.params: tensor([[-0.4398, -0.1081, -0.0734]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.13.ops_all.9.params: tensor([[ 0.1532, -0.0213,  0.0211]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.13.ops_all.10.params: tensor([[ 0.3047, -0.0249,  0.0326]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.13.ops_all.11.params: tensor([[ 0.3278,  0.0137, -0.0454]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.13.ops_all.12.params: tensor([[-0.1197, -0.3852,  0.1577]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.14.ops_all.0.params: tensor([[-0.3036,  0.2242,  0.6069]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.14.ops_all.1.params: tensor([[0.1650, 0.2724, 0.0396]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.14.ops_all.2.params: tensor([[-0.1866,  0.3768,  0.3277]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.14.ops_all.3.params: tensor([[-0.0593,  0.0163, -0.0187]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.14.ops_all.4.params: tensor([[-0.2017, -0.4082,  0.1729]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.14.ops_all.5.params: tensor([[ 0.1207, -0.2747, -0.3810]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.14.ops_all.6.params: tensor([[ 0.0509,  0.0667, -0.1979]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.14.ops_all.7.params: tensor([[0.1418, 0.1272, 0.0625]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.14.ops_all.8.params: tensor([[-0.0137,  0.1888, -0.1566]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.14.ops_all.9.params: tensor([[-0.1744, -0.1230, -0.1193]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.14.ops_all.10.params: tensor([[ 0.0586,  0.1046, -0.5697]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.14.ops_all.11.params: tensor([[-0.0477, -0.0176, -0.3363]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.14.ops_all.12.params: tensor([[ 0.2671, -0.0087, -0.5398]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.15.ops_all.0.params: tensor([[-0.0213,  0.0117, -0.0057]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.15.ops_all.1.params: tensor([[-0.4066, -0.0764,  0.0293]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.15.ops_all.2.params: tensor([[0.0901, 0.1627, 0.1739]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.15.ops_all.3.params: tensor([[-0.1621, -0.0030, -0.0098]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.15.ops_all.4.params: tensor([[0.3152, 0.0614, 0.1214]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.15.ops_all.5.params: tensor([[-0.2495,  0.0259,  0.1134]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.15.ops_all.6.params: tensor([[-0.3737, -0.0562,  0.0807]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.15.ops_all.7.params: tensor([[-0.4436, -0.0185, -0.0553]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.15.ops_all.8.params: tensor([[-0.0682,  0.0689,  0.0938]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.15.ops_all.9.params: tensor([[-0.1762,  0.0254, -0.0218]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.15.ops_all.10.params: tensor([[0.1986, 0.0130, 0.0313]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.15.ops_all.11.params: tensor([[ 0.1040,  0.0368, -0.1472]], device='cuda:1')\n",
      "Gradient of QuantumNN.u3_layers.15.ops_all.12.params: tensor([[ 0.3505,  0.0879, -0.2103]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.0.ops_all.0.params: tensor([[ 0.0091,  0.1952, -0.1801]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.0.ops_all.1.params: tensor([[ 0.2914,  0.3529, -0.0006]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.0.ops_all.2.params: tensor([[-0.0160,  0.0064, -0.0394]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.0.ops_all.3.params: tensor([[ 0.1184,  0.0346, -0.0964]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.0.ops_all.4.params: tensor([[ 0.0272, -0.2177, -0.0388]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.0.ops_all.5.params: tensor([[-0.0289, -0.1878, -0.0011]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.0.ops_all.6.params: tensor([[-0.0182, -0.1828, -0.1815]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.0.ops_all.7.params: tensor([[-0.1246,  0.0206, -0.0476]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.0.ops_all.8.params: tensor([[ 0.3782, -0.1012,  0.3278]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.0.ops_all.9.params: tensor([[-0.0835,  0.0428,  0.1286]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.0.ops_all.10.params: tensor([[ 0.1211, -0.0958,  0.0088]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.0.ops_all.11.params: tensor([[-0.1598, -0.2700, -0.2442]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.0.ops_all.12.params: tensor([[-0.0323, -0.3710, -0.1958]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.1.ops_all.0.params: tensor([[-0.0706,  0.0251,  0.0337]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.1.ops_all.1.params: tensor([[-0.1096,  0.0057,  0.3024]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.1.ops_all.2.params: tensor([[ 0.1375,  0.1803, -0.1350]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.1.ops_all.3.params: tensor([[-0.0086,  0.2920,  0.1613]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.1.ops_all.4.params: tensor([[-8.4669e-06,  2.2121e-01,  2.0337e-01]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.1.ops_all.5.params: tensor([[-0.1032,  0.3323,  0.2740]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.1.ops_all.6.params: tensor([[0.2684, 0.2487, 0.0590]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.1.ops_all.7.params: tensor([[ 0.0418,  0.1015, -0.0437]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.1.ops_all.8.params: tensor([[0.3233, 0.1230, 0.0818]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.1.ops_all.9.params: tensor([[ 0.2592, -0.0279, -0.0465]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.1.ops_all.10.params: tensor([[-0.1190,  0.2989,  0.1889]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.1.ops_all.11.params: tensor([[0.1811, 0.0393, 0.4081]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.1.ops_all.12.params: tensor([[ 0.0944, -0.0442,  0.0365]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.2.ops_all.0.params: tensor([[-0.1618, -0.1926,  0.3095]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.2.ops_all.1.params: tensor([[-0.0338, -0.0863, -0.0883]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.2.ops_all.2.params: tensor([[0.0323, 0.1437, 0.1396]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.2.ops_all.3.params: tensor([[ 0.0503,  0.1340, -0.1411]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.2.ops_all.4.params: tensor([[-0.0716, -0.0158,  0.4301]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.2.ops_all.5.params: tensor([[-0.1406,  0.0848,  0.1194]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.2.ops_all.6.params: tensor([[-0.0454,  0.0117,  0.0528]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.2.ops_all.7.params: tensor([[-0.0651, -0.1323, -0.3160]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.2.ops_all.8.params: tensor([[0.1464, 0.0928, 0.0933]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.2.ops_all.9.params: tensor([[ 0.2304, -0.3596,  0.0055]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.2.ops_all.10.params: tensor([[ 0.2852, -0.1821, -0.1024]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.2.ops_all.11.params: tensor([[ 0.0843,  0.1517, -0.1650]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.2.ops_all.12.params: tensor([[-0.3082,  0.1290,  0.1140]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.3.ops_all.0.params: tensor([[-0.1281,  0.4284, -0.1239]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.3.ops_all.1.params: tensor([[0.0889, 0.2088, 0.0412]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.3.ops_all.2.params: tensor([[0.0740, 0.1634, 0.1744]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.3.ops_all.3.params: tensor([[-0.1402, -0.1026,  0.0908]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.3.ops_all.4.params: tensor([[ 0.1208,  0.3059, -0.1467]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.3.ops_all.5.params: tensor([[0.1491, 0.1405, 0.2933]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.3.ops_all.6.params: tensor([[ 0.0433, -0.1085, -0.1410]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.3.ops_all.7.params: tensor([[0.4641, 0.1151, 0.0738]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.3.ops_all.8.params: tensor([[-0.2278,  0.0281,  0.2944]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.3.ops_all.9.params: tensor([[-0.1372, -0.0664, -0.2559]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.3.ops_all.10.params: tensor([[-0.1925,  0.2503,  0.2741]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.3.ops_all.11.params: tensor([[-0.1379, -0.1439,  0.1133]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.3.ops_all.12.params: tensor([[0.1065, 0.0927, 0.0827]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.4.ops_all.0.params: tensor([[-0.3194,  0.0314, -0.2889]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.4.ops_all.1.params: tensor([[ 0.2608,  0.1715, -0.2531]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.4.ops_all.2.params: tensor([[-0.2715,  0.2445,  0.2792]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.4.ops_all.3.params: tensor([[-0.0743, -0.4052, -0.3699]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.4.ops_all.4.params: tensor([[0.2468, 0.1923, 0.3431]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.4.ops_all.5.params: tensor([[0.0776, 0.0884, 0.3227]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.4.ops_all.6.params: tensor([[ 0.1061, -0.0584, -0.3124]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.4.ops_all.7.params: tensor([[ 0.0397, -0.2353, -0.1964]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.4.ops_all.8.params: tensor([[-0.2430, -0.2968, -0.1896]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.4.ops_all.9.params: tensor([[-0.4775,  0.0156,  0.1828]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.4.ops_all.10.params: tensor([[ 0.0823, -0.2419, -0.0142]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.4.ops_all.11.params: tensor([[ 0.0861, -0.1640, -0.0342]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.4.ops_all.12.params: tensor([[ 0.1282, -0.0471, -0.1544]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.5.ops_all.0.params: tensor([[-0.0880, -0.1462,  0.0169]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.5.ops_all.1.params: tensor([[-0.1336, -0.2018, -0.2429]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.5.ops_all.2.params: tensor([[-0.2287, -0.1966, -0.1810]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.5.ops_all.3.params: tensor([[-0.2844, -0.2574, -0.1890]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.5.ops_all.4.params: tensor([[0.1958, 0.3055, 0.2713]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.5.ops_all.5.params: tensor([[-0.1042,  0.1825,  0.2113]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.5.ops_all.6.params: tensor([[0.1536, 0.2034, 0.2009]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.5.ops_all.7.params: tensor([[ 0.0116,  0.3616, -0.1421]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.5.ops_all.8.params: tensor([[0.3708, 0.3029, 0.2578]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.5.ops_all.9.params: tensor([[ 0.0062, -0.0022, -0.1550]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.5.ops_all.10.params: tensor([[-0.0553,  0.2044,  0.1415]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.5.ops_all.11.params: tensor([[-0.0025,  0.1141,  0.3535]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.5.ops_all.12.params: tensor([[ 0.0531, -0.3270, -0.2172]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.6.ops_all.0.params: tensor([[ 0.1220, -0.0572,  0.2406]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.6.ops_all.1.params: tensor([[-0.0140,  0.1647, -0.5160]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.6.ops_all.2.params: tensor([[-0.0778,  0.0031, -0.1914]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.6.ops_all.3.params: tensor([[ 0.0561, -0.3020, -0.2688]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.6.ops_all.4.params: tensor([[0.3234, 0.2272, 0.2364]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.6.ops_all.5.params: tensor([[0.0394, 0.0070, 0.0877]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.6.ops_all.6.params: tensor([[ 0.0410, -0.4160,  0.0079]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.6.ops_all.7.params: tensor([[ 0.2771, -0.1838,  0.0126]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.6.ops_all.8.params: tensor([[ 0.1584, -0.3384, -0.3580]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.6.ops_all.9.params: tensor([[ 0.1181, -0.1451,  0.0801]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.6.ops_all.10.params: tensor([[-0.0590,  0.0390,  0.0565]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.6.ops_all.11.params: tensor([[-0.1300, -0.1447, -0.1512]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.6.ops_all.12.params: tensor([[0.0455, 0.1287, 0.1948]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.7.ops_all.0.params: tensor([[0.0610, 0.0444, 0.0397]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.7.ops_all.1.params: tensor([[ 0.2070, -0.1611,  0.1897]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.7.ops_all.2.params: tensor([[0.0753, 0.0803, 0.0598]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.7.ops_all.3.params: tensor([[0.2681, 0.1017, 0.0698]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.7.ops_all.4.params: tensor([[ 0.1742,  0.2834, -0.2759]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.7.ops_all.5.params: tensor([[0.1579, 0.0103, 0.0081]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.7.ops_all.6.params: tensor([[-0.4853, -0.0689, -0.0327]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.7.ops_all.7.params: tensor([[ 0.1204, -0.1142,  0.3746]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.7.ops_all.8.params: tensor([[ 0.0820, -0.0859, -0.2175]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.7.ops_all.9.params: tensor([[-0.1092, -0.2573, -0.1227]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.7.ops_all.10.params: tensor([[0.1873, 0.3989, 0.3114]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.7.ops_all.11.params: tensor([[-0.0020,  0.4884,  0.5141]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.7.ops_all.12.params: tensor([[0.0038, 0.1007, 0.0471]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.8.ops_all.0.params: tensor([[-0.0658,  0.1341,  0.1416]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.8.ops_all.1.params: tensor([[-0.0041, -0.1565,  0.1302]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.8.ops_all.2.params: tensor([[-0.1268,  0.0047, -0.1353]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.8.ops_all.3.params: tensor([[0.1166, 0.2024, 0.0076]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.8.ops_all.4.params: tensor([[ 0.0217, -0.0574, -0.2216]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.8.ops_all.5.params: tensor([[ 0.1782,  0.0034, -0.1325]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.8.ops_all.6.params: tensor([[0.2243, 0.3007, 0.1537]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.8.ops_all.7.params: tensor([[0.0665, 0.4825, 0.2052]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.8.ops_all.8.params: tensor([[ 0.2765, -0.1839, -0.1768]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.8.ops_all.9.params: tensor([[ 0.1842, -0.0450,  0.1210]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.8.ops_all.10.params: tensor([[0.0781, 0.0100, 0.0180]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.8.ops_all.11.params: tensor([[0.4474, 0.2813, 0.2961]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.8.ops_all.12.params: tensor([[0.0720, 0.2387, 0.1048]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.9.ops_all.0.params: tensor([[ 0.2409, -0.2378, -0.4411]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.9.ops_all.1.params: tensor([[0.4155, 0.0258, 0.1309]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.9.ops_all.2.params: tensor([[-0.0462, -0.1107,  0.1407]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.9.ops_all.3.params: tensor([[-0.2313, -0.1960, -0.2011]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.9.ops_all.4.params: tensor([[ 0.1156, -0.1749, -0.1100]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.9.ops_all.5.params: tensor([[-0.5545, -0.2599, -0.2624]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.9.ops_all.6.params: tensor([[-0.1823, -0.1150, -0.3336]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.9.ops_all.7.params: tensor([[ 0.1125, -0.0723, -0.2904]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.9.ops_all.8.params: tensor([[ 0.0773, -0.1862, -0.0787]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.9.ops_all.9.params: tensor([[0.0992, 0.1872, 0.1705]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.9.ops_all.10.params: tensor([[0.2464, 0.4433, 0.4586]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.9.ops_all.11.params: tensor([[ 0.0539, -0.0504, -0.1317]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.9.ops_all.12.params: tensor([[-0.0877, -0.0360, -0.0354]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.10.ops_all.0.params: tensor([[ 0.2277, -0.1221,  0.3483]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.10.ops_all.1.params: tensor([[-0.0962, -0.2445, -0.3293]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.10.ops_all.2.params: tensor([[ 0.3324,  0.0638, -0.1627]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.10.ops_all.3.params: tensor([[ 0.3558,  0.1445, -0.0685]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.10.ops_all.4.params: tensor([[-0.1033,  0.0155, -0.0835]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.10.ops_all.5.params: tensor([[-0.1271,  0.2430,  0.2542]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.10.ops_all.6.params: tensor([[0.1048, 0.2195, 0.3814]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.10.ops_all.7.params: tensor([[-0.1913, -0.0445,  0.0671]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.10.ops_all.8.params: tensor([[-0.0498, -0.3390, -0.3165]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.10.ops_all.9.params: tensor([[-0.0797,  0.0309,  0.0184]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.10.ops_all.10.params: tensor([[-0.0465,  0.0367,  0.0294]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.10.ops_all.11.params: tensor([[ 0.0145, -0.0472,  0.2996]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.10.ops_all.12.params: tensor([[-0.0601, -0.2265, -0.0187]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.11.ops_all.0.params: tensor([[ 0.1843,  0.0953, -0.1485]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.11.ops_all.1.params: tensor([[-0.0590,  0.0010, -0.0119]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.11.ops_all.2.params: tensor([[ 0.0804, -0.0873,  0.3517]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.11.ops_all.3.params: tensor([[ 0.1376,  0.1353, -0.1168]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.11.ops_all.4.params: tensor([[0.1015, 0.2076, 0.3080]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.11.ops_all.5.params: tensor([[0.2291, 0.0270, 0.0630]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.11.ops_all.6.params: tensor([[-0.1234,  0.3605,  0.2179]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.11.ops_all.7.params: tensor([[0.1268, 0.2383, 0.0377]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.11.ops_all.8.params: tensor([[0.2237, 0.0169, 0.0496]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.11.ops_all.9.params: tensor([[-0.1162,  0.2116,  0.2223]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.11.ops_all.10.params: tensor([[0.1241, 0.1579, 0.1318]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.11.ops_all.11.params: tensor([[-0.0873, -0.2268,  0.1307]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.11.ops_all.12.params: tensor([[-0.0504,  0.1767,  0.0713]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.12.ops_all.0.params: tensor([[-0.1300,  0.2482,  0.1688]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.12.ops_all.1.params: tensor([[ 0.0550,  0.2687, -0.0577]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.12.ops_all.2.params: tensor([[-0.1293,  0.1726, -0.0134]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.12.ops_all.3.params: tensor([[ 0.0882, -0.3712, -0.4892]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.12.ops_all.4.params: tensor([[-0.2123, -0.2861, -0.0329]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.12.ops_all.5.params: tensor([[-0.3058, -0.4418, -0.4008]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.12.ops_all.6.params: tensor([[ 0.1622, -0.5425, -0.1350]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.12.ops_all.7.params: tensor([[ 0.3298, -0.1934, -0.3170]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.12.ops_all.8.params: tensor([[ 0.0453, -0.1081,  0.0380]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.12.ops_all.9.params: tensor([[0.2289, 0.1703, 0.2053]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.12.ops_all.10.params: tensor([[ 0.0812,  0.2300, -0.2012]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.12.ops_all.11.params: tensor([[ 0.1417, -0.2680, -0.3615]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.12.ops_all.12.params: tensor([[-0.2554,  0.2819,  0.3604]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.13.ops_all.0.params: tensor([[-0.2062,  0.3097,  0.2843]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.13.ops_all.1.params: tensor([[-0.5183,  0.0730,  0.1086]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.13.ops_all.2.params: tensor([[0.1928, 0.2012, 0.1700]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.13.ops_all.3.params: tensor([[-0.0560,  0.1796, -0.3762]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.13.ops_all.4.params: tensor([[ 0.0597, -0.0395, -0.0910]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.13.ops_all.5.params: tensor([[-0.1109, -0.3817,  0.1930]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.13.ops_all.6.params: tensor([[-0.2812, -0.1875, -0.0931]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.13.ops_all.7.params: tensor([[0.1796, 0.0076, 0.0562]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.13.ops_all.8.params: tensor([[-0.0061, -0.1812, -0.0832]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.13.ops_all.9.params: tensor([[-0.2141, -0.3394,  0.2054]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.13.ops_all.10.params: tensor([[ 0.2732, -0.5414, -0.1914]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.13.ops_all.11.params: tensor([[-0.3662, -0.3879, -0.2333]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.13.ops_all.12.params: tensor([[-0.0708, -0.2212, -0.2190]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.14.ops_all.0.params: tensor([[0.1156, 0.2423, 0.4854]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.14.ops_all.1.params: tensor([[ 0.0920, -0.0949,  0.1080]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.14.ops_all.2.params: tensor([[0.2139, 0.1581, 0.1842]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.14.ops_all.3.params: tensor([[-0.2398,  0.2552, -0.2744]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.14.ops_all.4.params: tensor([[ 0.1797,  0.2051, -0.1830]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.14.ops_all.5.params: tensor([[0.0966, 0.1851, 0.1712]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.14.ops_all.6.params: tensor([[ 0.1040, -0.1292,  0.0533]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.14.ops_all.7.params: tensor([[ 0.0034, -0.0079,  0.0871]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.14.ops_all.8.params: tensor([[-0.0633, -0.0042, -0.1055]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.14.ops_all.9.params: tensor([[ 0.4180, -0.1085, -0.0351]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.14.ops_all.10.params: tensor([[0.2402, 0.0085, 0.1382]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.14.ops_all.11.params: tensor([[-0.3978, -0.1340,  0.0676]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.14.ops_all.12.params: tensor([[-0.0073, -0.2459, -0.0160]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.15.ops_all.0.params: tensor([[ 0.3731,  0.0190, -0.0573]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.15.ops_all.1.params: tensor([[0.0921, 0.0000, 0.1627]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.15.ops_all.2.params: tensor([[-1.9608e-02,  7.7300e-08, -2.9554e-03]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.15.ops_all.3.params: tensor([[-2.2346e-01, -5.7742e-08,  6.1414e-02]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.15.ops_all.4.params: tensor([[-1.6625e-01,  4.4703e-08,  2.5869e-02]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.15.ops_all.5.params: tensor([[-2.6100e-01, -1.5832e-08, -5.6202e-02]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.15.ops_all.6.params: tensor([[-1.1918e-01,  1.4901e-08, -1.8461e-02]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.15.ops_all.7.params: tensor([[ 1.7562e-02, -1.9558e-08,  6.8866e-02]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.15.ops_all.8.params: tensor([[ 1.3512e-01, -1.4668e-08,  2.5377e-02]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.15.ops_all.9.params: tensor([[0.3366, 0.0000, 0.0130]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.15.ops_all.10.params: tensor([[-1.6238e-03,  2.1420e-08,  3.6847e-02]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.15.ops_all.11.params: tensor([[-2.3700e-01, -4.8429e-08,  8.7905e-02]], device='cuda:1')\n",
      "Gradient of QuantumNN.cu3_layers.15.ops_all.12.params: tensor([[ 2.2265e-01, -2.0489e-08,  1.1674e-02]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "# Print gradients of all parameters\n",
    "for name, param in model_qt.named_parameters():\n",
    "    print(f\"Gradient of {name}: {param.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
